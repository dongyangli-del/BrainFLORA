{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "training_images_dir = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Images/images\"\n",
    "\n",
    "def count_images(directory):\n",
    "    total_dirs = 0\n",
    "    total_images = 0\n",
    "\n",
    "\n",
    "    for entry in os.listdir(directory):\n",
    "        path = os.path.join(directory, entry)\n",
    "        if os.path.isdir(path):\n",
    "            total_dirs += 1\n",
    "\n",
    "            total_images += len([file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))])\n",
    "\n",
    "    return total_dirs, total_images\n",
    "\n",
    "num_dirs, num_images = count_images(training_images_dir)\n",
    "\n",
    "print(f\"There are {num_dirs} subdirectories in total\")\n",
    "print(f\"All subdirectories together contain {num_images} images in total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "training_images_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/images_set_filter/training_images\"\n",
    "\n",
    "def count_images(directory):\n",
    "    total_dirs = 0\n",
    "    total_images = 0\n",
    "\n",
    "\n",
    "    for entry in os.listdir(directory):\n",
    "        path = os.path.join(directory, entry)\n",
    "        if os.path.isdir(path):\n",
    "            total_dirs += 1\n",
    "\n",
    "            total_images += len([file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))])\n",
    "\n",
    "    return total_dirs, total_images\n",
    "\n",
    "num_dirs, num_images = count_images(training_images_dir)\n",
    "\n",
    "print(f\"There are {num_dirs} subdirectories in total\")\n",
    "print(f\"All subdirectories together contain {num_images} images in total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "test_images_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/images_set_filter/test_images\"\n",
    "\n",
    "def count_images(directory):\n",
    "    total_dirs = 0\n",
    "    total_images = 0\n",
    "\n",
    "\n",
    "    for entry in os.listdir(directory):\n",
    "        path = os.path.join(directory, entry)\n",
    "        if os.path.isdir(path):\n",
    "            total_dirs += 1\n",
    "\n",
    "            total_images += len([file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))])\n",
    "\n",
    "    return total_dirs, total_images\n",
    "\n",
    "num_dirs, num_images = count_images(test_images_dir)\n",
    "\n",
    "print(f\"There are {num_dirs} subdirectories in total\")\n",
    "print(f\"All subdirectories together contain {num_images} images in total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19848, 271, 201)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "path = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/Preprocessed/sub-02/preprocessed_meg_training.pkl\"\n",
    "\n",
    "\n",
    "with open(path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "meg_data = data['meg_data']\n",
    "# ch_names = data['ch_names']\n",
    "# times = data['times']\n",
    "# meg_data.shape\n",
    "meg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 271, 201)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "path = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/Preprocessed/sub-02/preprocessed_meg_test.pkl\"\n",
    "\n",
    "\n",
    "with open(path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "meg_data = data['meg_data']\n",
    "# ch_names = data['ch_names']\n",
    "# times = data['times']\n",
    "meg_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "meg_test_features = torch.load(\"/mnt/dataset0/ldy/Workspace/Thanos/data_preparing/MEG_ViT-H-14_features_test.pt\")\n",
    "meg_test_features['img_features'].shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meg_train_features = torch.load(\"/mnt/dataset0/ldy/Workspace/Thanos/data_preparing/MEG_ViT-H-14_features_train.pt\")\n",
    "meg_train_features['img_features'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mne\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from random import choice\n",
    "import pandas as pd\n",
    "\n",
    "# 目录设置\n",
    "base_fif_dir = \"/mnt/dataset0/ldy/datasets/meg_dataset/original_preprocessed/preprocessed\"\n",
    "source_image_dir = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Images/images\"\n",
    "meg_image_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/images_set_12\"\n",
    "output_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/Preprocessed\"\n",
    "subjects = [\n",
    "    # ('sub-01', 'preprocessed_P1-epo.fif'),\n",
    "    ('sub-02', 'preprocessed_P2-epo.fif'),\n",
    "    # ('sub-03', 'preprocessed_P3-epo.fif'),\n",
    "    # ('sub-04', 'preprocessed_P4-epo.fif'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "\n",
    "def load_and_crop_epochs(fif_file):\n",
    "    epochs = mne.read_epochs(fif_file, preload=True)\n",
    "    epochs.crop(tmin=0, tmax=1.0)\n",
    "    data_array = epochs.get_data()\n",
    "    event_ids = epochs.events[:, 2]\n",
    "    \n",
    "    # 获取非 999999 的索引\n",
    "    valid_indices = np.where(event_ids != 999999)[0]\n",
    "    \n",
    "    # 使用 valid_indices 筛选 data_array 和 event_ids\n",
    "    data_array = data_array[valid_indices]\n",
    "    event_ids = event_ids[valid_indices]\n",
    "\n",
    "    print(\"event_ids\", event_ids.shape)\n",
    "    \n",
    "    print(\"data_array\", data_array.shape)\n",
    "        \n",
    "    # 获取唯一的 event_ids 的索引\n",
    "    unique_indices = np.unique(event_ids, return_index=True)[1]\n",
    "    \n",
    "    # 根据 unique_indices 筛选 data_array 和 event_ids\n",
    "    data_array = data_array[unique_indices]\n",
    "    event_ids = event_ids[unique_indices]\n",
    "    \n",
    "    print(\"event_ids\", event_ids.shape)\n",
    "    print(\"unique_indices\", unique_indices.shape)\n",
    "    print(\"data_array\", data_array.shape)\n",
    "    \n",
    "    return data_array, event_ids, epochs.ch_names, epochs.times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subject_id, fif_filename in subjects:\n",
    "fif_file = os.path.join(base_fif_dir, 'preprocessed_P2-epo.fif')\n",
    "data_array, event_ids, ch_names, times = load_and_crop_epochs(fif_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array.shape  #1654×12+200×12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_ids[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(event_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import mne\n",
    "import numpy as np\n",
    "\n",
    "# 目录设置\n",
    "csv_img_file_path = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Metadata/Image-specific/image_paths.csv\"\n",
    "base_fif_dir = \"/mnt/dataset0/ldy/datasets/meg_dataset/original_preprocessed/preprocessed\"\n",
    "source_image_dir = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Images/\"\n",
    "training_images_dir = Path(\"/mnt/dataset0/ldy/datasets/THINGS_MEG/images_set/training_images\")\n",
    "test_images_dir = Path(\"/mnt/dataset0/ldy/datasets/THINGS_MEG/images_set/test_images\")\n",
    "\n",
    "train_dir = Path(\"/mnt/dataset0/ldy/datasets/THINGS_EEG/images_set/training_images\")\n",
    "test_dir = Path(\"/mnt/dataset0/ldy/datasets/THINGS_EEG/images_set/test_images\")\n",
    "\n",
    "subjects = [\n",
    "    # ('sub-01', 'preprocessed_P1-epo.fif'),\n",
    "    ('sub-02', 'preprocessed_P2-epo.fif'),\n",
    "    # ('sub-03', 'preprocessed_P3-epo.fif'),\n",
    "    # ('sub-04', 'preprocessed_P4-epo.fif'),\n",
    "]\n",
    "\n",
    "# 提取类别编号和真实标签\n",
    "def get_label_dict(image_set_dir):\n",
    "    label_dict = {}\n",
    "    for subdir in image_set_dir.iterdir():\n",
    "        if subdir.is_dir():\n",
    "            folder_name = subdir.name\n",
    "            index, label = folder_name.split(\"_\", 1)  # 使用下划线分割编号和真实标签\n",
    "            label_dict[label] = folder_name  # 保存真实标签和完整文件夹名\n",
    "    return label_dict\n",
    "\n",
    "# 获取训练和测试标签字典\n",
    "train_label_dict = get_label_dict(train_dir)\n",
    "test_label_dict = get_label_dict(test_dir)\n",
    "\n",
    "# 读取 CSV 文件并创建图像路径映射\n",
    "def load_image_paths(csv_path):\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    image_paths = df[0].tolist()  # 提取第一列作为图像路径\n",
    "    return image_paths\n",
    "\n",
    "# 加载图像路径\n",
    "image_paths = load_image_paths(csv_img_file_path)\n",
    "print(\"image_paths:\", image_paths)\n",
    "\n",
    "# 读取 FIF 文件并筛选数据\n",
    "def load_and_crop_epochs(fif_file):\n",
    "    epochs = mne.read_epochs(fif_file, preload=True)\n",
    "    epochs.crop(tmin=0, tmax=1.0)\n",
    "    data_array = epochs.get_data()\n",
    "    event_ids = epochs.events[:, 2]\n",
    "    \n",
    "    # 获取非 999999 的索引\n",
    "    valid_indices = np.where(event_ids != 999999)[0]\n",
    "    data_array = data_array[valid_indices]\n",
    "    event_ids = event_ids[valid_indices]\n",
    "    \n",
    "    # 获取唯一的 event_ids 的索引\n",
    "    unique_indices = np.unique(event_ids, return_index=True)[1]\n",
    "    data_array = data_array[unique_indices]\n",
    "    event_ids = event_ids[unique_indices]\n",
    "    \n",
    "    return data_array, event_ids\n",
    "\n",
    "# 复制图像到训练集和测试集\n",
    "def copy_images(subject_id, event_ids):\n",
    "    for event_id in event_ids:\n",
    "        # 获取图片的相对路径\n",
    "        image_path = image_paths[event_id - 1]\n",
    "        full_image_path = os.path.join(source_image_dir, image_path)\n",
    "        \n",
    "        # 确定目标路径（训练或测试）\n",
    "        image_label = image_path.split(\"/\")[1]  # 假设标签是文件夹名称\n",
    "        if image_label in train_label_dict:\n",
    "            target_dir = training_images_dir / train_label_dict[image_label]\n",
    "        elif image_label in test_label_dict:\n",
    "            target_dir = test_images_dir / test_label_dict[image_label]\n",
    "        else:\n",
    "            print(\"对不上\")\n",
    "\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 拷贝图片\n",
    "        target_image_path = target_dir / os.path.basename(full_image_path)\n",
    "        shutil.copy(full_image_path, target_image_path)\n",
    "\n",
    "# 执行流程\n",
    "for subject_id, fif_filename in subjects:\n",
    "    fif_file = os.path.join(base_fif_dir, fif_filename)\n",
    "    _, event_ids = load_and_crop_epochs(fif_file)\n",
    "    copy_images(subject_id, event_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mne\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from random import choice\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_and_crop_epochs(fif_file):\n",
    "    epochs = mne.read_epochs(fif_file, preload=True)\n",
    "    epochs.crop(tmin=0, tmax=1.0)\n",
    "    data_array = epochs.get_data()\n",
    "    event_ids = epochs.events[:, 2]\n",
    "    \n",
    "    # 获取非 999999 的索引\n",
    "    valid_indices = np.where(event_ids != 999999)[0]\n",
    "    \n",
    "    # 使用 valid_indices 筛选 data_array 和 event_ids\n",
    "    data_array = data_array[valid_indices]\n",
    "    event_ids = event_ids[valid_indices]\n",
    "\n",
    "    print(\"event_ids\", event_ids.shape)    \n",
    "    print(\"data_array\", data_array.shape)\n",
    "        \n",
    "    # 获取唯一的 event_ids 的索引\n",
    "    unique_indices = np.unique(event_ids, return_index=True)[1]\n",
    "    \n",
    "    # 根据 unique_indices 筛选 data_array 和 event_ids\n",
    "    data_array = data_array[unique_indices]\n",
    "    event_ids = event_ids[unique_indices]\n",
    "    \n",
    "    print(\"event_ids\", event_ids.shape)\n",
    "    print(\"unique_indices\", unique_indices.shape)\n",
    "    print(\"data_array\", data_array.shape)\n",
    "    \n",
    "    return data_array, event_ids, epochs.ch_names, epochs.times\n",
    "\n",
    "def build_category_prefix_mapping(meg_image_dir):\n",
    "    category_prefix_mapping = {}\n",
    "    for split in [\"training_images\", \"test_images\"]:\n",
    "        split_dir = os.path.join(meg_image_dir, split)\n",
    "        if os.path.isdir(split_dir):\n",
    "            for category_with_prefix in os.listdir(split_dir):\n",
    "                prefix, category = category_with_prefix.split(\"_\", 1)\n",
    "                category_prefix_mapping[category] = category_with_prefix\n",
    "    return category_prefix_mapping\n",
    "\n",
    "def get_full_image_event_mapping(base_dir, category_prefix_mapping):\n",
    "    event_id = 1\n",
    "    full_image_event_mapping = {}\n",
    "    categories = sorted(os.listdir(base_dir))\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        if os.path.isdir(category_path) and category in category_prefix_mapping:\n",
    "            category_with_prefix = category_prefix_mapping[category]\n",
    "            images = sorted(os.listdir(category_path))\n",
    "            for img in images:\n",
    "                full_image_event_mapping[event_id] = (category_with_prefix, img)\n",
    "                event_id += 1\n",
    "    return full_image_event_mapping\n",
    "\n",
    "\n",
    "def find_unmatched_keys(event_ids, image_event_mapping):\n",
    "    return [key for key in image_event_mapping.keys() if key not in event_ids]\n",
    "\n",
    "def insert_unmatched_events(unmatched_keys, image_event_mapping, data_array, event_ids):\n",
    "    filled_data = []\n",
    "    filled_event_ids = []\n",
    "    insert_indices = []\n",
    "\n",
    "    for key in unmatched_keys:\n",
    "        category_with_prefix, img = image_event_mapping[key]\n",
    "        same_category_indices = [\n",
    "            i for i, event_id in enumerate(event_ids)\n",
    "            if event_id in image_event_mapping and image_event_mapping[event_id][0] == category_with_prefix\n",
    "        ]\n",
    "        \n",
    "        if same_category_indices:\n",
    "            sample_index = choice(same_category_indices)\n",
    "            filled_data.append(data_array[sample_index][np.newaxis, :].copy())\n",
    "            filled_event_ids.append(key)\n",
    "            insert_indices.append(list(image_event_mapping.keys()).index(key))\n",
    "            \n",
    "        else:\n",
    "            print(f\"警告：无法为类别 {category_with_prefix} 填充数据\")\n",
    "\n",
    "    filled_data = np.vstack(filled_data) if filled_data else np.empty((0, data_array.shape[1], data_array.shape[2]))\n",
    "\n",
    "    return filled_data, filled_event_ids, insert_indices\n",
    "\n",
    "\n",
    "\n",
    "# 目录设置\n",
    "csv_img_file_path = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Metadata/Image-specific/image_paths.csv\"\n",
    "base_fif_dir = \"/mnt/dataset0/ldy/datasets/meg_dataset/original_preprocessed/preprocessed\"\n",
    "source_image_dir = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Images/images\"\n",
    "meg_image_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/images_set\"\n",
    "output_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/Preprocessed\"\n",
    "subjects = [\n",
    "    # ('sub-01', 'preprocessed_P1-epo.fif'),\n",
    "    ('sub-02', 'preprocessed_P2-epo.fif'),\n",
    "    # ('sub-03', 'preprocessed_P3-epo.fif'),\n",
    "    # ('sub-04', 'preprocessed_P4-epo.fif'),\n",
    "]\n",
    "\n",
    "category_prefix_mapping = build_category_prefix_mapping(meg_image_dir)\n",
    "full_image_event_mapping = get_full_image_event_mapping(source_image_dir, category_prefix_mapping)\n",
    "\n",
    "train_images_expected = set()\n",
    "test_images_expected = set()\n",
    "\n",
    "for category_with_prefix in category_prefix_mapping.values():\n",
    "    train_dir = os.path.join(meg_image_dir, \"training_images\", category_with_prefix)\n",
    "    test_dir = os.path.join(meg_image_dir, \"test_images\", category_with_prefix)\n",
    "\n",
    "    if os.path.exists(train_dir):\n",
    "        train_images_expected.update(\n",
    "            [f\"{category_with_prefix}/{img}\" for img in sorted(os.listdir(train_dir))]\n",
    "        )\n",
    "    if os.path.exists(test_dir):\n",
    "        test_images_expected.update(\n",
    "            [f\"{category_with_prefix}/{img}\" for img in sorted(os.listdir(test_dir))]\n",
    "        )\n",
    "\n",
    "image_event_mapping_train = {}\n",
    "image_event_mapping_test = {}\n",
    "\n",
    "for event_id, (category_with_prefix, img) in full_image_event_mapping.items():\n",
    "    train_path = os.path.join(meg_image_dir, \"training_images\", category_with_prefix, img)\n",
    "    test_path = os.path.join(meg_image_dir, \"test_images\", category_with_prefix, img)\n",
    "    if os.path.exists(train_path):\n",
    "        image_event_mapping_train[event_id] = (category_with_prefix, img)\n",
    "    elif os.path.exists(test_path):\n",
    "        image_event_mapping_test[event_id] = (category_with_prefix, img)\n",
    "\n",
    "# image_event_mapping_train\n",
    "\n",
    "# 将字典转换为 DataFrame，便于查看\n",
    "# image_event_mapping_train_df = pd.DataFrame.from_dict(image_event_mapping_train, orient='index', columns=['Category_Prefix', 'Image'])\n",
    "\n",
    "# # 保存为 CSV 文件\n",
    "# train_file_path = \"image_event_mapping_train.csv\"\n",
    "# image_event_mapping_train_df.to_csv(train_file_path, index_label=\"Event_ID\")\n",
    "\n",
    "# print(f\"文件已保存到: {train_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# image_event_mapping_test\n",
    "# # 将字典转换为 DataFrame，便于查看\n",
    "# image_event_mapping_test_df = pd.DataFrame.from_dict(image_event_mapping_test, orient='index', columns=['Category_Prefix', 'Image'])\n",
    "\n",
    "# # 保存为 CSV 文件\n",
    "# file_path = \"image_event_mapping_test.csv\"\n",
    "# image_event_mapping_test_df.to_csv(file_path, index_label=\"Event_ID\")\n",
    "\n",
    "# print(f\"文件已保存到: {file_path}\")\n",
    "\n",
    "for subject_id, fif_filename in subjects:\n",
    "    fif_file = os.path.join(base_fif_dir, fif_filename)\n",
    "    data_array, event_ids, ch_names, times = load_and_crop_epochs(fif_file)\n",
    "    \n",
    "    train_array = []\n",
    "    test_array = []\n",
    "    \n",
    "    for i, event_id in enumerate(event_ids):\n",
    "        if event_id in image_event_mapping_train:\n",
    "            train_array.append(data_array[i])\n",
    "        elif event_id in image_event_mapping_test:\n",
    "            test_array.append(data_array[i])\n",
    "        else:\n",
    "            print(\"event_id 多余\", event_id)\n",
    "    train_array = np.array(train_array)\n",
    "    test_array = np.array(test_array)\n",
    "    \n",
    "    subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "    os.makedirs(subject_output_dir, exist_ok=True)\n",
    "    \n",
    "    unmatched_train_keys = find_unmatched_keys(event_ids, image_event_mapping_train)\n",
    "    unmatched_test_keys = find_unmatched_keys(event_ids, image_event_mapping_test)\n",
    "    \n",
    "    # filled_train_data, filled_train_ids, train_insert_indices = insert_unmatched_events(\n",
    "    #     unmatched_train_keys, image_event_mapping_train, data_array, event_ids)\n",
    "    # filled_test_data, filled_test_ids, test_insert_indices = insert_unmatched_events(\n",
    "    #     unmatched_test_keys, image_event_mapping_test, data_array, event_ids)\n",
    "    \n",
    "    # Sort indices in descending order to avoid index shift during insertion\n",
    "    # train_insert_indices = sorted(train_insert_indices)\n",
    "    # test_insert_indices = sorted(test_insert_indices)\n",
    "\n",
    "    # Insert filled data from the end of the array, ensuring indices stay consistent\n",
    "    # if filled_train_data.size > 0:\n",
    "    #     for data, idx in zip(filled_train_data, train_insert_indices):\n",
    "    #         print(\"insert train idx\", idx)\n",
    "    #         train_array = np.insert(train_array, idx, data, axis=0)\n",
    "\n",
    "    # if filled_test_data.size > 0:\n",
    "    #     for data, idx in zip(filled_test_data, test_insert_indices):\n",
    "    #         print(\"insert test idx\", idx)\n",
    "    #         test_array = np.insert(test_array, idx, data, axis=0)\n",
    "\n",
    "    save_data({'meg_data': train_array, 'ch_names': ch_names, 'times': times},\n",
    "              os.path.join(subject_output_dir, \"preprocessed_meg_training.pkl\"))\n",
    "    save_data({'meg_data': test_array, 'ch_names': ch_names, 'times': times},\n",
    "              os.path.join(subject_output_dir, \"preprocessed_meg_test.pkl\"))\n",
    "\n",
    "    print(f\"\\nUnmatched keys in training set for {subject_id}:\")\n",
    "    print(unmatched_train_keys)\n",
    "    \n",
    "    print(f\"\\nUnmatched keys in test set for {subject_id}:\")\n",
    "    print(unmatched_test_keys)\n",
    "\n",
    "print(\"处理完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mne\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_and_crop_epochs(fif_file):\n",
    "    epochs = mne.read_epochs(fif_file, preload=True)\n",
    "    epochs.crop(tmin=0, tmax=1.0)\n",
    "    data_array = epochs.get_data()\n",
    "    event_ids = epochs.events[:, 2]\n",
    "    \n",
    "    # 获取非 999999 的索引\n",
    "    valid_indices = np.where(event_ids != 999999)[0]\n",
    "    data_array = data_array[valid_indices]\n",
    "    event_ids = event_ids[valid_indices]\n",
    "    \n",
    "    # 获取唯一的 event_ids 的索引\n",
    "    unique_indices = np.unique(event_ids, return_index=True)[1]\n",
    "    data_array = data_array[unique_indices]\n",
    "    event_ids = event_ids[unique_indices]\n",
    "    \n",
    "    return data_array, event_ids, epochs.ch_names, epochs.times\n",
    "\n",
    "def build_category_prefix_mapping(meg_image_dir):\n",
    "    category_prefix_mapping = {}\n",
    "    for split in [\"training_images\", \"test_images\"]:\n",
    "        split_dir = os.path.join(meg_image_dir, split)\n",
    "        if os.path.isdir(split_dir):\n",
    "            for category_with_prefix in os.listdir(split_dir):\n",
    "                prefix, category = category_with_prefix.split(\"_\", 1)\n",
    "                category_prefix_mapping[category] = category_with_prefix\n",
    "    return category_prefix_mapping\n",
    "\n",
    "def get_full_image_event_mapping(csv_img_file_path, category_prefix_mapping):\n",
    "    full_image_event_mapping = {}\n",
    "    \n",
    "    # 使用 CSV 文件中图像路径顺序创建 event_id 映射\n",
    "    df = pd.read_csv(csv_img_file_path, header=None)\n",
    "    for event_id, row in enumerate(df[0], start=1):\n",
    "        image_path = row\n",
    "        category = image_path.split(\"/\")[1]  # 假设类别是文件夹名称\n",
    "        if category in category_prefix_mapping:\n",
    "            category_with_prefix = category_prefix_mapping[category]\n",
    "            img = os.path.basename(image_path)\n",
    "            full_image_event_mapping[event_id] = (category_with_prefix, img)\n",
    "            \n",
    "    return full_image_event_mapping\n",
    "\n",
    "def find_unmatched_keys(event_ids, image_event_mapping):\n",
    "    return [key for key in image_event_mapping.keys() if key not in event_ids]\n",
    "\n",
    "def insert_unmatched_events(unmatched_keys, image_event_mapping, data_array, event_ids):\n",
    "    filled_data = []\n",
    "    filled_event_ids = []\n",
    "    insert_indices = []\n",
    "\n",
    "    for key in unmatched_keys:\n",
    "        category_with_prefix, img = image_event_mapping[key]\n",
    "        same_category_indices = [\n",
    "            i for i, event_id in enumerate(event_ids)\n",
    "            if event_id in image_event_mapping and image_event_mapping[event_id][0] == category_with_prefix\n",
    "        ]\n",
    "        \n",
    "        if same_category_indices:\n",
    "            sample_index = choice(same_category_indices)\n",
    "            filled_data.append(data_array[sample_index][np.newaxis, :].copy())\n",
    "            filled_event_ids.append(key)\n",
    "            insert_indices.append(list(image_event_mapping.keys()).index(key))\n",
    "            \n",
    "        else:\n",
    "            print(f\"警告：无法为类别 {category_with_prefix} 填充数据\")\n",
    "\n",
    "    filled_data = np.vstack(filled_data) if filled_data else np.empty((0, data_array.shape[1], data_array.shape[2]))\n",
    "\n",
    "    return filled_data, filled_event_ids, insert_indices\n",
    "\n",
    "# 目录设置\n",
    "csv_img_file_path = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Metadata/Image-specific/image_paths.csv\"\n",
    "base_fif_dir = \"/mnt/dataset0/ldy/datasets/meg_dataset/original_preprocessed/preprocessed\"\n",
    "source_image_dir = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Images/images\"\n",
    "meg_image_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/images_set\"\n",
    "output_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/Preprocessed\"\n",
    "subjects = [\n",
    "    ('sub-02', 'preprocessed_P2-epo.fif'),\n",
    "]\n",
    "\n",
    "category_prefix_mapping = build_category_prefix_mapping(meg_image_dir)\n",
    "full_image_event_mapping = get_full_image_event_mapping(csv_img_file_path, category_prefix_mapping)\n",
    "\n",
    "image_event_mapping_train = {}\n",
    "image_event_mapping_test = {}\n",
    "\n",
    "for event_id, (category_with_prefix, img) in full_image_event_mapping.items():\n",
    "    train_path = os.path.join(meg_image_dir, \"training_images\", category_with_prefix, img)\n",
    "    test_path = os.path.join(meg_image_dir, \"test_images\", category_with_prefix, img)\n",
    "    if os.path.exists(train_path):\n",
    "        image_event_mapping_train[event_id] = (category_with_prefix, img)\n",
    "    elif os.path.exists(test_path):\n",
    "        image_event_mapping_test[event_id] = (category_with_prefix, img)\n",
    "\n",
    "for subject_id, fif_filename in subjects:\n",
    "    fif_file = os.path.join(base_fif_dir, fif_filename)\n",
    "    data_array, event_ids, ch_names, times = load_and_crop_epochs(fif_file)\n",
    "    \n",
    "    train_array = []\n",
    "    test_array = []\n",
    "    \n",
    "    for i, event_id in enumerate(event_ids):\n",
    "        if event_id in image_event_mapping_train:\n",
    "            train_array.append(data_array[i])\n",
    "        elif event_id in image_event_mapping_test:\n",
    "            test_array.append(data_array[i])\n",
    "        else:\n",
    "            print(\"event_id 多余\", event_id)\n",
    "    train_array = np.array(train_array)\n",
    "    test_array = np.array(test_array)\n",
    "    \n",
    "    subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "    os.makedirs(subject_output_dir, exist_ok=True)\n",
    "    \n",
    "    unmatched_train_keys = find_unmatched_keys(event_ids, image_event_mapping_train)\n",
    "    unmatched_test_keys = find_unmatched_keys(event_ids, image_event_mapping_test)\n",
    "    \n",
    "    save_data({'meg_data': train_array, 'ch_names': ch_names, 'times': times},\n",
    "              os.path.join(subject_output_dir, \"preprocessed_meg_training.pkl\"))\n",
    "    save_data({'meg_data': test_array, 'ch_names': ch_names, 'times': times},\n",
    "              os.path.join(subject_output_dir, \"preprocessed_meg_test.pkl\"))\n",
    "\n",
    "    print(f\"\\nUnmatched keys in training set for {subject_id}:\")\n",
    "    print(unmatched_train_keys)\n",
    "    \n",
    "    print(f\"\\nUnmatched keys in test set for {subject_id}:\")\n",
    "    print(unmatched_test_keys)\n",
    "\n",
    "print(\"处理完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mne\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_and_crop_epochs(fif_file):\n",
    "    epochs = mne.read_epochs(fif_file, preload=True)\n",
    "    epochs.crop(tmin=0, tmax=1.0)\n",
    "    data_array = epochs.get_data()\n",
    "    event_ids = epochs.events[:, 2]\n",
    "    \n",
    "    # 获取非 999999 的索引\n",
    "    valid_indices = np.where(event_ids != 999999)[0]\n",
    "    data_array = data_array[valid_indices]\n",
    "    event_ids = event_ids[valid_indices]\n",
    "    \n",
    "    # 获取唯一的 event_ids 的索引\n",
    "    unique_indices = np.unique(event_ids, return_index=True)[1]\n",
    "    data_array = data_array[unique_indices]\n",
    "    event_ids = event_ids[unique_indices]\n",
    "    \n",
    "    return data_array, event_ids, epochs.ch_names, epochs.times\n",
    "\n",
    "def build_category_prefix_mapping(meg_image_dir):\n",
    "    category_prefix_mapping = {}\n",
    "    for split in [\"training_images\", \"test_images\"]:\n",
    "        split_dir = os.path.join(meg_image_dir, split)\n",
    "        if os.path.isdir(split_dir):\n",
    "            for category_with_prefix in os.listdir(split_dir):\n",
    "                prefix, category = category_with_prefix.split(\"_\", 1)\n",
    "                category_prefix_mapping[category] = category_with_prefix\n",
    "    return category_prefix_mapping\n",
    "\n",
    "def get_full_image_event_mapping(csv_img_file_path, category_prefix_mapping):\n",
    "    full_image_event_mapping = {}\n",
    "    \n",
    "    # 使用 CSV 文件中图像路径顺序创建 event_id 映射\n",
    "    df = pd.read_csv(csv_img_file_path, header=None)\n",
    "    for event_id, row in enumerate(df[0], start=1):\n",
    "        image_path = row\n",
    "        category = image_path.split(\"/\")[1]  # 假设类别是文件夹名称\n",
    "        if category in category_prefix_mapping:\n",
    "            category_with_prefix = category_prefix_mapping[category]\n",
    "            img = os.path.basename(image_path)\n",
    "            full_image_event_mapping[event_id] = (category_with_prefix, img)\n",
    "            \n",
    "    return full_image_event_mapping\n",
    "\n",
    "def filter_and_copy_images(image_event_mapping, data_array, event_ids, output_array, output_dir, category_limit=12):\n",
    "    image_count = defaultdict(int)  # 用于记录每个类别的图像数量\n",
    "    incomplete_categories = []  # 用于记录不满足12张图像的类别\n",
    "    filtered_event_ids = []  # 用于存储过滤后的 event_id\n",
    "    \n",
    "    # 新建输出目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i, event_id in enumerate(event_ids):\n",
    "        if event_id not in image_event_mapping:\n",
    "            continue\n",
    "        category_with_prefix, img = image_event_mapping[event_id]\n",
    "        \n",
    "        # 检查每个类别的图像数量是否已达到限制\n",
    "        if image_count[category_with_prefix] < category_limit:\n",
    "            output_array.append(data_array[i])  # 保留该样本\n",
    "            filtered_event_ids.append(event_id)  # 将符合条件的 event_id 添加到过滤列表\n",
    "            image_count[category_with_prefix] += 1\n",
    "            \n",
    "            # 提取不带前缀的类别名\n",
    "            # category = category_with_prefix.split(\"_\", 1)[1]  # 提取不带前缀的类别名\n",
    "            \n",
    "            # 构建源路径（使用不带前缀的类别名）和目标路径\n",
    "            # source_path = os.path.join(source_image_dir, \"images\", category, img)  # 使用不带前缀的类别名\n",
    "            # category_output_dir = os.path.join(output_dir, category_with_prefix)  # 目标路径使用带前缀的类别名\n",
    "            # os.makedirs(category_output_dir, exist_ok=True)\n",
    "            # target_path = os.path.join(category_output_dir, img)\n",
    "            \n",
    "            # 拷贝文件\n",
    "            # shutil.copyfile(source_path, target_path)\n",
    "    \n",
    "    # 检查是否存在不满足12张图像的类别\n",
    "    for category, count in image_count.items():\n",
    "        if count < category_limit:\n",
    "            incomplete_categories.append((category, count))\n",
    "    \n",
    "    return incomplete_categories, filtered_event_ids\n",
    "\n",
    "# 目录设置\n",
    "csv_img_file_path = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Metadata/Image-specific/image_paths.csv\"\n",
    "base_fif_dir = \"/mnt/dataset0/ldy/datasets/meg_dataset/original_preprocessed/preprocessed\"\n",
    "source_image_dir = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Images/\"\n",
    "meg_image_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/images_set\"\n",
    "output_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/Preprocessed\"\n",
    "filtered_image_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/images_set_filter\"\n",
    "subjects = [\n",
    "    ('sub-02', 'preprocessed_P2-epo.fif'),\n",
    "]\n",
    "\n",
    "category_prefix_mapping = build_category_prefix_mapping(meg_image_dir)\n",
    "full_image_event_mapping = get_full_image_event_mapping(csv_img_file_path, category_prefix_mapping)\n",
    "\n",
    "image_event_mapping_train = {}\n",
    "image_event_mapping_test = {}\n",
    "\n",
    "for event_id, (category_with_prefix, img) in full_image_event_mapping.items():\n",
    "    train_path = os.path.join(meg_image_dir, \"training_images\", category_with_prefix, img)\n",
    "    test_path = os.path.join(meg_image_dir, \"test_images\", category_with_prefix, img)\n",
    "    if os.path.exists(train_path):\n",
    "        image_event_mapping_train[event_id] = (category_with_prefix, img)\n",
    "    elif os.path.exists(test_path):\n",
    "        image_event_mapping_test[event_id] = (category_with_prefix, img)\n",
    "\n",
    "# 主处理代码\n",
    "filtered_train_event_ids = []\n",
    "filtered_test_event_ids = []\n",
    "\n",
    "for subject_id, fif_filename in subjects:\n",
    "    fif_file = os.path.join(base_fif_dir, fif_filename)\n",
    "    data_array, event_ids, ch_names, times = load_and_crop_epochs(fif_file)\n",
    "    print(\"event_ids\", event_ids)\n",
    "    train_array = []\n",
    "    test_array = []\n",
    "    \n",
    "    # 过滤训练集和测试集，保留每个类别的前12张图像并获取过滤后的 event_ids\n",
    "    incomplete_train_categories, filtered_train_event_ids = filter_and_copy_images(\n",
    "        image_event_mapping_train, data_array, event_ids, train_array, os.path.join(filtered_image_dir, \"training_images\"))\n",
    "    incomplete_test_categories, filtered_test_event_ids = filter_and_copy_images(\n",
    "        image_event_mapping_test, data_array, event_ids, test_array, os.path.join(filtered_image_dir, \"test_images\"))\n",
    "\n",
    "    # 将结果转换为 numpy 数组\n",
    "    train_array = np.array(train_array)\n",
    "    test_array = np.array(test_array)\n",
    "    \n",
    "    subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "    os.makedirs(subject_output_dir, exist_ok=True)\n",
    "    \n",
    "    save_data({'meg_data': train_array, 'ch_names': ch_names, 'times': times},\n",
    "              os.path.join(subject_output_dir, \"preprocessed_meg_training.pkl\"))\n",
    "    save_data({'meg_data': test_array, 'ch_names': ch_names, 'times': times},\n",
    "              os.path.join(subject_output_dir, \"preprocessed_meg_test.pkl\"))\n",
    "\n",
    "    # 输出不满足12张图像的类别\n",
    "    if incomplete_train_categories:\n",
    "        print(f\"\\n训练集中不满足12张图像的类别（数量不足） for {subject_id}:\")\n",
    "        for category, count in incomplete_train_categories:\n",
    "            print(f\"  类别 '{category}': 仅 {count} 张图像\")\n",
    "    \n",
    "    if incomplete_test_categories:\n",
    "        print(f\"\\n测试集中不满足12张图像的类别（数量不足） for {subject_id}:\")\n",
    "        for category, count in incomplete_test_categories:\n",
    "            print(f\"  类别 '{category}': 仅 {count} 张图像\")\n",
    "\n",
    "# 处理完成后，直接使用 filtered_train_event_ids 和 filtered_test_event_ids\n",
    "print(\"处理完成！\")\n",
    "print(\"过滤后的训练集的 event_ids:\", filtered_train_event_ids)\n",
    "print(\"过滤后的测试集的 event_ids:\", filtered_test_event_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用epochs整体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /mnt/dataset0/ldy/datasets/meg_dataset/original_preprocessed/preprocessed/preprocessed_P2-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -100.00 ...    1300.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Reading /mnt/dataset0/ldy/datasets/meg_dataset/original_preprocessed/preprocessed/preprocessed_P2-epo-1.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -100.00 ...    1300.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Reading /mnt/dataset0/ldy/datasets/meg_dataset/original_preprocessed/preprocessed/preprocessed_P2-epo-2.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -100.00 ...    1300.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Reading /mnt/dataset0/ldy/datasets/meg_dataset/original_preprocessed/preprocessed/preprocessed_P2-epo-3.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -100.00 ...    1300.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 18 columns\n",
      "27048 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mne\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "\n",
    "def load_and_crop_epochs(fif_file):\n",
    "    epochs = mne.read_epochs(fif_file, preload=True)\n",
    "    epochs.crop(tmin=0, tmax=1.0)\n",
    "    \n",
    "    sorted_indices = np.argsort(epochs.events[:, 2])\n",
    "    epochs = epochs[sorted_indices]\n",
    "    \n",
    "    filtered_epochs = epochs[epochs.events[:, 2] != 999999]\n",
    "    \n",
    "    \n",
    "    # 根据事件进行去重处理\n",
    "    unique_events, unique_indices = np.unique(filtered_epochs.events[:, 2], return_index=True)\n",
    "    unique_filtered_epochs = filtered_epochs[unique_indices]    \n",
    "    \n",
    "    return unique_filtered_epochs\n",
    "\n",
    "\n",
    "# 读取 meg 类别标签文件\n",
    "csv_file_path = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Metadata/Concept-specific/image_concept_index.csv\"\n",
    "concept_df = pd.read_csv(csv_file_path, header=None, names=['Category_Label'])\n",
    "\n",
    "subjects = [\n",
    "    ('sub-02', 'preprocessed_P2-epo.fif'),\n",
    "]\n",
    "base_fif_dir = \"/mnt/dataset0/ldy/datasets/meg_dataset/original_preprocessed/preprocessed\"\n",
    "\n",
    "for subject_id, fif_filename in subjects:\n",
    "    fif_file = os.path.join(base_fif_dir, fif_filename)\n",
    "    epochs = load_and_crop_epochs(fif_file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs.times.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     2,     3, ..., 26104, 26105, 26106], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs.events[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22448"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs.events[2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别在过滤后的 MEG 数据中样本数量:\n",
      "类别 1: 12 个样本\n",
      "类别 2: 12 个样本\n",
      "类别 3: 12 个样本\n",
      "类别 4: 12 个样本\n",
      "类别 5: 12 个样本\n",
      "类别 6: 12 个样本\n",
      "类别 7: 12 个样本\n",
      "类别 8: 12 个样本\n",
      "类别 9: 12 个样本\n",
      "类别 10: 13 个样本\n",
      "类别 11: 12 个样本\n",
      "类别 12: 12 个样本\n",
      "类别 13: 13 个样本\n",
      "类别 14: 12 个样本\n",
      "类别 15: 12 个样本\n",
      "类别 16: 12 个样本\n",
      "类别 17: 13 个样本\n",
      "类别 18: 13 个样本\n",
      "类别 19: 12 个样本\n",
      "类别 20: 12 个样本\n",
      "类别 21: 12 个样本\n",
      "类别 22: 12 个样本\n",
      "类别 23: 12 个样本\n",
      "类别 24: 12 个样本\n",
      "类别 25: 12 个样本\n",
      "类别 26: 12 个样本\n",
      "类别 27: 12 个样本\n",
      "类别 28: 12 个样本\n",
      "类别 29: 12 个样本\n",
      "类别 30: 12 个样本\n",
      "类别 31: 12 个样本\n",
      "类别 32: 12 个样本\n",
      "类别 33: 12 个样本\n",
      "类别 34: 12 个样本\n",
      "类别 35: 12 个样本\n",
      "类别 36: 12 个样本\n",
      "类别 37: 12 个样本\n",
      "类别 38: 12 个样本\n",
      "类别 39: 12 个样本\n",
      "类别 40: 12 个样本\n",
      "类别 41: 12 个样本\n",
      "类别 42: 13 个样本\n",
      "类别 43: 12 个样本\n",
      "类别 44: 13 个样本\n",
      "类别 45: 12 个样本\n",
      "类别 46: 13 个样本\n",
      "类别 47: 13 个样本\n",
      "类别 48: 12 个样本\n",
      "类别 49: 12 个样本\n",
      "类别 50: 12 个样本\n",
      "类别 51: 12 个样本\n",
      "类别 52: 12 个样本\n",
      "类别 53: 12 个样本\n",
      "类别 54: 12 个样本\n",
      "类别 55: 13 个样本\n",
      "类别 56: 13 个样本\n",
      "类别 57: 12 个样本\n",
      "类别 58: 12 个样本\n",
      "类别 59: 12 个样本\n",
      "类别 60: 13 个样本\n",
      "类别 61: 13 个样本\n",
      "类别 62: 12 个样本\n",
      "类别 63: 13 个样本\n",
      "类别 64: 13 个样本\n",
      "类别 65: 12 个样本\n",
      "类别 66: 12 个样本\n",
      "类别 67: 12 个样本\n",
      "类别 68: 12 个样本\n",
      "类别 69: 12 个样本\n",
      "类别 70: 12 个样本\n",
      "类别 71: 12 个样本\n",
      "类别 72: 12 个样本\n",
      "类别 73: 12 个样本\n",
      "类别 74: 12 个样本\n",
      "类别 75: 12 个样本\n",
      "类别 76: 12 个样本\n",
      "类别 77: 12 个样本\n",
      "类别 78: 12 个样本\n",
      "类别 79: 12 个样本\n",
      "类别 80: 12 个样本\n",
      "类别 81: 12 个样本\n",
      "类别 82: 12 个样本\n",
      "类别 83: 12 个样本\n",
      "类别 84: 12 个样本\n",
      "类别 85: 12 个样本\n",
      "类别 86: 13 个样本\n",
      "类别 87: 12 个样本\n",
      "类别 88: 12 个样本\n",
      "类别 89: 12 个样本\n",
      "类别 90: 12 个样本\n",
      "类别 91: 12 个样本\n",
      "类别 92: 12 个样本\n",
      "类别 93: 12 个样本\n",
      "类别 94: 12 个样本\n",
      "类别 95: 12 个样本\n",
      "类别 96: 12 个样本\n",
      "类别 97: 12 个样本\n",
      "类别 98: 13 个样本\n",
      "类别 99: 12 个样本\n",
      "类别 100: 13 个样本\n",
      "类别 101: 12 个样本\n",
      "类别 102: 12 个样本\n",
      "类别 103: 13 个样本\n",
      "类别 104: 12 个样本\n",
      "类别 105: 12 个样本\n",
      "类别 106: 13 个样本\n",
      "类别 107: 12 个样本\n",
      "类别 108: 13 个样本\n",
      "类别 109: 13 个样本\n",
      "类别 110: 12 个样本\n",
      "类别 111: 12 个样本\n",
      "类别 112: 13 个样本\n",
      "类别 113: 12 个样本\n",
      "类别 114: 13 个样本\n",
      "类别 115: 12 个样本\n",
      "类别 116: 12 个样本\n",
      "类别 117: 13 个样本\n",
      "类别 118: 12 个样本\n",
      "类别 119: 12 个样本\n",
      "类别 120: 12 个样本\n",
      "类别 121: 13 个样本\n",
      "类别 122: 12 个样本\n",
      "类别 123: 12 个样本\n",
      "类别 124: 13 个样本\n",
      "类别 125: 12 个样本\n",
      "类别 126: 12 个样本\n",
      "类别 127: 12 个样本\n",
      "类别 128: 12 个样本\n",
      "类别 129: 12 个样本\n",
      "类别 130: 12 个样本\n",
      "类别 131: 12 个样本\n",
      "类别 132: 12 个样本\n",
      "类别 133: 12 个样本\n",
      "类别 134: 12 个样本\n",
      "类别 135: 12 个样本\n",
      "类别 136: 12 个样本\n",
      "类别 137: 12 个样本\n",
      "类别 138: 12 个样本\n",
      "类别 139: 12 个样本\n",
      "类别 140: 12 个样本\n",
      "类别 141: 13 个样本\n",
      "类别 142: 12 个样本\n",
      "类别 143: 12 个样本\n",
      "类别 144: 12 个样本\n",
      "类别 145: 12 个样本\n",
      "类别 146: 12 个样本\n",
      "类别 147: 12 个样本\n",
      "类别 148: 12 个样本\n",
      "类别 149: 13 个样本\n",
      "类别 150: 12 个样本\n",
      "类别 151: 13 个样本\n",
      "类别 152: 12 个样本\n",
      "类别 153: 12 个样本\n",
      "类别 154: 12 个样本\n",
      "类别 155: 13 个样本\n",
      "类别 156: 13 个样本\n",
      "类别 157: 12 个样本\n",
      "类别 158: 12 个样本\n",
      "类别 159: 12 个样本\n",
      "类别 160: 12 个样本\n",
      "类别 161: 12 个样本\n",
      "类别 162: 12 个样本\n",
      "类别 163: 12 个样本\n",
      "类别 164: 12 个样本\n",
      "类别 165: 12 个样本\n",
      "类别 166: 12 个样本\n",
      "类别 167: 12 个样本\n",
      "类别 168: 12 个样本\n",
      "类别 169: 12 个样本\n",
      "类别 170: 12 个样本\n",
      "类别 171: 12 个样本\n",
      "类别 172: 12 个样本\n",
      "类别 173: 12 个样本\n",
      "类别 174: 12 个样本\n",
      "类别 175: 12 个样本\n",
      "类别 176: 12 个样本\n",
      "类别 177: 12 个样本\n",
      "类别 178: 12 个样本\n",
      "类别 179: 12 个样本\n",
      "类别 180: 12 个样本\n",
      "类别 181: 12 个样本\n",
      "类别 182: 12 个样本\n",
      "类别 183: 13 个样本\n",
      "类别 184: 12 个样本\n",
      "类别 185: 12 个样本\n",
      "类别 186: 12 个样本\n",
      "类别 187: 13 个样本\n",
      "类别 188: 12 个样本\n",
      "类别 189: 12 个样本\n",
      "类别 190: 12 个样本\n",
      "类别 191: 12 个样本\n",
      "类别 192: 12 个样本\n",
      "类别 193: 12 个样本\n",
      "类别 194: 12 个样本\n",
      "类别 195: 12 个样本\n",
      "类别 196: 12 个样本\n",
      "类别 197: 12 个样本\n",
      "类别 198: 12 个样本\n",
      "类别 199: 12 个样本\n",
      "类别 200: 12 个样本\n",
      "类别 201: 12 个样本\n",
      "类别 202: 13 个样本\n",
      "类别 203: 12 个样本\n",
      "类别 204: 12 个样本\n",
      "类别 205: 13 个样本\n",
      "类别 206: 12 个样本\n",
      "类别 207: 12 个样本\n",
      "类别 208: 12 个样本\n",
      "类别 209: 12 个样本\n",
      "类别 210: 12 个样本\n",
      "类别 211: 12 个样本\n",
      "类别 212: 12 个样本\n",
      "类别 213: 12 个样本\n",
      "类别 214: 12 个样本\n",
      "类别 215: 12 个样本\n",
      "类别 216: 13 个样本\n",
      "类别 217: 13 个样本\n",
      "类别 218: 12 个样本\n",
      "类别 219: 12 个样本\n",
      "类别 220: 12 个样本\n",
      "类别 221: 12 个样本\n",
      "类别 222: 12 个样本\n",
      "类别 223: 12 个样本\n",
      "类别 224: 12 个样本\n",
      "类别 225: 12 个样本\n",
      "类别 226: 12 个样本\n",
      "类别 227: 12 个样本\n",
      "类别 228: 12 个样本\n",
      "类别 229: 12 个样本\n",
      "类别 230: 13 个样本\n",
      "类别 231: 12 个样本\n",
      "类别 232: 12 个样本\n",
      "类别 233: 12 个样本\n",
      "类别 234: 12 个样本\n",
      "类别 235: 12 个样本\n",
      "类别 236: 12 个样本\n",
      "类别 237: 12 个样本\n",
      "类别 238: 12 个样本\n",
      "类别 239: 12 个样本\n",
      "类别 240: 12 个样本\n",
      "类别 241: 12 个样本\n",
      "类别 242: 12 个样本\n",
      "类别 243: 12 个样本\n",
      "类别 244: 12 个样本\n",
      "类别 245: 12 个样本\n",
      "类别 246: 13 个样本\n",
      "类别 247: 12 个样本\n",
      "类别 248: 12 个样本\n",
      "类别 249: 12 个样本\n",
      "类别 250: 12 个样本\n",
      "类别 251: 12 个样本\n",
      "类别 252: 12 个样本\n",
      "类别 253: 13 个样本\n",
      "类别 254: 12 个样本\n",
      "类别 255: 12 个样本\n",
      "类别 256: 12 个样本\n",
      "类别 257: 12 个样本\n",
      "类别 258: 12 个样本\n",
      "类别 259: 12 个样本\n",
      "类别 260: 12 个样本\n",
      "类别 261: 12 个样本\n",
      "类别 262: 12 个样本\n",
      "类别 263: 13 个样本\n",
      "类别 264: 12 个样本\n",
      "类别 265: 12 个样本\n",
      "类别 266: 12 个样本\n",
      "类别 267: 12 个样本\n",
      "类别 268: 12 个样本\n",
      "类别 269: 12 个样本\n",
      "类别 270: 12 个样本\n",
      "类别 271: 12 个样本\n",
      "类别 272: 12 个样本\n",
      "类别 273: 12 个样本\n",
      "类别 274: 13 个样本\n",
      "类别 275: 12 个样本\n",
      "类别 276: 12 个样本\n",
      "类别 277: 12 个样本\n",
      "类别 278: 12 个样本\n",
      "类别 279: 12 个样本\n",
      "类别 280: 12 个样本\n",
      "类别 281: 12 个样本\n",
      "类别 282: 13 个样本\n",
      "类别 283: 12 个样本\n",
      "类别 284: 12 个样本\n",
      "类别 285: 12 个样本\n",
      "类别 286: 12 个样本\n",
      "类别 287: 13 个样本\n",
      "类别 288: 12 个样本\n",
      "类别 289: 12 个样本\n",
      "类别 290: 12 个样本\n",
      "类别 291: 12 个样本\n",
      "类别 292: 12 个样本\n",
      "类别 293: 12 个样本\n",
      "类别 294: 12 个样本\n",
      "类别 295: 12 个样本\n",
      "类别 296: 12 个样本\n",
      "类别 297: 12 个样本\n",
      "类别 298: 12 个样本\n",
      "类别 299: 12 个样本\n",
      "类别 300: 12 个样本\n",
      "类别 301: 12 个样本\n",
      "类别 302: 12 个样本\n",
      "类别 303: 12 个样本\n",
      "类别 304: 12 个样本\n",
      "类别 305: 12 个样本\n",
      "类别 306: 12 个样本\n",
      "类别 307: 12 个样本\n",
      "类别 308: 12 个样本\n",
      "类别 309: 12 个样本\n",
      "类别 310: 12 个样本\n",
      "类别 311: 12 个样本\n",
      "类别 312: 12 个样本\n",
      "类别 313: 13 个样本\n",
      "类别 314: 12 个样本\n",
      "类别 315: 12 个样本\n",
      "类别 316: 13 个样本\n",
      "类别 317: 12 个样本\n",
      "类别 318: 12 个样本\n",
      "类别 319: 12 个样本\n",
      "类别 320: 13 个样本\n",
      "类别 321: 12 个样本\n",
      "类别 322: 12 个样本\n",
      "类别 323: 12 个样本\n",
      "类别 324: 12 个样本\n",
      "类别 325: 12 个样本\n",
      "类别 326: 12 个样本\n",
      "类别 327: 12 个样本\n",
      "类别 328: 12 个样本\n",
      "类别 329: 12 个样本\n",
      "类别 330: 12 个样本\n",
      "类别 331: 12 个样本\n",
      "类别 332: 12 个样本\n",
      "类别 333: 12 个样本\n",
      "类别 334: 12 个样本\n",
      "类别 335: 13 个样本\n",
      "类别 336: 12 个样本\n",
      "类别 337: 12 个样本\n",
      "类别 338: 12 个样本\n",
      "类别 339: 12 个样本\n",
      "类别 340: 12 个样本\n",
      "类别 341: 12 个样本\n",
      "类别 342: 12 个样本\n",
      "类别 343: 12 个样本\n",
      "类别 344: 12 个样本\n",
      "类别 345: 12 个样本\n",
      "类别 346: 12 个样本\n",
      "类别 347: 12 个样本\n",
      "类别 348: 12 个样本\n",
      "类别 349: 12 个样本\n",
      "类别 350: 12 个样本\n",
      "类别 351: 12 个样本\n",
      "类别 352: 12 个样本\n",
      "类别 353: 12 个样本\n",
      "类别 354: 12 个样本\n",
      "类别 355: 12 个样本\n",
      "类别 356: 13 个样本\n",
      "类别 357: 12 个样本\n",
      "类别 358: 12 个样本\n",
      "类别 359: 12 个样本\n",
      "类别 360: 12 个样本\n",
      "类别 361: 13 个样本\n",
      "类别 362: 12 个样本\n",
      "类别 363: 12 个样本\n",
      "类别 364: 12 个样本\n",
      "类别 365: 12 个样本\n",
      "类别 366: 12 个样本\n",
      "类别 367: 12 个样本\n",
      "类别 368: 12 个样本\n",
      "类别 369: 12 个样本\n",
      "类别 370: 12 个样本\n",
      "类别 371: 13 个样本\n",
      "类别 372: 12 个样本\n",
      "类别 373: 12 个样本\n",
      "类别 374: 12 个样本\n",
      "类别 375: 12 个样本\n",
      "类别 376: 12 个样本\n",
      "类别 377: 12 个样本\n",
      "类别 378: 12 个样本\n",
      "类别 379: 12 个样本\n",
      "类别 380: 12 个样本\n",
      "类别 381: 12 个样本\n",
      "类别 382: 12 个样本\n",
      "类别 383: 12 个样本\n",
      "类别 384: 12 个样本\n",
      "类别 385: 12 个样本\n",
      "类别 386: 12 个样本\n",
      "类别 387: 12 个样本\n",
      "类别 388: 12 个样本\n",
      "类别 389: 12 个样本\n",
      "类别 390: 12 个样本\n",
      "类别 391: 12 个样本\n",
      "类别 392: 12 个样本\n",
      "类别 393: 12 个样本\n",
      "类别 394: 12 个样本\n",
      "类别 395: 12 个样本\n",
      "类别 396: 13 个样本\n",
      "类别 397: 12 个样本\n",
      "类别 398: 12 个样本\n",
      "类别 399: 12 个样本\n",
      "类别 400: 12 个样本\n",
      "类别 401: 13 个样本\n",
      "类别 402: 12 个样本\n",
      "类别 403: 12 个样本\n",
      "类别 404: 12 个样本\n",
      "类别 405: 12 个样本\n",
      "类别 406: 12 个样本\n",
      "类别 407: 12 个样本\n",
      "类别 408: 12 个样本\n",
      "类别 409: 12 个样本\n",
      "类别 410: 12 个样本\n",
      "类别 411: 12 个样本\n",
      "类别 412: 12 个样本\n",
      "类别 413: 12 个样本\n",
      "类别 414: 12 个样本\n",
      "类别 415: 12 个样本\n",
      "类别 416: 12 个样本\n",
      "类别 417: 12 个样本\n",
      "类别 418: 12 个样本\n",
      "类别 419: 12 个样本\n",
      "类别 420: 12 个样本\n",
      "类别 421: 12 个样本\n",
      "类别 422: 13 个样本\n",
      "类别 423: 13 个样本\n",
      "类别 424: 12 个样本\n",
      "类别 425: 12 个样本\n",
      "类别 426: 12 个样本\n",
      "类别 427: 13 个样本\n",
      "类别 428: 12 个样本\n",
      "类别 429: 12 个样本\n",
      "类别 430: 12 个样本\n",
      "类别 431: 12 个样本\n",
      "类别 432: 12 个样本\n",
      "类别 433: 13 个样本\n",
      "类别 434: 12 个样本\n",
      "类别 435: 12 个样本\n",
      "类别 436: 13 个样本\n",
      "类别 437: 12 个样本\n",
      "类别 438: 12 个样本\n",
      "类别 439: 12 个样本\n",
      "类别 440: 12 个样本\n",
      "类别 441: 12 个样本\n",
      "类别 442: 13 个样本\n",
      "类别 443: 12 个样本\n",
      "类别 444: 12 个样本\n",
      "类别 445: 12 个样本\n",
      "类别 446: 12 个样本\n",
      "类别 447: 12 个样本\n",
      "类别 448: 12 个样本\n",
      "类别 449: 12 个样本\n",
      "类别 450: 12 个样本\n",
      "类别 451: 12 个样本\n",
      "类别 452: 12 个样本\n",
      "类别 453: 12 个样本\n",
      "类别 454: 12 个样本\n",
      "类别 455: 12 个样本\n",
      "类别 456: 12 个样本\n",
      "类别 457: 12 个样本\n",
      "类别 458: 13 个样本\n",
      "类别 459: 13 个样本\n",
      "类别 460: 12 个样本\n",
      "类别 461: 12 个样本\n",
      "类别 462: 13 个样本\n",
      "类别 463: 12 个样本\n",
      "类别 464: 12 个样本\n",
      "类别 465: 12 个样本\n",
      "类别 466: 12 个样本\n",
      "类别 467: 12 个样本\n",
      "类别 468: 12 个样本\n",
      "类别 469: 13 个样本\n",
      "类别 470: 12 个样本\n",
      "类别 471: 12 个样本\n",
      "类别 472: 12 个样本\n",
      "类别 473: 12 个样本\n",
      "类别 474: 12 个样本\n",
      "类别 475: 12 个样本\n",
      "类别 476: 12 个样本\n",
      "类别 477: 12 个样本\n",
      "类别 478: 13 个样本\n",
      "类别 479: 12 个样本\n",
      "类别 480: 12 个样本\n",
      "类别 481: 12 个样本\n",
      "类别 482: 12 个样本\n",
      "类别 483: 12 个样本\n",
      "类别 484: 12 个样本\n",
      "类别 485: 12 个样本\n",
      "类别 486: 12 个样本\n",
      "类别 487: 12 个样本\n",
      "类别 488: 12 个样本\n",
      "类别 489: 12 个样本\n",
      "类别 490: 13 个样本\n",
      "类别 491: 12 个样本\n",
      "类别 492: 12 个样本\n",
      "类别 493: 12 个样本\n",
      "类别 494: 12 个样本\n",
      "类别 495: 12 个样本\n",
      "类别 496: 12 个样本\n",
      "类别 497: 12 个样本\n",
      "类别 498: 12 个样本\n",
      "类别 499: 12 个样本\n",
      "类别 500: 12 个样本\n",
      "类别 501: 12 个样本\n",
      "类别 502: 13 个样本\n",
      "类别 503: 12 个样本\n",
      "类别 504: 12 个样本\n",
      "类别 505: 12 个样本\n",
      "类别 506: 13 个样本\n",
      "类别 507: 12 个样本\n",
      "类别 508: 12 个样本\n",
      "类别 509: 12 个样本\n",
      "类别 510: 12 个样本\n",
      "类别 511: 12 个样本\n",
      "类别 512: 12 个样本\n",
      "类别 513: 12 个样本\n",
      "类别 514: 13 个样本\n",
      "类别 515: 13 个样本\n",
      "类别 516: 13 个样本\n",
      "类别 517: 13 个样本\n",
      "类别 518: 12 个样本\n",
      "类别 519: 13 个样本\n",
      "类别 520: 13 个样本\n",
      "类别 521: 12 个样本\n",
      "类别 522: 12 个样本\n",
      "类别 523: 12 个样本\n",
      "类别 524: 12 个样本\n",
      "类别 525: 12 个样本\n",
      "类别 526: 12 个样本\n",
      "类别 527: 12 个样本\n",
      "类别 528: 12 个样本\n",
      "类别 529: 12 个样本\n",
      "类别 530: 12 个样本\n",
      "类别 531: 12 个样本\n",
      "类别 532: 12 个样本\n",
      "类别 533: 12 个样本\n",
      "类别 534: 12 个样本\n",
      "类别 535: 12 个样本\n",
      "类别 536: 12 个样本\n",
      "类别 537: 12 个样本\n",
      "类别 538: 12 个样本\n",
      "类别 539: 12 个样本\n",
      "类别 540: 12 个样本\n",
      "类别 541: 13 个样本\n",
      "类别 542: 12 个样本\n",
      "类别 543: 13 个样本\n",
      "类别 544: 12 个样本\n",
      "类别 545: 12 个样本\n",
      "类别 546: 12 个样本\n",
      "类别 547: 12 个样本\n",
      "类别 548: 12 个样本\n",
      "类别 549: 13 个样本\n",
      "类别 550: 12 个样本\n",
      "类别 551: 12 个样本\n",
      "类别 552: 12 个样本\n",
      "类别 553: 12 个样本\n",
      "类别 554: 12 个样本\n",
      "类别 555: 12 个样本\n",
      "类别 556: 12 个样本\n",
      "类别 557: 12 个样本\n",
      "类别 558: 12 个样本\n",
      "类别 559: 12 个样本\n",
      "类别 560: 12 个样本\n",
      "类别 561: 12 个样本\n",
      "类别 562: 12 个样本\n",
      "类别 563: 12 个样本\n",
      "类别 564: 13 个样本\n",
      "类别 565: 12 个样本\n",
      "类别 566: 12 个样本\n",
      "类别 567: 12 个样本\n",
      "类别 568: 12 个样本\n",
      "类别 569: 12 个样本\n",
      "类别 570: 12 个样本\n",
      "类别 571: 12 个样本\n",
      "类别 572: 12 个样本\n",
      "类别 573: 12 个样本\n",
      "类别 574: 12 个样本\n",
      "类别 575: 12 个样本\n",
      "类别 576: 12 个样本\n",
      "类别 577: 12 个样本\n",
      "类别 578: 12 个样本\n",
      "类别 579: 12 个样本\n",
      "类别 580: 13 个样本\n",
      "类别 581: 12 个样本\n",
      "类别 582: 12 个样本\n",
      "类别 583: 12 个样本\n",
      "类别 584: 12 个样本\n",
      "类别 585: 12 个样本\n",
      "类别 586: 12 个样本\n",
      "类别 587: 12 个样本\n",
      "类别 588: 12 个样本\n",
      "类别 589: 12 个样本\n",
      "类别 590: 12 个样本\n",
      "类别 591: 12 个样本\n",
      "类别 592: 12 个样本\n",
      "类别 593: 12 个样本\n",
      "类别 594: 12 个样本\n",
      "类别 595: 12 个样本\n",
      "类别 596: 12 个样本\n",
      "类别 597: 12 个样本\n",
      "类别 598: 12 个样本\n",
      "类别 599: 12 个样本\n",
      "类别 600: 12 个样本\n",
      "类别 601: 13 个样本\n",
      "类别 602: 12 个样本\n",
      "类别 603: 12 个样本\n",
      "类别 604: 12 个样本\n",
      "类别 605: 12 个样本\n",
      "类别 606: 12 个样本\n",
      "类别 607: 12 个样本\n",
      "类别 608: 12 个样本\n",
      "类别 609: 12 个样本\n",
      "类别 610: 12 个样本\n",
      "类别 611: 12 个样本\n",
      "类别 612: 12 个样本\n",
      "类别 613: 12 个样本\n",
      "类别 614: 12 个样本\n",
      "类别 615: 12 个样本\n",
      "类别 616: 12 个样本\n",
      "类别 617: 12 个样本\n",
      "类别 618: 13 个样本\n",
      "类别 619: 12 个样本\n",
      "类别 620: 13 个样本\n",
      "类别 621: 12 个样本\n",
      "类别 622: 12 个样本\n",
      "类别 623: 12 个样本\n",
      "类别 624: 12 个样本\n",
      "类别 625: 12 个样本\n",
      "类别 626: 12 个样本\n",
      "类别 627: 12 个样本\n",
      "类别 628: 12 个样本\n",
      "类别 629: 12 个样本\n",
      "类别 630: 12 个样本\n",
      "类别 631: 13 个样本\n",
      "类别 632: 12 个样本\n",
      "类别 633: 13 个样本\n",
      "类别 634: 12 个样本\n",
      "类别 635: 12 个样本\n",
      "类别 636: 12 个样本\n",
      "类别 637: 12 个样本\n",
      "类别 638: 12 个样本\n",
      "类别 639: 12 个样本\n",
      "类别 640: 12 个样本\n",
      "类别 641: 12 个样本\n",
      "类别 642: 12 个样本\n",
      "类别 643: 12 个样本\n",
      "类别 644: 12 个样本\n",
      "类别 645: 12 个样本\n",
      "类别 646: 12 个样本\n",
      "类别 647: 12 个样本\n",
      "类别 648: 13 个样本\n",
      "类别 649: 12 个样本\n",
      "类别 650: 12 个样本\n",
      "类别 651: 12 个样本\n",
      "类别 652: 12 个样本\n",
      "类别 653: 12 个样本\n",
      "类别 654: 12 个样本\n",
      "类别 655: 12 个样本\n",
      "类别 656: 12 个样本\n",
      "类别 657: 12 个样本\n",
      "类别 658: 12 个样本\n",
      "类别 659: 12 个样本\n",
      "类别 660: 12 个样本\n",
      "类别 661: 12 个样本\n",
      "类别 662: 12 个样本\n",
      "类别 663: 12 个样本\n",
      "类别 664: 12 个样本\n",
      "类别 665: 12 个样本\n",
      "类别 666: 12 个样本\n",
      "类别 667: 12 个样本\n",
      "类别 668: 12 个样本\n",
      "类别 669: 12 个样本\n",
      "类别 670: 13 个样本\n",
      "类别 671: 12 个样本\n",
      "类别 672: 12 个样本\n",
      "类别 673: 12 个样本\n",
      "类别 674: 12 个样本\n",
      "类别 675: 12 个样本\n",
      "类别 676: 12 个样本\n",
      "类别 677: 12 个样本\n",
      "类别 678: 12 个样本\n",
      "类别 679: 12 个样本\n",
      "类别 680: 12 个样本\n",
      "类别 681: 12 个样本\n",
      "类别 682: 12 个样本\n",
      "类别 683: 12 个样本\n",
      "类别 684: 12 个样本\n",
      "类别 685: 12 个样本\n",
      "类别 686: 12 个样本\n",
      "类别 687: 12 个样本\n",
      "类别 688: 12 个样本\n",
      "类别 689: 12 个样本\n",
      "类别 690: 12 个样本\n",
      "类别 691: 12 个样本\n",
      "类别 692: 12 个样本\n",
      "类别 693: 12 个样本\n",
      "类别 694: 12 个样本\n",
      "类别 695: 12 个样本\n",
      "类别 696: 12 个样本\n",
      "类别 697: 12 个样本\n",
      "类别 698: 13 个样本\n",
      "类别 699: 12 个样本\n",
      "类别 700: 12 个样本\n",
      "类别 701: 12 个样本\n",
      "类别 702: 12 个样本\n",
      "类别 703: 13 个样本\n",
      "类别 704: 12 个样本\n",
      "类别 705: 12 个样本\n",
      "类别 706: 12 个样本\n",
      "类别 707: 12 个样本\n",
      "类别 708: 13 个样本\n",
      "类别 709: 12 个样本\n",
      "类别 710: 12 个样本\n",
      "类别 711: 12 个样本\n",
      "类别 712: 12 个样本\n",
      "类别 713: 12 个样本\n",
      "类别 714: 12 个样本\n",
      "类别 715: 12 个样本\n",
      "类别 716: 12 个样本\n",
      "类别 717: 12 个样本\n",
      "类别 718: 12 个样本\n",
      "类别 719: 12 个样本\n",
      "类别 720: 13 个样本\n",
      "类别 721: 13 个样本\n",
      "类别 722: 12 个样本\n",
      "类别 723: 12 个样本\n",
      "类别 724: 12 个样本\n",
      "类别 725: 12 个样本\n",
      "类别 726: 12 个样本\n",
      "类别 727: 12 个样本\n",
      "类别 728: 12 个样本\n",
      "类别 729: 12 个样本\n",
      "类别 730: 12 个样本\n",
      "类别 731: 12 个样本\n",
      "类别 732: 12 个样本\n",
      "类别 733: 12 个样本\n",
      "类别 734: 12 个样本\n",
      "类别 735: 13 个样本\n",
      "类别 736: 12 个样本\n",
      "类别 737: 12 个样本\n",
      "类别 738: 12 个样本\n",
      "类别 739: 12 个样本\n",
      "类别 740: 12 个样本\n",
      "类别 741: 12 个样本\n",
      "类别 742: 12 个样本\n",
      "类别 743: 12 个样本\n",
      "类别 744: 12 个样本\n",
      "类别 745: 12 个样本\n",
      "类别 746: 12 个样本\n",
      "类别 747: 12 个样本\n",
      "类别 748: 12 个样本\n",
      "类别 749: 12 个样本\n",
      "类别 750: 12 个样本\n",
      "类别 751: 12 个样本\n",
      "类别 752: 12 个样本\n",
      "类别 753: 12 个样本\n",
      "类别 754: 12 个样本\n",
      "类别 755: 12 个样本\n",
      "类别 756: 12 个样本\n",
      "类别 757: 12 个样本\n",
      "类别 758: 12 个样本\n",
      "类别 759: 12 个样本\n",
      "类别 760: 12 个样本\n",
      "类别 761: 12 个样本\n",
      "类别 762: 13 个样本\n",
      "类别 763: 12 个样本\n",
      "类别 764: 12 个样本\n",
      "类别 765: 13 个样本\n",
      "类别 766: 12 个样本\n",
      "类别 767: 13 个样本\n",
      "类别 768: 12 个样本\n",
      "类别 769: 12 个样本\n",
      "类别 770: 12 个样本\n",
      "类别 771: 12 个样本\n",
      "类别 772: 12 个样本\n",
      "类别 773: 12 个样本\n",
      "类别 774: 12 个样本\n",
      "类别 775: 13 个样本\n",
      "类别 776: 13 个样本\n",
      "类别 777: 12 个样本\n",
      "类别 778: 12 个样本\n",
      "类别 779: 12 个样本\n",
      "类别 780: 12 个样本\n",
      "类别 781: 12 个样本\n",
      "类别 782: 13 个样本\n",
      "类别 783: 12 个样本\n",
      "类别 784: 12 个样本\n",
      "类别 785: 12 个样本\n",
      "类别 786: 12 个样本\n",
      "类别 787: 12 个样本\n",
      "类别 788: 12 个样本\n",
      "类别 789: 12 个样本\n",
      "类别 790: 12 个样本\n",
      "类别 791: 12 个样本\n",
      "类别 792: 12 个样本\n",
      "类别 793: 12 个样本\n",
      "类别 794: 12 个样本\n",
      "类别 795: 12 个样本\n",
      "类别 796: 12 个样本\n",
      "类别 797: 12 个样本\n",
      "类别 798: 12 个样本\n",
      "类别 799: 13 个样本\n",
      "类别 800: 13 个样本\n",
      "类别 801: 12 个样本\n",
      "类别 802: 12 个样本\n",
      "类别 803: 12 个样本\n",
      "类别 804: 12 个样本\n",
      "类别 805: 12 个样本\n",
      "类别 806: 12 个样本\n",
      "类别 807: 12 个样本\n",
      "类别 808: 12 个样本\n",
      "类别 809: 13 个样本\n",
      "类别 810: 12 个样本\n",
      "类别 811: 13 个样本\n",
      "类别 812: 12 个样本\n",
      "类别 813: 12 个样本\n",
      "类别 814: 12 个样本\n",
      "类别 815: 12 个样本\n",
      "类别 816: 12 个样本\n",
      "类别 817: 12 个样本\n",
      "类别 818: 12 个样本\n",
      "类别 819: 12 个样本\n",
      "类别 820: 12 个样本\n",
      "类别 821: 12 个样本\n",
      "类别 822: 12 个样本\n",
      "类别 823: 12 个样本\n",
      "类别 824: 12 个样本\n",
      "类别 825: 12 个样本\n",
      "类别 826: 13 个样本\n",
      "类别 827: 12 个样本\n",
      "类别 828: 12 个样本\n",
      "类别 829: 12 个样本\n",
      "类别 830: 12 个样本\n",
      "类别 831: 12 个样本\n",
      "类别 832: 12 个样本\n",
      "类别 833: 13 个样本\n",
      "类别 834: 12 个样本\n",
      "类别 835: 12 个样本\n",
      "类别 836: 12 个样本\n",
      "类别 837: 12 个样本\n",
      "类别 838: 12 个样本\n",
      "类别 839: 12 个样本\n",
      "类别 840: 13 个样本\n",
      "类别 841: 13 个样本\n",
      "类别 842: 12 个样本\n",
      "类别 843: 12 个样本\n",
      "类别 844: 12 个样本\n",
      "类别 845: 12 个样本\n",
      "类别 846: 12 个样本\n",
      "类别 847: 12 个样本\n",
      "类别 848: 12 个样本\n",
      "类别 849: 12 个样本\n",
      "类别 850: 12 个样本\n",
      "类别 851: 12 个样本\n",
      "类别 852: 12 个样本\n",
      "类别 853: 13 个样本\n",
      "类别 854: 12 个样本\n",
      "类别 855: 12 个样本\n",
      "类别 856: 12 个样本\n",
      "类别 857: 12 个样本\n",
      "类别 858: 12 个样本\n",
      "类别 859: 12 个样本\n",
      "类别 860: 12 个样本\n",
      "类别 861: 12 个样本\n",
      "类别 862: 12 个样本\n",
      "类别 863: 12 个样本\n",
      "类别 864: 12 个样本\n",
      "类别 865: 13 个样本\n",
      "类别 866: 12 个样本\n",
      "类别 867: 12 个样本\n",
      "类别 868: 12 个样本\n",
      "类别 869: 12 个样本\n",
      "类别 870: 13 个样本\n",
      "类别 871: 12 个样本\n",
      "类别 872: 12 个样本\n",
      "类别 873: 12 个样本\n",
      "类别 874: 13 个样本\n",
      "类别 875: 12 个样本\n",
      "类别 876: 12 个样本\n",
      "类别 877: 12 个样本\n",
      "类别 878: 12 个样本\n",
      "类别 879: 12 个样本\n",
      "类别 880: 13 个样本\n",
      "类别 881: 12 个样本\n",
      "类别 882: 12 个样本\n",
      "类别 883: 12 个样本\n",
      "类别 884: 12 个样本\n",
      "类别 885: 12 个样本\n",
      "类别 886: 12 个样本\n",
      "类别 887: 13 个样本\n",
      "类别 888: 12 个样本\n",
      "类别 889: 12 个样本\n",
      "类别 890: 12 个样本\n",
      "类别 891: 12 个样本\n",
      "类别 892: 12 个样本\n",
      "类别 893: 12 个样本\n",
      "类别 894: 12 个样本\n",
      "类别 895: 12 个样本\n",
      "类别 896: 12 个样本\n",
      "类别 897: 12 个样本\n",
      "类别 898: 12 个样本\n",
      "类别 899: 13 个样本\n",
      "类别 900: 12 个样本\n",
      "类别 901: 12 个样本\n",
      "类别 902: 12 个样本\n",
      "类别 903: 12 个样本\n",
      "类别 904: 12 个样本\n",
      "类别 905: 12 个样本\n",
      "类别 906: 13 个样本\n",
      "类别 907: 12 个样本\n",
      "类别 908: 12 个样本\n",
      "类别 909: 12 个样本\n",
      "类别 910: 12 个样本\n",
      "类别 911: 12 个样本\n",
      "类别 912: 12 个样本\n",
      "类别 913: 12 个样本\n",
      "类别 914: 12 个样本\n",
      "类别 915: 12 个样本\n",
      "类别 916: 13 个样本\n",
      "类别 917: 12 个样本\n",
      "类别 918: 12 个样本\n",
      "类别 919: 12 个样本\n",
      "类别 920: 12 个样本\n",
      "类别 921: 12 个样本\n",
      "类别 922: 12 个样本\n",
      "类别 923: 12 个样本\n",
      "类别 924: 12 个样本\n",
      "类别 925: 12 个样本\n",
      "类别 926: 12 个样本\n",
      "类别 927: 12 个样本\n",
      "类别 928: 12 个样本\n",
      "类别 929: 12 个样本\n",
      "类别 930: 12 个样本\n",
      "类别 931: 12 个样本\n",
      "类别 932: 13 个样本\n",
      "类别 933: 12 个样本\n",
      "类别 934: 13 个样本\n",
      "类别 935: 12 个样本\n",
      "类别 936: 12 个样本\n",
      "类别 937: 12 个样本\n",
      "类别 938: 12 个样本\n",
      "类别 939: 12 个样本\n",
      "类别 940: 12 个样本\n",
      "类别 941: 12 个样本\n",
      "类别 942: 12 个样本\n",
      "类别 943: 12 个样本\n",
      "类别 944: 12 个样本\n",
      "类别 945: 12 个样本\n",
      "类别 946: 12 个样本\n",
      "类别 947: 12 个样本\n",
      "类别 948: 12 个样本\n",
      "类别 949: 12 个样本\n",
      "类别 950: 12 个样本\n",
      "类别 951: 12 个样本\n",
      "类别 952: 12 个样本\n",
      "类别 953: 12 个样本\n",
      "类别 954: 12 个样本\n",
      "类别 955: 12 个样本\n",
      "类别 956: 12 个样本\n",
      "类别 957: 12 个样本\n",
      "类别 958: 12 个样本\n",
      "类别 959: 12 个样本\n",
      "类别 960: 12 个样本\n",
      "类别 961: 13 个样本\n",
      "类别 962: 12 个样本\n",
      "类别 963: 12 个样本\n",
      "类别 964: 12 个样本\n",
      "类别 965: 12 个样本\n",
      "类别 966: 12 个样本\n",
      "类别 967: 13 个样本\n",
      "类别 968: 13 个样本\n",
      "类别 969: 12 个样本\n",
      "类别 970: 12 个样本\n",
      "类别 971: 12 个样本\n",
      "类别 972: 12 个样本\n",
      "类别 973: 12 个样本\n",
      "类别 974: 12 个样本\n",
      "类别 975: 12 个样本\n",
      "类别 976: 12 个样本\n",
      "类别 977: 12 个样本\n",
      "类别 978: 13 个样本\n",
      "类别 979: 12 个样本\n",
      "类别 980: 12 个样本\n",
      "类别 981: 12 个样本\n",
      "类别 982: 12 个样本\n",
      "类别 983: 12 个样本\n",
      "类别 984: 12 个样本\n",
      "类别 985: 12 个样本\n",
      "类别 986: 12 个样本\n",
      "类别 987: 12 个样本\n",
      "类别 988: 13 个样本\n",
      "类别 989: 12 个样本\n",
      "类别 990: 13 个样本\n",
      "类别 991: 12 个样本\n",
      "类别 992: 12 个样本\n",
      "类别 993: 12 个样本\n",
      "类别 994: 12 个样本\n",
      "类别 995: 12 个样本\n",
      "类别 996: 12 个样本\n",
      "类别 997: 12 个样本\n",
      "类别 998: 12 个样本\n",
      "类别 999: 12 个样本\n",
      "类别 1000: 13 个样本\n",
      "类别 1001: 12 个样本\n",
      "类别 1002: 12 个样本\n",
      "类别 1003: 13 个样本\n",
      "类别 1004: 12 个样本\n",
      "类别 1005: 12 个样本\n",
      "类别 1006: 12 个样本\n",
      "类别 1007: 12 个样本\n",
      "类别 1008: 12 个样本\n",
      "类别 1009: 12 个样本\n",
      "类别 1010: 12 个样本\n",
      "类别 1011: 12 个样本\n",
      "类别 1012: 12 个样本\n",
      "类别 1013: 12 个样本\n",
      "类别 1014: 12 个样本\n",
      "类别 1015: 12 个样本\n",
      "类别 1016: 12 个样本\n",
      "类别 1017: 12 个样本\n",
      "类别 1018: 13 个样本\n",
      "类别 1019: 12 个样本\n",
      "类别 1020: 12 个样本\n",
      "类别 1021: 13 个样本\n",
      "类别 1022: 13 个样本\n",
      "类别 1023: 12 个样本\n",
      "类别 1024: 12 个样本\n",
      "类别 1025: 12 个样本\n",
      "类别 1026: 12 个样本\n",
      "类别 1027: 12 个样本\n",
      "类别 1028: 12 个样本\n",
      "类别 1029: 12 个样本\n",
      "类别 1030: 13 个样本\n",
      "类别 1031: 12 个样本\n",
      "类别 1032: 12 个样本\n",
      "类别 1033: 12 个样本\n",
      "类别 1034: 12 个样本\n",
      "类别 1035: 12 个样本\n",
      "类别 1036: 12 个样本\n",
      "类别 1037: 12 个样本\n",
      "类别 1038: 12 个样本\n",
      "类别 1039: 12 个样本\n",
      "类别 1040: 12 个样本\n",
      "类别 1041: 12 个样本\n",
      "类别 1042: 12 个样本\n",
      "类别 1043: 12 个样本\n",
      "类别 1044: 12 个样本\n",
      "类别 1045: 12 个样本\n",
      "类别 1046: 12 个样本\n",
      "类别 1047: 12 个样本\n",
      "类别 1048: 12 个样本\n",
      "类别 1049: 12 个样本\n",
      "类别 1050: 12 个样本\n",
      "类别 1051: 12 个样本\n",
      "类别 1052: 12 个样本\n",
      "类别 1053: 12 个样本\n",
      "类别 1054: 12 个样本\n",
      "类别 1055: 12 个样本\n",
      "类别 1056: 12 个样本\n",
      "类别 1057: 13 个样本\n",
      "类别 1058: 12 个样本\n",
      "类别 1059: 12 个样本\n",
      "类别 1060: 12 个样本\n",
      "类别 1061: 12 个样本\n",
      "类别 1062: 12 个样本\n",
      "类别 1063: 12 个样本\n",
      "类别 1064: 13 个样本\n",
      "类别 1065: 12 个样本\n",
      "类别 1066: 12 个样本\n",
      "类别 1067: 12 个样本\n",
      "类别 1068: 12 个样本\n",
      "类别 1069: 12 个样本\n",
      "类别 1070: 13 个样本\n",
      "类别 1071: 12 个样本\n",
      "类别 1072: 12 个样本\n",
      "类别 1073: 12 个样本\n",
      "类别 1074: 12 个样本\n",
      "类别 1075: 13 个样本\n",
      "类别 1076: 12 个样本\n",
      "类别 1077: 13 个样本\n",
      "类别 1078: 12 个样本\n",
      "类别 1079: 12 个样本\n",
      "类别 1080: 12 个样本\n",
      "类别 1081: 12 个样本\n",
      "类别 1082: 12 个样本\n",
      "类别 1083: 12 个样本\n",
      "类别 1084: 12 个样本\n",
      "类别 1085: 12 个样本\n",
      "类别 1086: 12 个样本\n",
      "类别 1087: 12 个样本\n",
      "类别 1088: 12 个样本\n",
      "类别 1089: 12 个样本\n",
      "类别 1090: 12 个样本\n",
      "类别 1091: 12 个样本\n",
      "类别 1092: 12 个样本\n",
      "类别 1093: 12 个样本\n",
      "类别 1094: 13 个样本\n",
      "类别 1095: 12 个样本\n",
      "类别 1096: 13 个样本\n",
      "类别 1097: 12 个样本\n",
      "类别 1098: 12 个样本\n",
      "类别 1099: 12 个样本\n",
      "类别 1100: 12 个样本\n",
      "类别 1101: 12 个样本\n",
      "类别 1102: 12 个样本\n",
      "类别 1103: 12 个样本\n",
      "类别 1104: 13 个样本\n",
      "类别 1105: 12 个样本\n",
      "类别 1106: 13 个样本\n",
      "类别 1107: 12 个样本\n",
      "类别 1108: 12 个样本\n",
      "类别 1109: 12 个样本\n",
      "类别 1110: 12 个样本\n",
      "类别 1111: 12 个样本\n",
      "类别 1112: 12 个样本\n",
      "类别 1113: 12 个样本\n",
      "类别 1114: 12 个样本\n",
      "类别 1115: 12 个样本\n",
      "类别 1116: 12 个样本\n",
      "类别 1117: 12 个样本\n",
      "类别 1118: 12 个样本\n",
      "类别 1119: 12 个样本\n",
      "类别 1120: 12 个样本\n",
      "类别 1121: 12 个样本\n",
      "类别 1122: 12 个样本\n",
      "类别 1123: 12 个样本\n",
      "类别 1124: 12 个样本\n",
      "类别 1125: 12 个样本\n",
      "类别 1126: 12 个样本\n",
      "类别 1127: 12 个样本\n",
      "类别 1128: 12 个样本\n",
      "类别 1129: 12 个样本\n",
      "类别 1130: 13 个样本\n",
      "类别 1131: 12 个样本\n",
      "类别 1132: 12 个样本\n",
      "类别 1133: 12 个样本\n",
      "类别 1134: 13 个样本\n",
      "类别 1135: 12 个样本\n",
      "类别 1136: 12 个样本\n",
      "类别 1137: 12 个样本\n",
      "类别 1138: 12 个样本\n",
      "类别 1139: 12 个样本\n",
      "类别 1140: 12 个样本\n",
      "类别 1141: 12 个样本\n",
      "类别 1142: 12 个样本\n",
      "类别 1143: 12 个样本\n",
      "类别 1144: 12 个样本\n",
      "类别 1145: 12 个样本\n",
      "类别 1146: 13 个样本\n",
      "类别 1147: 12 个样本\n",
      "类别 1148: 12 个样本\n",
      "类别 1149: 12 个样本\n",
      "类别 1150: 12 个样本\n",
      "类别 1151: 12 个样本\n",
      "类别 1152: 12 个样本\n",
      "类别 1153: 12 个样本\n",
      "类别 1154: 12 个样本\n",
      "类别 1155: 12 个样本\n",
      "类别 1156: 12 个样本\n",
      "类别 1157: 12 个样本\n",
      "类别 1158: 12 个样本\n",
      "类别 1159: 12 个样本\n",
      "类别 1160: 12 个样本\n",
      "类别 1161: 12 个样本\n",
      "类别 1162: 12 个样本\n",
      "类别 1163: 12 个样本\n",
      "类别 1164: 12 个样本\n",
      "类别 1165: 12 个样本\n",
      "类别 1166: 13 个样本\n",
      "类别 1167: 12 个样本\n",
      "类别 1168: 12 个样本\n",
      "类别 1169: 13 个样本\n",
      "类别 1170: 12 个样本\n",
      "类别 1171: 12 个样本\n",
      "类别 1172: 12 个样本\n",
      "类别 1173: 12 个样本\n",
      "类别 1174: 12 个样本\n",
      "类别 1175: 12 个样本\n",
      "类别 1176: 12 个样本\n",
      "类别 1177: 12 个样本\n",
      "类别 1178: 12 个样本\n",
      "类别 1179: 12 个样本\n",
      "类别 1180: 12 个样本\n",
      "类别 1181: 12 个样本\n",
      "类别 1182: 12 个样本\n",
      "类别 1183: 12 个样本\n",
      "类别 1184: 12 个样本\n",
      "类别 1185: 12 个样本\n",
      "类别 1186: 12 个样本\n",
      "类别 1187: 12 个样本\n",
      "类别 1188: 12 个样本\n",
      "类别 1189: 12 个样本\n",
      "类别 1190: 12 个样本\n",
      "类别 1191: 12 个样本\n",
      "类别 1192: 12 个样本\n",
      "类别 1193: 12 个样本\n",
      "类别 1194: 12 个样本\n",
      "类别 1195: 12 个样本\n",
      "类别 1196: 12 个样本\n",
      "类别 1197: 12 个样本\n",
      "类别 1198: 12 个样本\n",
      "类别 1199: 13 个样本\n",
      "类别 1200: 12 个样本\n",
      "类别 1201: 12 个样本\n",
      "类别 1202: 12 个样本\n",
      "类别 1203: 12 个样本\n",
      "类别 1204: 12 个样本\n",
      "类别 1205: 12 个样本\n",
      "类别 1206: 12 个样本\n",
      "类别 1207: 12 个样本\n",
      "类别 1208: 12 个样本\n",
      "类别 1209: 12 个样本\n",
      "类别 1210: 12 个样本\n",
      "类别 1211: 12 个样本\n",
      "类别 1212: 12 个样本\n",
      "类别 1213: 13 个样本\n",
      "类别 1214: 12 个样本\n",
      "类别 1215: 12 个样本\n",
      "类别 1216: 12 个样本\n",
      "类别 1217: 12 个样本\n",
      "类别 1218: 12 个样本\n",
      "类别 1219: 12 个样本\n",
      "类别 1220: 12 个样本\n",
      "类别 1221: 12 个样本\n",
      "类别 1222: 12 个样本\n",
      "类别 1223: 12 个样本\n",
      "类别 1224: 12 个样本\n",
      "类别 1225: 12 个样本\n",
      "类别 1226: 12 个样本\n",
      "类别 1227: 12 个样本\n",
      "类别 1228: 12 个样本\n",
      "类别 1229: 12 个样本\n",
      "类别 1230: 12 个样本\n",
      "类别 1231: 13 个样本\n",
      "类别 1232: 12 个样本\n",
      "类别 1233: 12 个样本\n",
      "类别 1234: 13 个样本\n",
      "类别 1235: 12 个样本\n",
      "类别 1236: 12 个样本\n",
      "类别 1237: 12 个样本\n",
      "类别 1238: 12 个样本\n",
      "类别 1239: 12 个样本\n",
      "类别 1240: 12 个样本\n",
      "类别 1241: 12 个样本\n",
      "类别 1242: 12 个样本\n",
      "类别 1243: 12 个样本\n",
      "类别 1244: 12 个样本\n",
      "类别 1245: 12 个样本\n",
      "类别 1246: 12 个样本\n",
      "类别 1247: 12 个样本\n",
      "类别 1248: 12 个样本\n",
      "类别 1249: 12 个样本\n",
      "类别 1250: 12 个样本\n",
      "类别 1251: 12 个样本\n",
      "类别 1252: 12 个样本\n",
      "类别 1253: 12 个样本\n",
      "类别 1254: 13 个样本\n",
      "类别 1255: 12 个样本\n",
      "类别 1256: 12 个样本\n",
      "类别 1257: 12 个样本\n",
      "类别 1258: 12 个样本\n",
      "类别 1259: 12 个样本\n",
      "类别 1260: 12 个样本\n",
      "类别 1261: 12 个样本\n",
      "类别 1262: 12 个样本\n",
      "类别 1263: 12 个样本\n",
      "类别 1264: 13 个样本\n",
      "类别 1265: 12 个样本\n",
      "类别 1266: 13 个样本\n",
      "类别 1267: 12 个样本\n",
      "类别 1268: 12 个样本\n",
      "类别 1269: 12 个样本\n",
      "类别 1270: 12 个样本\n",
      "类别 1271: 12 个样本\n",
      "类别 1272: 12 个样本\n",
      "类别 1273: 12 个样本\n",
      "类别 1274: 12 个样本\n",
      "类别 1275: 12 个样本\n",
      "类别 1276: 12 个样本\n",
      "类别 1277: 12 个样本\n",
      "类别 1278: 12 个样本\n",
      "类别 1279: 12 个样本\n",
      "类别 1280: 12 个样本\n",
      "类别 1281: 12 个样本\n",
      "类别 1282: 12 个样本\n",
      "类别 1283: 12 个样本\n",
      "类别 1284: 12 个样本\n",
      "类别 1285: 12 个样本\n",
      "类别 1286: 12 个样本\n",
      "类别 1287: 12 个样本\n",
      "类别 1288: 12 个样本\n",
      "类别 1289: 12 个样本\n",
      "类别 1290: 12 个样本\n",
      "类别 1291: 12 个样本\n",
      "类别 1292: 12 个样本\n",
      "类别 1293: 12 个样本\n",
      "类别 1294: 12 个样本\n",
      "类别 1295: 12 个样本\n",
      "类别 1296: 12 个样本\n",
      "类别 1297: 12 个样本\n",
      "类别 1298: 12 个样本\n",
      "类别 1299: 12 个样本\n",
      "类别 1300: 12 个样本\n",
      "类别 1301: 12 个样本\n",
      "类别 1302: 12 个样本\n",
      "类别 1303: 12 个样本\n",
      "类别 1304: 12 个样本\n",
      "类别 1305: 12 个样本\n",
      "类别 1306: 12 个样本\n",
      "类别 1307: 12 个样本\n",
      "类别 1308: 12 个样本\n",
      "类别 1309: 13 个样本\n",
      "类别 1310: 12 个样本\n",
      "类别 1311: 12 个样本\n",
      "类别 1312: 12 个样本\n",
      "类别 1313: 12 个样本\n",
      "类别 1314: 12 个样本\n",
      "类别 1315: 12 个样本\n",
      "类别 1316: 13 个样本\n",
      "类别 1317: 12 个样本\n",
      "类别 1318: 12 个样本\n",
      "类别 1319: 12 个样本\n",
      "类别 1320: 12 个样本\n",
      "类别 1321: 12 个样本\n",
      "类别 1322: 12 个样本\n",
      "类别 1323: 12 个样本\n",
      "类别 1324: 12 个样本\n",
      "类别 1325: 12 个样本\n",
      "类别 1326: 12 个样本\n",
      "类别 1327: 12 个样本\n",
      "类别 1328: 12 个样本\n",
      "类别 1329: 12 个样本\n",
      "类别 1330: 12 个样本\n",
      "类别 1331: 12 个样本\n",
      "类别 1332: 12 个样本\n",
      "类别 1333: 12 个样本\n",
      "类别 1334: 12 个样本\n",
      "类别 1335: 12 个样本\n",
      "类别 1336: 12 个样本\n",
      "类别 1337: 12 个样本\n",
      "类别 1338: 12 个样本\n",
      "类别 1339: 12 个样本\n",
      "类别 1340: 12 个样本\n",
      "类别 1341: 12 个样本\n",
      "类别 1342: 12 个样本\n",
      "类别 1343: 12 个样本\n",
      "类别 1344: 12 个样本\n",
      "类别 1345: 12 个样本\n",
      "类别 1346: 12 个样本\n",
      "类别 1347: 12 个样本\n",
      "类别 1348: 12 个样本\n",
      "类别 1349: 12 个样本\n",
      "类别 1350: 12 个样本\n",
      "类别 1351: 12 个样本\n",
      "类别 1352: 12 个样本\n",
      "类别 1353: 12 个样本\n",
      "类别 1354: 12 个样本\n",
      "类别 1355: 12 个样本\n",
      "类别 1356: 12 个样本\n",
      "类别 1357: 12 个样本\n",
      "类别 1358: 12 个样本\n",
      "类别 1359: 12 个样本\n",
      "类别 1360: 12 个样本\n",
      "类别 1361: 12 个样本\n",
      "类别 1362: 12 个样本\n",
      "类别 1363: 12 个样本\n",
      "类别 1364: 12 个样本\n",
      "类别 1365: 12 个样本\n",
      "类别 1366: 12 个样本\n",
      "类别 1367: 12 个样本\n",
      "类别 1368: 12 个样本\n",
      "类别 1369: 12 个样本\n",
      "类别 1370: 12 个样本\n",
      "类别 1371: 12 个样本\n",
      "类别 1372: 12 个样本\n",
      "类别 1373: 12 个样本\n",
      "类别 1374: 12 个样本\n",
      "类别 1375: 12 个样本\n",
      "类别 1376: 12 个样本\n",
      "类别 1377: 12 个样本\n",
      "类别 1378: 12 个样本\n",
      "类别 1379: 12 个样本\n",
      "类别 1380: 12 个样本\n",
      "类别 1381: 12 个样本\n",
      "类别 1382: 12 个样本\n",
      "类别 1383: 12 个样本\n",
      "类别 1384: 12 个样本\n",
      "类别 1385: 12 个样本\n",
      "类别 1386: 12 个样本\n",
      "类别 1387: 12 个样本\n",
      "类别 1388: 12 个样本\n",
      "类别 1389: 12 个样本\n",
      "类别 1390: 12 个样本\n",
      "类别 1391: 12 个样本\n",
      "类别 1392: 13 个样本\n",
      "类别 1393: 12 个样本\n",
      "类别 1394: 12 个样本\n",
      "类别 1395: 12 个样本\n",
      "类别 1396: 12 个样本\n",
      "类别 1397: 12 个样本\n",
      "类别 1398: 12 个样本\n",
      "类别 1399: 12 个样本\n",
      "类别 1400: 12 个样本\n",
      "类别 1401: 12 个样本\n",
      "类别 1402: 12 个样本\n",
      "类别 1403: 13 个样本\n",
      "类别 1404: 12 个样本\n",
      "类别 1405: 12 个样本\n",
      "类别 1406: 12 个样本\n",
      "类别 1407: 13 个样本\n",
      "类别 1408: 12 个样本\n",
      "类别 1409: 12 个样本\n",
      "类别 1410: 12 个样本\n",
      "类别 1411: 12 个样本\n",
      "类别 1412: 12 个样本\n",
      "类别 1413: 12 个样本\n",
      "类别 1414: 12 个样本\n",
      "类别 1415: 12 个样本\n",
      "类别 1416: 12 个样本\n",
      "类别 1417: 12 个样本\n",
      "类别 1418: 12 个样本\n",
      "类别 1419: 13 个样本\n",
      "类别 1420: 12 个样本\n",
      "类别 1421: 12 个样本\n",
      "类别 1422: 12 个样本\n",
      "类别 1423: 12 个样本\n",
      "类别 1424: 13 个样本\n",
      "类别 1425: 12 个样本\n",
      "类别 1426: 12 个样本\n",
      "类别 1427: 12 个样本\n",
      "类别 1428: 12 个样本\n",
      "类别 1429: 12 个样本\n",
      "类别 1430: 12 个样本\n",
      "类别 1431: 12 个样本\n",
      "类别 1432: 12 个样本\n",
      "类别 1433: 12 个样本\n",
      "类别 1434: 12 个样本\n",
      "类别 1435: 12 个样本\n",
      "类别 1436: 12 个样本\n",
      "类别 1437: 12 个样本\n",
      "类别 1438: 13 个样本\n",
      "类别 1439: 12 个样本\n",
      "类别 1440: 12 个样本\n",
      "类别 1441: 12 个样本\n",
      "类别 1442: 12 个样本\n",
      "类别 1443: 12 个样本\n",
      "类别 1444: 12 个样本\n",
      "类别 1445: 12 个样本\n",
      "类别 1446: 12 个样本\n",
      "类别 1447: 13 个样本\n",
      "类别 1448: 13 个样本\n",
      "类别 1449: 12 个样本\n",
      "类别 1450: 12 个样本\n",
      "类别 1451: 12 个样本\n",
      "类别 1452: 12 个样本\n",
      "类别 1453: 12 个样本\n",
      "类别 1454: 12 个样本\n",
      "类别 1455: 12 个样本\n",
      "类别 1456: 12 个样本\n",
      "类别 1457: 12 个样本\n",
      "类别 1458: 12 个样本\n",
      "类别 1459: 12 个样本\n",
      "类别 1460: 12 个样本\n",
      "类别 1461: 12 个样本\n",
      "类别 1462: 12 个样本\n",
      "类别 1463: 12 个样本\n",
      "类别 1464: 12 个样本\n",
      "类别 1465: 12 个样本\n",
      "类别 1466: 12 个样本\n",
      "类别 1467: 12 个样本\n",
      "类别 1468: 12 个样本\n",
      "类别 1469: 12 个样本\n",
      "类别 1470: 12 个样本\n",
      "类别 1471: 12 个样本\n",
      "类别 1472: 12 个样本\n",
      "类别 1473: 12 个样本\n",
      "类别 1474: 12 个样本\n",
      "类别 1475: 12 个样本\n",
      "类别 1476: 12 个样本\n",
      "类别 1477: 12 个样本\n",
      "类别 1478: 12 个样本\n",
      "类别 1479: 12 个样本\n",
      "类别 1480: 12 个样本\n",
      "类别 1481: 12 个样本\n",
      "类别 1482: 12 个样本\n",
      "类别 1483: 12 个样本\n",
      "类别 1484: 12 个样本\n",
      "类别 1485: 12 个样本\n",
      "类别 1486: 12 个样本\n",
      "类别 1487: 12 个样本\n",
      "类别 1488: 12 个样本\n",
      "类别 1489: 12 个样本\n",
      "类别 1490: 12 个样本\n",
      "类别 1491: 12 个样本\n",
      "类别 1492: 12 个样本\n",
      "类别 1493: 12 个样本\n",
      "类别 1494: 12 个样本\n",
      "类别 1495: 12 个样本\n",
      "类别 1496: 12 个样本\n",
      "类别 1497: 12 个样本\n",
      "类别 1498: 12 个样本\n",
      "类别 1499: 12 个样本\n",
      "类别 1500: 12 个样本\n",
      "类别 1501: 12 个样本\n",
      "类别 1502: 12 个样本\n",
      "类别 1503: 12 个样本\n",
      "类别 1504: 12 个样本\n",
      "类别 1505: 12 个样本\n",
      "类别 1506: 12 个样本\n",
      "类别 1507: 12 个样本\n",
      "类别 1508: 12 个样本\n",
      "类别 1509: 12 个样本\n",
      "类别 1510: 13 个样本\n",
      "类别 1511: 12 个样本\n",
      "类别 1512: 12 个样本\n",
      "类别 1513: 12 个样本\n",
      "类别 1514: 12 个样本\n",
      "类别 1515: 12 个样本\n",
      "类别 1516: 12 个样本\n",
      "类别 1517: 12 个样本\n",
      "类别 1518: 12 个样本\n",
      "类别 1519: 12 个样本\n",
      "类别 1520: 13 个样本\n",
      "类别 1521: 12 个样本\n",
      "类别 1522: 12 个样本\n",
      "类别 1523: 12 个样本\n",
      "类别 1524: 12 个样本\n",
      "类别 1525: 12 个样本\n",
      "类别 1526: 12 个样本\n",
      "类别 1527: 12 个样本\n",
      "类别 1528: 12 个样本\n",
      "类别 1529: 12 个样本\n",
      "类别 1530: 12 个样本\n",
      "类别 1531: 12 个样本\n",
      "类别 1532: 12 个样本\n",
      "类别 1533: 12 个样本\n",
      "类别 1534: 12 个样本\n",
      "类别 1535: 13 个样本\n",
      "类别 1536: 12 个样本\n",
      "类别 1537: 12 个样本\n",
      "类别 1538: 12 个样本\n",
      "类别 1539: 12 个样本\n",
      "类别 1540: 12 个样本\n",
      "类别 1541: 12 个样本\n",
      "类别 1542: 13 个样本\n",
      "类别 1543: 12 个样本\n",
      "类别 1544: 12 个样本\n",
      "类别 1545: 12 个样本\n",
      "类别 1546: 12 个样本\n",
      "类别 1547: 12 个样本\n",
      "类别 1548: 12 个样本\n",
      "类别 1549: 12 个样本\n",
      "类别 1550: 12 个样本\n",
      "类别 1551: 12 个样本\n",
      "类别 1552: 12 个样本\n",
      "类别 1553: 12 个样本\n",
      "类别 1554: 12 个样本\n",
      "类别 1555: 13 个样本\n",
      "类别 1556: 12 个样本\n",
      "类别 1557: 12 个样本\n",
      "类别 1558: 12 个样本\n",
      "类别 1559: 12 个样本\n",
      "类别 1560: 12 个样本\n",
      "类别 1561: 12 个样本\n",
      "类别 1562: 12 个样本\n",
      "类别 1563: 13 个样本\n",
      "类别 1564: 12 个样本\n",
      "类别 1565: 12 个样本\n",
      "类别 1566: 12 个样本\n",
      "类别 1567: 13 个样本\n",
      "类别 1568: 12 个样本\n",
      "类别 1569: 12 个样本\n",
      "类别 1570: 12 个样本\n",
      "类别 1571: 12 个样本\n",
      "类别 1572: 13 个样本\n",
      "类别 1573: 12 个样本\n",
      "类别 1574: 12 个样本\n",
      "类别 1575: 12 个样本\n",
      "类别 1576: 12 个样本\n",
      "类别 1577: 12 个样本\n",
      "类别 1578: 12 个样本\n",
      "类别 1579: 12 个样本\n",
      "类别 1580: 12 个样本\n",
      "类别 1581: 12 个样本\n",
      "类别 1582: 12 个样本\n",
      "类别 1583: 13 个样本\n",
      "类别 1584: 13 个样本\n",
      "类别 1585: 12 个样本\n",
      "类别 1586: 12 个样本\n",
      "类别 1587: 12 个样本\n",
      "类别 1588: 12 个样本\n",
      "类别 1589: 12 个样本\n",
      "类别 1590: 13 个样本\n",
      "类别 1591: 12 个样本\n",
      "类别 1592: 12 个样本\n",
      "类别 1593: 12 个样本\n",
      "类别 1594: 12 个样本\n",
      "类别 1595: 12 个样本\n",
      "类别 1596: 12 个样本\n",
      "类别 1597: 12 个样本\n",
      "类别 1598: 12 个样本\n",
      "类别 1599: 12 个样本\n",
      "类别 1600: 12 个样本\n",
      "类别 1601: 12 个样本\n",
      "类别 1602: 12 个样本\n",
      "类别 1603: 13 个样本\n",
      "类别 1604: 12 个样本\n",
      "类别 1605: 12 个样本\n",
      "类别 1606: 12 个样本\n",
      "类别 1607: 13 个样本\n",
      "类别 1608: 12 个样本\n",
      "类别 1609: 12 个样本\n",
      "类别 1610: 12 个样本\n",
      "类别 1611: 12 个样本\n",
      "类别 1612: 12 个样本\n",
      "类别 1613: 12 个样本\n",
      "类别 1614: 13 个样本\n",
      "类别 1615: 12 个样本\n",
      "类别 1616: 12 个样本\n",
      "类别 1617: 12 个样本\n",
      "类别 1618: 13 个样本\n",
      "类别 1619: 13 个样本\n",
      "类别 1620: 12 个样本\n",
      "类别 1621: 12 个样本\n",
      "类别 1622: 12 个样本\n",
      "类别 1623: 12 个样本\n",
      "类别 1624: 12 个样本\n",
      "类别 1625: 12 个样本\n",
      "类别 1626: 12 个样本\n",
      "类别 1627: 12 个样本\n",
      "类别 1628: 12 个样本\n",
      "类别 1629: 12 个样本\n",
      "类别 1630: 13 个样本\n",
      "类别 1631: 12 个样本\n",
      "类别 1632: 12 个样本\n",
      "类别 1633: 12 个样本\n",
      "类别 1634: 12 个样本\n",
      "类别 1635: 12 个样本\n",
      "类别 1636: 12 个样本\n",
      "类别 1637: 12 个样本\n",
      "类别 1638: 12 个样本\n",
      "类别 1639: 12 个样本\n",
      "类别 1640: 12 个样本\n",
      "类别 1641: 13 个样本\n",
      "类别 1642: 13 个样本\n",
      "类别 1643: 12 个样本\n",
      "类别 1644: 13 个样本\n",
      "类别 1645: 12 个样本\n",
      "类别 1646: 12 个样本\n",
      "类别 1647: 12 个样本\n",
      "类别 1648: 12 个样本\n",
      "类别 1649: 13 个样本\n",
      "类别 1650: 12 个样本\n",
      "类别 1651: 12 个样本\n",
      "类别 1652: 12 个样本\n",
      "类别 1653: 12 个样本\n",
      "类别 1654: 12 个样本\n",
      "类别 1655: 13 个样本\n",
      "类别 1656: 12 个样本\n",
      "类别 1657: 12 个样本\n",
      "类别 1658: 12 个样本\n",
      "类别 1659: 12 个样本\n",
      "类别 1660: 12 个样本\n",
      "类别 1661: 12 个样本\n",
      "类别 1662: 12 个样本\n",
      "类别 1663: 12 个样本\n",
      "类别 1664: 13 个样本\n",
      "类别 1665: 12 个样本\n",
      "类别 1666: 12 个样本\n",
      "类别 1667: 12 个样本\n",
      "类别 1668: 12 个样本\n",
      "类别 1669: 12 个样本\n",
      "类别 1670: 12 个样本\n",
      "类别 1671: 12 个样本\n",
      "类别 1672: 12 个样本\n",
      "类别 1673: 13 个样本\n",
      "类别 1674: 12 个样本\n",
      "类别 1675: 12 个样本\n",
      "类别 1676: 12 个样本\n",
      "类别 1677: 12 个样本\n",
      "类别 1678: 12 个样本\n",
      "类别 1679: 12 个样本\n",
      "类别 1680: 12 个样本\n",
      "类别 1681: 12 个样本\n",
      "类别 1682: 13 个样本\n",
      "类别 1683: 12 个样本\n",
      "类别 1684: 12 个样本\n",
      "类别 1685: 12 个样本\n",
      "类别 1686: 12 个样本\n",
      "类别 1687: 12 个样本\n",
      "类别 1688: 12 个样本\n",
      "类别 1689: 12 个样本\n",
      "类别 1690: 12 个样本\n",
      "类别 1691: 12 个样本\n",
      "类别 1692: 12 个样本\n",
      "类别 1693: 12 个样本\n",
      "类别 1694: 12 个样本\n",
      "类别 1695: 12 个样本\n",
      "类别 1696: 12 个样本\n",
      "类别 1697: 12 个样本\n",
      "类别 1698: 12 个样本\n",
      "类别 1699: 12 个样本\n",
      "类别 1700: 12 个样本\n",
      "类别 1701: 12 个样本\n",
      "类别 1702: 12 个样本\n",
      "类别 1703: 12 个样本\n",
      "类别 1704: 12 个样本\n",
      "类别 1705: 12 个样本\n",
      "类别 1706: 12 个样本\n",
      "类别 1707: 13 个样本\n",
      "类别 1708: 12 个样本\n",
      "类别 1709: 12 个样本\n",
      "类别 1710: 12 个样本\n",
      "类别 1711: 12 个样本\n",
      "类别 1712: 12 个样本\n",
      "类别 1713: 12 个样本\n",
      "类别 1714: 12 个样本\n",
      "类别 1715: 12 个样本\n",
      "类别 1716: 12 个样本\n",
      "类别 1717: 12 个样本\n",
      "类别 1718: 12 个样本\n",
      "类别 1719: 12 个样本\n",
      "类别 1720: 13 个样本\n",
      "类别 1721: 12 个样本\n",
      "类别 1722: 12 个样本\n",
      "类别 1723: 12 个样本\n",
      "类别 1724: 12 个样本\n",
      "类别 1725: 12 个样本\n",
      "类别 1726: 12 个样本\n",
      "类别 1727: 12 个样本\n",
      "类别 1728: 12 个样本\n",
      "类别 1729: 12 个样本\n",
      "类别 1730: 12 个样本\n",
      "类别 1731: 12 个样本\n",
      "类别 1732: 12 个样本\n",
      "类别 1733: 12 个样本\n",
      "类别 1734: 12 个样本\n",
      "类别 1735: 12 个样本\n",
      "类别 1736: 12 个样本\n",
      "类别 1737: 13 个样本\n",
      "类别 1738: 12 个样本\n",
      "类别 1739: 13 个样本\n",
      "类别 1740: 12 个样本\n",
      "类别 1741: 12 个样本\n",
      "类别 1742: 13 个样本\n",
      "类别 1743: 12 个样本\n",
      "类别 1744: 13 个样本\n",
      "类别 1745: 12 个样本\n",
      "类别 1746: 12 个样本\n",
      "类别 1747: 12 个样本\n",
      "类别 1748: 13 个样本\n",
      "类别 1749: 13 个样本\n",
      "类别 1750: 12 个样本\n",
      "类别 1751: 12 个样本\n",
      "类别 1752: 12 个样本\n",
      "类别 1753: 12 个样本\n",
      "类别 1754: 13 个样本\n",
      "类别 1755: 12 个样本\n",
      "类别 1756: 12 个样本\n",
      "类别 1757: 13 个样本\n",
      "类别 1758: 12 个样本\n",
      "类别 1759: 12 个样本\n",
      "类别 1760: 12 个样本\n",
      "类别 1761: 12 个样本\n",
      "类别 1762: 12 个样本\n",
      "类别 1763: 12 个样本\n",
      "类别 1764: 12 个样本\n",
      "类别 1765: 12 个样本\n",
      "类别 1766: 12 个样本\n",
      "类别 1767: 12 个样本\n",
      "类别 1768: 12 个样本\n",
      "类别 1769: 12 个样本\n",
      "类别 1770: 12 个样本\n",
      "类别 1771: 12 个样本\n",
      "类别 1772: 13 个样本\n",
      "类别 1773: 12 个样本\n",
      "类别 1774: 12 个样本\n",
      "类别 1775: 12 个样本\n",
      "类别 1776: 12 个样本\n",
      "类别 1777: 12 个样本\n",
      "类别 1778: 13 个样本\n",
      "类别 1779: 12 个样本\n",
      "类别 1780: 12 个样本\n",
      "类别 1781: 12 个样本\n",
      "类别 1782: 12 个样本\n",
      "类别 1783: 12 个样本\n",
      "类别 1784: 12 个样本\n",
      "类别 1785: 12 个样本\n",
      "类别 1786: 13 个样本\n",
      "类别 1787: 13 个样本\n",
      "类别 1788: 12 个样本\n",
      "类别 1789: 12 个样本\n",
      "类别 1790: 12 个样本\n",
      "类别 1791: 12 个样本\n",
      "类别 1792: 12 个样本\n",
      "类别 1793: 12 个样本\n",
      "类别 1794: 12 个样本\n",
      "类别 1795: 12 个样本\n",
      "类别 1796: 12 个样本\n",
      "类别 1797: 12 个样本\n",
      "类别 1798: 12 个样本\n",
      "类别 1799: 12 个样本\n",
      "类别 1800: 12 个样本\n",
      "类别 1801: 12 个样本\n",
      "类别 1802: 12 个样本\n",
      "类别 1803: 12 个样本\n",
      "类别 1804: 12 个样本\n",
      "类别 1805: 12 个样本\n",
      "类别 1806: 12 个样本\n",
      "类别 1807: 12 个样本\n",
      "类别 1808: 12 个样本\n",
      "类别 1809: 12 个样本\n",
      "类别 1810: 13 个样本\n",
      "类别 1811: 12 个样本\n",
      "类别 1812: 12 个样本\n",
      "类别 1813: 12 个样本\n",
      "类别 1814: 12 个样本\n",
      "类别 1815: 12 个样本\n",
      "类别 1816: 12 个样本\n",
      "类别 1817: 13 个样本\n",
      "类别 1818: 12 个样本\n",
      "类别 1819: 12 个样本\n",
      "类别 1820: 12 个样本\n",
      "类别 1821: 12 个样本\n",
      "类别 1822: 12 个样本\n",
      "类别 1823: 12 个样本\n",
      "类别 1824: 12 个样本\n",
      "类别 1825: 12 个样本\n",
      "类别 1826: 12 个样本\n",
      "类别 1827: 12 个样本\n",
      "类别 1828: 12 个样本\n",
      "类别 1829: 12 个样本\n",
      "类别 1830: 12 个样本\n",
      "类别 1831: 12 个样本\n",
      "类别 1832: 12 个样本\n",
      "类别 1833: 12 个样本\n",
      "类别 1834: 12 个样本\n",
      "类别 1835: 12 个样本\n",
      "类别 1836: 12 个样本\n",
      "类别 1837: 12 个样本\n",
      "类别 1838: 12 个样本\n",
      "类别 1839: 12 个样本\n",
      "类别 1840: 12 个样本\n",
      "类别 1841: 12 个样本\n",
      "类别 1842: 12 个样本\n",
      "类别 1843: 12 个样本\n",
      "类别 1844: 12 个样本\n",
      "类别 1845: 12 个样本\n",
      "类别 1846: 12 个样本\n",
      "类别 1847: 12 个样本\n",
      "类别 1848: 12 个样本\n",
      "类别 1849: 12 个样本\n",
      "类别 1850: 12 个样本\n",
      "类别 1851: 12 个样本\n",
      "类别 1852: 12 个样本\n",
      "类别 1853: 12 个样本\n",
      "类别 1854: 12 个样本\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mne\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "# 读取 meg 类别标签文件\n",
    "csv_file_path = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Metadata/Concept-specific/image_concept_index.csv\"\n",
    "concept_df = pd.read_csv(csv_file_path, header=None, names=['Category_Label'])\n",
    "# 检查行号是否与 event_id 一致\n",
    "# 如果每行对应的行号就是 event_id，则行号即是 event_id\n",
    "concept_df['Event_ID'] = concept_df.index + 1  # 添加 Event_ID 列\n",
    "\n",
    "\n",
    "def count_samples_per_category(event_ids, concept_df):\n",
    "    # 筛选出在过滤后的 event_ids 中的类别标签\n",
    "    filtered_concept_df = concept_df[concept_df['Event_ID'].isin(event_ids)]\n",
    "\n",
    "    # 统计每个类别在过滤后的数据中的样本数量\n",
    "    category_counts = filtered_concept_df['Category_Label'].value_counts().sort_index()\n",
    "\n",
    "    # 将统计结果转为字典并返回\n",
    "    return category_counts.to_dict()\n",
    "\n",
    "# 调用函数进行统计\n",
    "category_counts = count_samples_per_category(epochs.events[:, 2], concept_df)\n",
    "\n",
    "# 输出结果\n",
    "print(\"每个类别在过滤后的 MEG 数据中样本数量:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"类别 {category}: {count} 个样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     2,     3, ..., 26104, 26105, 26106], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs.events[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目录设置\n",
    "csv_img_file_path = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Metadata/Image-specific/image_paths.csv\"\n",
    "base_fif_dir = \"/mnt/dataset0/ldy/datasets/meg_dataset/original_preprocessed/preprocessed\"\n",
    "source_image_dir = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Images/\"\n",
    "# meg_image_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/images_set\"\n",
    "meg_image_dir = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/images_set\"\n",
    "output_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/Preprocessed\"\n",
    "filtered_image_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/images_set_filter\"\n",
    "\n",
    "\n",
    "def build_category_prefix_mapping(meg_image_dir):\n",
    "    category_prefix_mapping = {}\n",
    "    for split in [\"training_images/images\", \"test_images/images\"]:\n",
    "        split_dir = os.path.join(meg_image_dir, split)\n",
    "        if os.path.isdir(split_dir):\n",
    "            for category_with_prefix in os.listdir(split_dir):\n",
    "                prefix, category = category_with_prefix.split(\"_\", 1)\n",
    "                category_prefix_mapping[category] = category_with_prefix\n",
    "    return category_prefix_mapping\n",
    "\n",
    "\n",
    "def get_full_image_event_mapping(csv_img_file_path, category_prefix_mapping):\n",
    "    full_image_event_mapping = {}\n",
    "    \n",
    "    # 使用 CSV 文件中图像路径顺序创建 event_id 映射\n",
    "    df = pd.read_csv(csv_img_file_path, header=None)\n",
    "    for event_id, row in enumerate(df[0], start=1):\n",
    "        image_path = row\n",
    "        \n",
    "        category = image_path.split(\"/\")[1]  # 假设类别是文件夹名称\n",
    "        # print(image_path.split(\"/\"))\n",
    "        if category in category_prefix_mapping:\n",
    "            category_with_prefix = category_prefix_mapping[category]\n",
    "            img = os.path.basename(image_path)\n",
    "            full_image_event_mapping[event_id] = (category_with_prefix, img)\n",
    "            \n",
    "    return full_image_event_mapping\n",
    "\n",
    "\n",
    "def filter_and_copy_images(epochs, image_event_mapping_train, image_event_mapping_test, category_limit=12):\n",
    "    image_count_test = defaultdict(int)\n",
    "    image_count_train = defaultdict(int)\n",
    "    test_indices = []\n",
    "    train_indices = []\n",
    "    incomplete_categories = []\n",
    "\n",
    "    # 按 event_id 对 epochs 进行排序\n",
    "    sorted_indices = np.argsort(epochs.events[:, 2])\n",
    "    epochs = epochs[sorted_indices]\n",
    "\n",
    "    # 新建输出目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 仅遍历 epochs 一次，区分训练集和测试集\n",
    "    for epoch_idx in range(len(epochs)):\n",
    "        event_id = epochs.events[epoch_idx, 2]\n",
    "\n",
    "        # 判断事件是否属于测试集\n",
    "        if event_id in image_event_mapping_test:\n",
    "            category_with_prefix, img = image_event_mapping_test[event_id]\n",
    "            # 检查测试集每个类别的图像数量是否已达到限制\n",
    "            if image_count_test[category_with_prefix] < category_limit:\n",
    "                test_indices.append(epoch_idx)  # 记录测试集样本的索引\n",
    "                image_count_test[category_with_prefix] += 1\n",
    "\n",
    "        # 判断事件是否属于训练集\n",
    "        elif event_id in image_event_mapping_train:\n",
    "            category_with_prefix, img = image_event_mapping_train[event_id]\n",
    "            # 检查训练集每个类别的图像数量是否已达到限制\n",
    "            if image_count_train[category_with_prefix] < category_limit:\n",
    "                train_indices.append(epoch_idx)  # 记录训练集样本的索引\n",
    "                image_count_train[category_with_prefix] += 1\n",
    "\n",
    "    # 根据索引提取过滤后的 train_epochs 和 test_epochs\n",
    "    train_epochs = epochs[train_indices]\n",
    "    test_epochs = epochs[test_indices]\n",
    "\n",
    "    # 检查测试集和训练集中是否存在不满足12张图像的类别\n",
    "    for category, count in image_count_test.items():\n",
    "        if count < category_limit:\n",
    "            incomplete_categories.append((category, count))\n",
    "    for category, count in image_count_train.items():\n",
    "        if count < category_limit:\n",
    "            incomplete_categories.append((category, count))\n",
    "\n",
    "    return train_epochs, test_epochs, incomplete_categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 category_prefix_mapping 和 image_event_mapping\n",
    "category_prefix_mapping = build_category_prefix_mapping(meg_image_dir)\n",
    "full_image_event_mapping = get_full_image_event_mapping(csv_img_file_path, category_prefix_mapping)\n",
    "\n",
    "image_event_mapping_train = {}\n",
    "image_event_mapping_test = {}\n",
    "\n",
    "# 将事件分类到训练集和测试集\n",
    "for event_id, (category_with_prefix, img) in full_image_event_mapping.items():\n",
    "    train_path = os.path.join(meg_image_dir, \"training_images/images\", category_with_prefix, img)\n",
    "    test_path = os.path.join(meg_image_dir, \"test_images/images\", category_with_prefix, img)\n",
    "    # print(test_path)\n",
    "    if os.path.exists(train_path):\n",
    "        image_event_mapping_train[event_id] = (category_with_prefix, img)\n",
    "    elif os.path.exists(test_path):\n",
    "        image_event_mapping_test[event_id] = (category_with_prefix, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主处理代码\n",
    "filtered_train_event_ids = []\n",
    "filtered_test_event_ids = []\n",
    "\n",
    "# for subject_id, fif_filename in subjects:\n",
    "#     fif_file = os.path.join(base_fif_dir, fif_filename)\n",
    "#     epochs = load_and_crop_epochs(fif_file)\n",
    "    \n",
    "# 使用修改后的函数来划分训练集和测试集\n",
    "train_epochs, test_epochs, incomplete_categories = filter_and_copy_images(\n",
    "    epochs, image_event_mapping_train, image_event_mapping_test, category_limit=12 )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('00010_aircraft_carrier', 1),\n",
       " ('00013_alligator', 1),\n",
       " ('00017_altar', 1),\n",
       " ('00018_aluminum_foil', 1),\n",
       " ('00042_ashtray', 1),\n",
       " ('00044_avocado', 1),\n",
       " ('00046_axe', 1),\n",
       " ('00047_baby', 1),\n",
       " ('00055_bag', 1),\n",
       " ('00056_bagel', 1),\n",
       " ('00060_ball', 1),\n",
       " ('00061_balloon', 1),\n",
       " ('00063_bamboo', 1),\n",
       " ('00064_banana', 1),\n",
       " ('00086_bassinet', 1),\n",
       " ('00098_battery', 1),\n",
       " ('00100_beachball', 1),\n",
       " ('00103_bean', 1),\n",
       " ('00106_bear', 1),\n",
       " ('00108_beaver', 1),\n",
       " ('00109_bed', 1),\n",
       " ('00112_bee', 1),\n",
       " ('00114_beer', 1),\n",
       " ('00117_bell', 1),\n",
       " ('00121_bench', 1),\n",
       " ('00124_bike', 1),\n",
       " ('00141_blind', 1),\n",
       " ('00149_blowtorch', 1),\n",
       " ('00151_boa', 1),\n",
       " ('00155_boat', 1),\n",
       " ('00156_bobsled', 1),\n",
       " ('00183_boxer_shorts', 1),\n",
       " ('00187_brace', 1),\n",
       " ('00202_broccoli', 1),\n",
       " ('00205_brownie', 1),\n",
       " ('00216_bulldozer', 1),\n",
       " ('00217_bullet', 1),\n",
       " ('00230_butterfly', 1),\n",
       " ('00246_camel', 1),\n",
       " ('00253_candelabra', 1),\n",
       " ('00263_canoe', 1),\n",
       " ('00274_cardigan', 1),\n",
       " ('00282_cashew', 1),\n",
       " ('00287_caterpillar', 1),\n",
       " ('00313_cheese', 1),\n",
       " ('00316_cheetah', 1),\n",
       " ('00320_chest1', 1),\n",
       " ('00335_chipmunk', 1),\n",
       " ('00356_clipboard', 1),\n",
       " ('00361_closet', 1),\n",
       " ('00371_coat_rack', 1),\n",
       " ('00396_computer_screen', 1),\n",
       " ('00401_cookie', 1),\n",
       " ('00422_cotton_candy', 1),\n",
       " ('00423_couch', 1),\n",
       " ('00427_cow', 1),\n",
       " ('00433_crank', 1),\n",
       " ('00436_crayon', 1),\n",
       " ('00442_crib', 1),\n",
       " ('00458_cucumber', 1),\n",
       " ('00459_cufflink', 1),\n",
       " ('00462_cupcake', 1),\n",
       " ('00469_cymbal', 1),\n",
       " ('00478_defibrillator', 1),\n",
       " ('00490_dish', 1),\n",
       " ('00502_dolly', 1),\n",
       " ('00506_donut', 1),\n",
       " ('00514_dough', 1),\n",
       " ('00515_dragonfly', 1),\n",
       " ('00516_drain', 1),\n",
       " ('00517_drawer', 1),\n",
       " ('00519_dress', 1),\n",
       " ('00520_dresser', 1),\n",
       " ('00541_earring', 1),\n",
       " ('00543_easel', 1),\n",
       " ('00549_eggbeater', 1),\n",
       " ('00564_extinguisher', 1),\n",
       " ('00580_ferris_wheel', 1),\n",
       " ('00601_fish', 1),\n",
       " ('00618_floss', 1),\n",
       " ('00620_flower', 1),\n",
       " ('00631_football_helmet', 1),\n",
       " ('00633_footprint', 1),\n",
       " ('00648_fudge', 1),\n",
       " ('00670_gel', 1),\n",
       " ('00698_graffiti', 1),\n",
       " ('00703_grape', 1),\n",
       " ('00708_grate', 1),\n",
       " ('00720_guacamole', 1),\n",
       " ('00721_guardrail', 1),\n",
       " ('00735_hairbrush', 1),\n",
       " ('00762_hay', 1),\n",
       " ('00765_headlamp', 1),\n",
       " ('00767_headphones', 1),\n",
       " ('00775_hedgehog', 1),\n",
       " ('00776_helicopter', 1),\n",
       " ('00782_hippopotamus', 1),\n",
       " ('00799_horse', 1),\n",
       " ('00800_horseshoe', 1),\n",
       " ('00809_hovercraft', 1),\n",
       " ('00811_hula_hoop', 1),\n",
       " ('00826_iguana', 1),\n",
       " ('00833_iron', 1),\n",
       " ('00840_jam', 1),\n",
       " ('00841_jar', 1),\n",
       " ('00853_joystick', 1),\n",
       " ('00865_kazoo', 1),\n",
       " ('00870_key', 1),\n",
       " ('00874_kimono', 1),\n",
       " ('00880_knife', 1),\n",
       " ('00887_ladder', 1),\n",
       " ('00899_lasagna', 1),\n",
       " ('00906_lawnmower', 1),\n",
       " ('00916_lemonade', 1),\n",
       " ('00932_limousine', 1),\n",
       " ('00934_lion', 1),\n",
       " ('00961_mailbox', 1),\n",
       " ('00967_mango', 1),\n",
       " ('00968_manhole', 1),\n",
       " ('00978_marshmallow', 1),\n",
       " ('00988_meat', 1),\n",
       " ('00990_meatball', 1),\n",
       " ('01000_microscope', 1),\n",
       " ('01003_milkshake', 1),\n",
       " ('01018_monkey', 1),\n",
       " ('01021_mosquito', 1),\n",
       " ('01022_mosquito_net', 1),\n",
       " ('01030_mousetrap', 1),\n",
       " ('01057_nest', 1),\n",
       " ('01064_nose', 1),\n",
       " ('01070_oatmeal', 1),\n",
       " ('01075_okra', 1),\n",
       " ('01077_omelet', 1),\n",
       " ('01094_pacifier', 1),\n",
       " ('01096_padlock', 1),\n",
       " ('01104_pan', 1),\n",
       " ('01106_panda', 1),\n",
       " ('01130_peach', 1),\n",
       " ('01134_pear', 1),\n",
       " ('01146_penguin', 1),\n",
       " ('01166_piano', 1),\n",
       " ('01169_pie', 1),\n",
       " ('01199_platypus', 1),\n",
       " ('01213_polaroid', 1),\n",
       " ('01231_pot', 1),\n",
       " ('01234_pothole', 1),\n",
       " ('01254_pumpkin', 1),\n",
       " ('01264_quill', 1),\n",
       " ('01266_rabbit', 1),\n",
       " ('01309_ribbon', 1),\n",
       " ('01316_road_sign', 1),\n",
       " ('01392_screwdriver', 1),\n",
       " ('01403_seesaw', 1),\n",
       " ('01407_sewing_kit', 1),\n",
       " ('01419_shell2', 1),\n",
       " ('01424_shoe', 1),\n",
       " ('01438_shredder', 1),\n",
       " ('01447_sim_card', 1),\n",
       " ('01448_sink', 1),\n",
       " ('01510_speaker', 1),\n",
       " ('01520_spoon', 1),\n",
       " ('01535_stalagmite', 1),\n",
       " ('01542_starfish', 1),\n",
       " ('01555_stiletto', 1),\n",
       " ('01563_stopwatch', 1),\n",
       " ('01567_strainer', 1),\n",
       " ('01572_streetlight', 1),\n",
       " ('01583_sundae', 1),\n",
       " ('01584_sundial', 1),\n",
       " ('01590_suspenders', 1),\n",
       " ('01603_sword', 1),\n",
       " ('01607_t-shirt', 1),\n",
       " ('01614_taco', 1),\n",
       " ('01618_taillight', 1),\n",
       " ('01619_tamale', 1),\n",
       " ('01630_tattoo', 1),\n",
       " ('01641_telescope', 1),\n",
       " ('01642_television', 1),\n",
       " ('01644_tent', 1),\n",
       " ('01649_thermostat', 1),\n",
       " ('01655_thumbtack', 1),\n",
       " ('01664_tiramisu', 1),\n",
       " ('01673_tomato', 1),\n",
       " ('01682_toothpick', 1),\n",
       " ('01707_treadmill', 1),\n",
       " ('01720_trowel', 1),\n",
       " ('01737_turtle', 1),\n",
       " ('01739_tuxedo', 1),\n",
       " ('01742_typewriter', 1),\n",
       " ('01744_umbrella', 1),\n",
       " ('01748_uniform', 1),\n",
       " ('01749_urinal', 1),\n",
       " ('01754_vase', 1),\n",
       " ('01757_velcro', 1),\n",
       " ('01772_waffle_iron', 1),\n",
       " ('01778_wallpaper', 1),\n",
       " ('01786_wasp', 1),\n",
       " ('01787_watch', 1),\n",
       " ('01810_whip', 1),\n",
       " ('01817_wig', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incomplete_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_epochs\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "len(train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 148,  191,  244,  264,  591,  619,  646,  663,  808,  828,  895,\n",
       "        914,  944,  960, 1279, 1481, 1510, 1556, 1610, 1640, 1664, 1719,\n",
       "       1748, 1794, 1858, 1899, 2159, 2286, 2312, 2378, 2393, 2783, 2839,\n",
       "       3061, 3099, 3255, 3268, 3435, 3651, 3739, 3880, 4030, 4137, 4205,\n",
       "       4562, 4603, 4654, 4851, 5161, 5226], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_epochs.events[:, 2][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.005, 0.01 , 0.015, 0.02 , 0.025, 0.03 , 0.035, 0.04 ,\n",
       "       0.045, 0.05 , 0.055, 0.06 , 0.065, 0.07 , 0.075, 0.08 , 0.085,\n",
       "       0.09 , 0.095, 0.1  , 0.105, 0.11 , 0.115, 0.12 , 0.125, 0.13 ,\n",
       "       0.135, 0.14 , 0.145, 0.15 , 0.155, 0.16 , 0.165, 0.17 , 0.175,\n",
       "       0.18 , 0.185, 0.19 , 0.195, 0.2  , 0.205, 0.21 , 0.215, 0.22 ,\n",
       "       0.225, 0.23 , 0.235, 0.24 , 0.245, 0.25 , 0.255, 0.26 , 0.265,\n",
       "       0.27 , 0.275, 0.28 , 0.285, 0.29 , 0.295, 0.3  , 0.305, 0.31 ,\n",
       "       0.315, 0.32 , 0.325, 0.33 , 0.335, 0.34 , 0.345, 0.35 , 0.355,\n",
       "       0.36 , 0.365, 0.37 , 0.375, 0.38 , 0.385, 0.39 , 0.395, 0.4  ,\n",
       "       0.405, 0.41 , 0.415, 0.42 , 0.425, 0.43 , 0.435, 0.44 , 0.445,\n",
       "       0.45 , 0.455, 0.46 , 0.465, 0.47 , 0.475, 0.48 , 0.485, 0.49 ,\n",
       "       0.495, 0.5  , 0.505, 0.51 , 0.515, 0.52 , 0.525, 0.53 , 0.535,\n",
       "       0.54 , 0.545, 0.55 , 0.555, 0.56 , 0.565, 0.57 , 0.575, 0.58 ,\n",
       "       0.585, 0.59 , 0.595, 0.6  , 0.605, 0.61 , 0.615, 0.62 , 0.625,\n",
       "       0.63 , 0.635, 0.64 , 0.645, 0.65 , 0.655, 0.66 , 0.665, 0.67 ,\n",
       "       0.675, 0.68 , 0.685, 0.69 , 0.695, 0.7  , 0.705, 0.71 , 0.715,\n",
       "       0.72 , 0.725, 0.73 , 0.735, 0.74 , 0.745, 0.75 , 0.755, 0.76 ,\n",
       "       0.765, 0.77 , 0.775, 0.78 , 0.785, 0.79 , 0.795, 0.8  , 0.805,\n",
       "       0.81 , 0.815, 0.82 , 0.825, 0.83 , 0.835, 0.84 , 0.845, 0.85 ,\n",
       "       0.855, 0.86 , 0.865, 0.87 , 0.875, 0.88 , 0.885, 0.89 , 0.895,\n",
       "       0.9  , 0.905, 0.91 , 0.915, 0.92 , 0.925, 0.93 , 0.935, 0.94 ,\n",
       "       0.945, 0.95 , 0.955, 0.96 , 0.965, 0.97 , 0.975, 0.98 , 0.985,\n",
       "       0.99 , 0.995, 1.   ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = train_epochs.times \n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/dataset0/ldy/datasets/THINGS_MEG/Preprocessed/sub-02'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成！\n"
     ]
    }
   ],
   "source": [
    "# 设置输出路径\n",
    "subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "os.makedirs(subject_output_dir, exist_ok=True)\n",
    "\n",
    "# 从训练和测试 epochs 中提取数据\n",
    "train_data = train_epochs.get_data()  # 提取数据\n",
    "test_data = test_epochs.get_data()  # 提取数据\n",
    "ch_names = train_epochs.ch_names  # 提取通道名称\n",
    "times = train_epochs.times  # 提取时间点信息\n",
    "\n",
    "# 使用 save_data 函数保存数据为 pkl 格式\n",
    "save_data({'meg_data': train_data, 'ch_names': ch_names, 'times': times},\n",
    "            os.path.join(subject_output_dir, \"preprocessed_meg_training.pkl\"))\n",
    "save_data({'meg_data': test_data, 'ch_names': ch_names, 'times': times},\n",
    "            os.path.join(subject_output_dir, \"preprocessed_meg_test.pkl\"))\n",
    "\n",
    "# # 输出不满足12张图像的类别\n",
    "# if incomplete_train_categories:\n",
    "#     print(f\"\\n训练集中不满足12张图像的类别（数量不足） for {subject_id}:\")\n",
    "#     for category, count in incomplete_train_categories:\n",
    "#         print(f\"  类别 '{category}': 仅 {count} 张图像\")\n",
    "\n",
    "# if incomplete_test_categories:\n",
    "#     print(f\"\\n测试集中不满足12张图像的类别（数量不足） for {subject_id}:\")\n",
    "#     for category, count in incomplete_test_categories:\n",
    "#         print(f\"  类别 '{category}': 仅 {count} 张图像\")\n",
    "\n",
    "print(\"处理完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mne\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_and_crop_epochs(fif_file):\n",
    "    epochs = mne.read_epochs(fif_file, preload=True)\n",
    "    epochs.crop(tmin=0, tmax=1.0)\n",
    "    \n",
    "    sorted_indices = np.argsort(epochs.events[:, 2])\n",
    "    epochs = epochs[sorted_indices]\n",
    "    \n",
    "    filtered_epochs = epochs[epochs.events[:, 2] != 999999]\n",
    "    unique_event_ids, counts = np.unique(epochs.events[:, 2], return_counts=True)\n",
    "    valid_epochs = filtered_epochs[np.isin(filtered_epochs.events[:, 2], unique_event_ids)]\n",
    "    # # 获取非 999999 的索引\n",
    "    # valid_indices = np.where(event_ids != 999999)[0]\n",
    "    # data_array = data_array[valid_indices]\n",
    "    # event_ids = event_ids[valid_indices]\n",
    "    \n",
    "    # # 获取唯一的 event_ids 的索引\n",
    "    # unique_indices = np.unique(event_ids, return_index=True)[1]\n",
    "    # data_array = data_array[unique_indices]\n",
    "    # event_ids = event_ids[unique_indices]\n",
    "    \n",
    "    return valid_epochs\n",
    "\n",
    "def build_category_prefix_mapping(meg_image_dir):\n",
    "    category_prefix_mapping = {}\n",
    "    for split in [\"training_images\", \"test_images\"]:\n",
    "        split_dir = os.path.join(meg_image_dir, split)\n",
    "        if os.path.isdir(split_dir):\n",
    "            for category_with_prefix in os.listdir(split_dir):\n",
    "                prefix, category = category_with_prefix.split(\"_\", 1)\n",
    "                category_prefix_mapping[category] = category_with_prefix\n",
    "    return category_prefix_mapping\n",
    "\n",
    "def get_full_image_event_mapping(csv_img_file_path, category_prefix_mapping):\n",
    "    full_image_event_mapping = {}\n",
    "    \n",
    "    # 使用 CSV 文件中图像路径顺序创建 event_id 映射\n",
    "    df = pd.read_csv(csv_img_file_path, header=None)\n",
    "    for event_id, row in enumerate(df[0], start=1):\n",
    "        image_path = row\n",
    "        category = image_path.split(\"/\")[1]  # 假设类别是文件夹名称\n",
    "        if category in category_prefix_mapping:\n",
    "            category_with_prefix = category_prefix_mapping[category]\n",
    "            img = os.path.basename(image_path)\n",
    "            full_image_event_mapping[event_id] = (category_with_prefix, img)\n",
    "            \n",
    "    return full_image_event_mapping\n",
    "\n",
    "def filter_and_copy_images(image_event_mapping, data_array, event_ids, output_array, output_dir, category_limit=12):\n",
    "    image_count = defaultdict(int)  # 用于记录每个类别的图像数量\n",
    "    incomplete_categories = []  # 用于记录不满足12张图像的类别\n",
    "    filtered_event_ids = []  # 用于存储过滤后的 event_id\n",
    "    \n",
    "    # 新建输出目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i, event_id in enumerate(event_ids):\n",
    "        if event_id not in image_event_mapping:\n",
    "            continue\n",
    "        category_with_prefix, img = image_event_mapping[event_id]\n",
    "        \n",
    "        # 检查每个类别的图像数量是否已达到限制\n",
    "        if image_count[category_with_prefix] < category_limit:\n",
    "            output_array.append(data_array[i])  # 保留该样本\n",
    "            filtered_event_ids.append(event_id)  # 将符合条件的 event_id 添加到过滤列表\n",
    "            image_count[category_with_prefix] += 1\n",
    "            \n",
    "            # 提取不带前缀的类别名\n",
    "            # category = category_with_prefix.split(\"_\", 1)[1]  # 提取不带前缀的类别名\n",
    "            \n",
    "            # 构建源路径（使用不带前缀的类别名）和目标路径\n",
    "            # source_path = os.path.join(source_image_dir, \"images\", category, img)  # 使用不带前缀的类别名\n",
    "            # category_output_dir = os.path.join(output_dir, category_with_prefix)  # 目标路径使用带前缀的类别名\n",
    "            # os.makedirs(category_output_dir, exist_ok=True)\n",
    "            # target_path = os.path.join(category_output_dir, img)\n",
    "            \n",
    "            # 拷贝文件\n",
    "            # shutil.copyfile(source_path, target_path)\n",
    "    \n",
    "    # 检查是否存在不满足12张图像的类别\n",
    "    for category, count in image_count.items():\n",
    "        if count < category_limit:\n",
    "            incomplete_categories.append((category, count))\n",
    "    \n",
    "    return incomplete_categories, filtered_event_ids\n",
    "\n",
    "# 目录设置\n",
    "csv_img_file_path = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Metadata/Image-specific/image_paths.csv\"\n",
    "base_fif_dir = \"/mnt/dataset0/ldy/datasets/meg_dataset/original_preprocessed/preprocessed\"\n",
    "source_image_dir = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Images/\"\n",
    "meg_image_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/images_set\"\n",
    "output_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/Preprocessed\"\n",
    "filtered_image_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/images_set_filter\"\n",
    "subjects = [\n",
    "    ('sub-02', 'preprocessed_P2-epo.fif'),\n",
    "]\n",
    "\n",
    "category_prefix_mapping = build_category_prefix_mapping(meg_image_dir)\n",
    "full_image_event_mapping = get_full_image_event_mapping(csv_img_file_path, category_prefix_mapping)\n",
    "\n",
    "image_event_mapping_train = {}\n",
    "image_event_mapping_test = {}\n",
    "\n",
    "for event_id, (category_with_prefix, img) in full_image_event_mapping.items():\n",
    "    train_path = os.path.join(meg_image_dir, \"training_images\", category_with_prefix, img)\n",
    "    test_path = os.path.join(meg_image_dir, \"test_images\", category_with_prefix, img)\n",
    "    if os.path.exists(train_path):\n",
    "        image_event_mapping_train[event_id] = (category_with_prefix, img)\n",
    "    elif os.path.exists(test_path):\n",
    "        image_event_mapping_test[event_id] = (category_with_prefix, img)\n",
    "\n",
    "# 主处理代码\n",
    "filtered_train_event_ids = []\n",
    "filtered_test_event_ids = []\n",
    "\n",
    "for subject_id, fif_filename in subjects:\n",
    "    fif_file = os.path.join(base_fif_dir, fif_filename)\n",
    "    data_array, event_ids, ch_names, times = load_and_crop_epochs(fif_file)\n",
    "    print(\"event_ids\", event_ids)\n",
    "    train_array = []\n",
    "    test_array = []\n",
    "    \n",
    "    # 过滤训练集和测试集，保留每个类别的前12张图像并获取过滤后的 event_ids\n",
    "    incomplete_train_categories, filtered_train_event_ids = filter_and_copy_images(\n",
    "        image_event_mapping_train, data_array, event_ids, train_array, os.path.join(filtered_image_dir, \"training_images\"))\n",
    "    incomplete_test_categories, filtered_test_event_ids = filter_and_copy_images(\n",
    "        image_event_mapping_test, data_array, event_ids, test_array, os.path.join(filtered_image_dir, \"test_images\"))\n",
    "\n",
    "    # 将结果转换为 numpy 数组\n",
    "    train_array = np.array(train_array)\n",
    "    test_array = np.array(test_array)\n",
    "    \n",
    "    subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "    os.makedirs(subject_output_dir, exist_ok=True)\n",
    "    \n",
    "    save_data({'meg_data': train_array, 'ch_names': ch_names, 'times': times},\n",
    "              os.path.join(subject_output_dir, \"preprocessed_meg_training.pkl\"))\n",
    "    save_data({'meg_data': test_array, 'ch_names': ch_names, 'times': times},\n",
    "              os.path.join(subject_output_dir, \"preprocessed_meg_test.pkl\"))\n",
    "\n",
    "    # 输出不满足12张图像的类别\n",
    "    if incomplete_train_categories:\n",
    "        print(f\"\\n训练集中不满足12张图像的类别（数量不足） for {subject_id}:\")\n",
    "        for category, count in incomplete_train_categories:\n",
    "            print(f\"  类别 '{category}': 仅 {count} 张图像\")\n",
    "    \n",
    "    if incomplete_test_categories:\n",
    "        print(f\"\\n测试集中不满足12张图像的类别（数量不足） for {subject_id}:\")\n",
    "        for category, count in incomplete_test_categories:\n",
    "            print(f\"  类别 '{category}': 仅 {count} 张图像\")\n",
    "\n",
    "# 处理完成后，直接使用 filtered_train_event_ids 和 filtered_test_event_ids\n",
    "print(\"处理完成！\")\n",
    "print(\"过滤后的训练集的 event_ids:\", filtered_train_event_ids)\n",
    "print(\"过滤后的测试集的 event_ids:\", filtered_test_event_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mne\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "def load_and_crop_epochs(fif_file):\n",
    "    epochs = mne.read_epochs(fif_file, preload=True)\n",
    "    epochs.crop(tmin=0, tmax=1.0)\n",
    "    data_array = epochs.get_data()\n",
    "    event_ids = epochs.events[:, 2]\n",
    "    \n",
    "    # 获取非 999999 的索引\n",
    "    valid_indices = np.where(event_ids != 999999)[0]\n",
    "    data_array = data_array[valid_indices]\n",
    "    event_ids = event_ids[valid_indices]\n",
    "    \n",
    "    # 获取唯一的 event_ids 的索引\n",
    "    unique_indices = np.unique(event_ids, return_index=True)[1]\n",
    "    data_array = data_array[unique_indices]\n",
    "    event_ids = event_ids[unique_indices]\n",
    "    \n",
    "    return data_array, event_ids, epochs.ch_names, epochs.times\n",
    "\n",
    "# 读取 meg 类别标签文件\n",
    "csv_file_path = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Metadata/Concept-specific/image_concept_index.csv\"\n",
    "concept_df = pd.read_csv(csv_file_path, header=None, names=['Category_Label'])\n",
    "\n",
    "# 检查行号是否与 event_id 一致\n",
    "# 如果每行对应的行号就是 event_id，则行号即是 event_id\n",
    "concept_df['Event_ID'] = concept_df.index + 1  # 添加 Event_ID 列\n",
    "\n",
    "def count_samples_per_category(event_ids, concept_df):\n",
    "    # 筛选出在过滤后的 event_ids 中的类别标签\n",
    "    filtered_concept_df = concept_df[concept_df['Event_ID'].isin(event_ids)]\n",
    "\n",
    "    # 统计每个类别在过滤后的数据中的样本数量\n",
    "    category_counts = filtered_concept_df['Category_Label'].value_counts().sort_index()\n",
    "\n",
    "    # 将统计结果转为字典并返回\n",
    "    return category_counts.to_dict()\n",
    "\n",
    "subjects = [\n",
    "    ('sub-02', 'preprocessed_P2-epo.fif'),\n",
    "]\n",
    "base_fif_dir = \"/mnt/dataset0/ldy/datasets/meg_dataset/original_preprocessed/preprocessed\"\n",
    "\n",
    "for subject_id, fif_filename in subjects:\n",
    "    fif_file = os.path.join(base_fif_dir, fif_filename)\n",
    "    data_array, event_ids, ch_names, times = load_and_crop_epochs(fif_file)\n",
    "\n",
    "    # # 调用函数进行统计\n",
    "    # category_counts = count_samples_per_category(event_ids, concept_df)\n",
    "\n",
    "    # # 输出结果\n",
    "    # print(\"每个类别在过滤后的 MEG 数据中样本数量:\")\n",
    "    # for category, count in category_counts.items():\n",
    "    #     print(f\"类别 {category}: {count} 个样本\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mne\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "# 读取 meg 类别标签文件\n",
    "csv_file_path = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Metadata/Concept-specific/image_concept_index.csv\"\n",
    "concept_df = pd.read_csv(csv_file_path, header=None, names=['Category_Label'])\n",
    "# 检查行号是否与 event_id 一致\n",
    "# 如果每行对应的行号就是 event_id，则行号即是 event_id\n",
    "concept_df['Event_ID'] = concept_df.index + 1  # 添加 Event_ID 列\n",
    "\n",
    "\n",
    "def count_samples_per_category(event_ids, concept_df):\n",
    "    # 筛选出在过滤后的 event_ids 中的类别标签\n",
    "    filtered_concept_df = concept_df[concept_df['Event_ID'].isin(event_ids)]\n",
    "\n",
    "    # 统计每个类别在过滤后的数据中的样本数量\n",
    "    category_counts = filtered_concept_df['Category_Label'].value_counts().sort_index()\n",
    "\n",
    "    # 将统计结果转为字典并返回\n",
    "    return category_counts.to_dict()\n",
    "def load_event_ids_from_txt(filename):\n",
    "    \"\"\"\n",
    "    从 txt 文件中加载 event_ids，返回一个整数列表。\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        event_ids = [int(line.strip()) for line in f]\n",
    "    return event_ids\n",
    "\n",
    "train_img_event_ids = load_event_ids_from_txt(\"train_img_event_ids.txt\")\n",
    "test_img_event_ids = load_event_ids_from_txt(\"test_img_event_ids.txt\")\n",
    "# 调用函数进行统计\n",
    "category_counts = count_samples_per_category(test_img_event_ids, concept_df)\n",
    "\n",
    "# 输出结果\n",
    "print(\"每个类别在过滤后的 MEG 数据中样本数量:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"类别 {category}: {count} 个样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用函数进行统计\n",
    "category_counts = count_samples_per_category(train_img_event_ids, concept_df)\n",
    "\n",
    "# 输出结果\n",
    "print(\"每个类别在过滤后的 MEG 数据中样本数量:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"类别 {category}: {count} 个样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 将 category_prefix_mapping 字典转换为 DataFrame，便于查看\n",
    "category_prefix_mapping_df = pd.DataFrame(list(category_prefix_mapping.items()), columns=['Category', 'Category_Prefix'])\n",
    "\n",
    "# 保存为 CSV 文件\n",
    "category_mapping_file_path = \"category_prefix_mapping.csv\"\n",
    "category_prefix_mapping_df.to_csv(category_mapping_file_path, index=False)\n",
    "\n",
    "print(f\"文件已保存到: {category_mapping_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_data({'meg_data': train_array, 'ch_names': ch_names, 'times': times},\n",
    "#             os.path.join(subject_output_dir, \"preprocessed_meg_training.pkl\"))\n",
    "# save_data({'meg_data': test_array, 'ch_names': ch_names, 'times': times},\n",
    "#             os.path.join(subject_output_dir, \"preprocessed_meg_test.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存过滤后的训练集和测试集 event_ids 到 txt 文件\n",
    "def save_event_ids_to_txt(event_ids, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for event_id in event_ids:\n",
    "            f.write(f\"{event_id}\\n\")\n",
    "\n",
    "# 将 filtered_train_event_ids 和 filtered_test_event_ids 保存到文件\n",
    "save_event_ids_to_txt(filtered_train_event_ids, \"filtered_train_event_ids.txt\")\n",
    "save_event_ids_to_txt(filtered_test_event_ids, \"filtered_test_event_ids.txt\")\n",
    "\n",
    "print(\"过滤后的训练集的 event_ids 已保存到 filtered_train_event_ids.txt\")\n",
    "print(\"过滤后的测试集的 event_ids 已保存到 filtered_test_event_ids.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# CSV 文件路径\n",
    "csv_img_file_path = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Metadata/Image-specific/image_paths.csv\"\n",
    "filtered_image_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/images_set_filter\"\n",
    "\n",
    "def load_img_event_id_mapping(csv_img_file_path):\n",
    "    \"\"\"\n",
    "    从 CSV 文件中加载图像名称到 img_event_id 的映射。\n",
    "    返回字典：{image_name: img_event_id}\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_img_file_path, header=None)\n",
    "    img_event_id_mapping = {os.path.basename(row[0]): idx + 1 for idx, row in df.iterrows()}  # 提取文件名作为键\n",
    "    return img_event_id_mapping\n",
    "\n",
    "def get_img_event_ids(image_dir, img_event_id_mapping):\n",
    "    \"\"\"\n",
    "    根据图像目录中的文件名查找 img_event_id。\n",
    "    返回 img_event_ids 列表。\n",
    "    \"\"\"\n",
    "    img_event_ids = []\n",
    "    for category_dir in sorted(os.listdir(image_dir)):  # 遍历类别目录\n",
    "        category_path = os.path.join(image_dir, category_dir)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "        for img_file in sorted(os.listdir(category_path)):  # 遍历图像文件\n",
    "            img_event_id = img_event_id_mapping.get(img_file)\n",
    "            if img_event_id is not None:\n",
    "                img_event_ids.append(img_event_id)\n",
    "    return img_event_ids\n",
    "\n",
    "# 加载 img_event_id 映射\n",
    "img_event_id_mapping = load_img_event_id_mapping(csv_img_file_path)\n",
    "\n",
    "# 获取训练集和测试集的 img_event_ids\n",
    "train_img_event_ids = get_img_event_ids(os.path.join(filtered_image_dir, \"training_images\"), img_event_id_mapping)\n",
    "test_img_event_ids = get_img_event_ids(os.path.join(filtered_image_dir, \"test_images\"), img_event_id_mapping)\n",
    "\n",
    "# 输出结果\n",
    "print(\"训练集的 img_event_ids:\", train_img_event_ids)\n",
    "print(\"测试集的 img_event_ids:\", test_img_event_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 img_event_ids 到 txt 文件\n",
    "def save_img_event_ids_to_txt(event_ids, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for event_id in event_ids:\n",
    "            f.write(f\"{event_id}\\n\")\n",
    "\n",
    "# 保存训练集和测试集的 img_event_ids 到文件\n",
    "save_img_event_ids_to_txt(train_img_event_ids, \"train_img_event_ids.txt\")\n",
    "save_img_event_ids_to_txt(test_img_event_ids, \"test_img_event_ids.txt\")\n",
    "\n",
    "print(\"训练集的 img_event_ids 已保存到 train_img_event_ids.txt\")\n",
    "print(\"测试集的 img_event_ids 已保存到 test_img_event_ids.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_event_ids_from_txt(filename):\n",
    "    \"\"\"\n",
    "    从 txt 文件中加载 event_ids，返回一个整数列表。\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        event_ids = [int(line.strip()) for line in f]\n",
    "    return event_ids\n",
    "\n",
    "def compare_event_ids(file1, file2):\n",
    "    \"\"\"\n",
    "    对比两个文件中的 event_ids，返回差异。\n",
    "    \"\"\"\n",
    "    event_ids_1 = set(load_event_ids_from_txt(file1))\n",
    "    event_ids_2 = set(load_event_ids_from_txt(file2))\n",
    "    \n",
    "    # 找出只在 file1 中存在的 event_ids\n",
    "    only_in_file1 = event_ids_1 - event_ids_2\n",
    "    # 找出只在 file2 中存在的 event_ids\n",
    "    only_in_file2 = event_ids_2 - event_ids_1\n",
    "    \n",
    "    return only_in_file1, only_in_file2\n",
    "\n",
    "# 文件路径\n",
    "filtered_test_event_ids_path = \"filtered_test_event_ids.txt\"\n",
    "test_img_event_ids_path = \"test_img_event_ids.txt\"\n",
    "filtered_train_event_ids_path = \"filtered_train_event_ids.txt\"\n",
    "train_img_event_ids_path = \"train_img_event_ids.txt\"\n",
    "\n",
    "# 对比测试集的差异\n",
    "test_only_in_filtered, test_only_in_img = compare_event_ids(filtered_test_event_ids_path, test_img_event_ids_path)\n",
    "print(\"只在 filtered_test_event_ids.txt 中的 event_ids:\", test_only_in_filtered)\n",
    "print(\"只在 test_img_event_ids.txt 中的 event_ids:\", test_only_in_img)\n",
    "\n",
    "# 对比训练集的差异\n",
    "train_only_in_filtered, train_only_in_img = compare_event_ids(filtered_train_event_ids_path, train_img_event_ids_path)\n",
    "print(\"只在 filtered_train_event_ids.txt 中的 event_ids:\", train_only_in_filtered)\n",
    "print(\"只在 train_img_event_ids.txt 中的 event_ids:\", train_only_in_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mne\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "base_fif_dir = \"/mnt/dataset0/ldy/datasets/meg_dataset/original_preprocessed/preprocessed\"\n",
    "\n",
    "def load_and_crop_epochs(fif_file):\n",
    "    epochs = mne.read_epochs(fif_file, preload=True)\n",
    "    epochs.crop(tmin=0, tmax=1.0)\n",
    "    data_array = epochs.get_data()\n",
    "    event_ids = epochs.events[:, 2]\n",
    "    \n",
    "    # 获取非 999999 的索引\n",
    "    valid_indices = np.where(event_ids != 999999)[0]\n",
    "    data_array = data_array[valid_indices]\n",
    "    event_ids = event_ids[valid_indices]\n",
    "    \n",
    "    # 获取唯一的 event_ids 的索引\n",
    "    unique_indices = np.unique(event_ids, return_index=True)[1]\n",
    "    data_array = data_array[unique_indices]\n",
    "    event_ids = event_ids[unique_indices]\n",
    "    \n",
    "    return data_array, event_ids, epochs.ch_names, epochs.times\n",
    "\n",
    "\n",
    "subjects = [\n",
    "    ('sub-02', 'preprocessed_P2-epo.fif'),\n",
    "]\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_event_mapping_train\n",
    "\n",
    "# 将字典转换为 DataFrame，便于查看\n",
    "image_event_mapping_train_df = pd.DataFrame.from_dict(image_event_mapping_train, orient='index', columns=['Category_Prefix', 'Image'])\n",
    "\n",
    "# 保存为 CSV 文件\n",
    "train_file_path = \"image_event_mapping_train.csv\"\n",
    "image_event_mapping_train_df.to_csv(train_file_path, index_label=\"Event_ID\")\n",
    "\n",
    "print(f\"文件已保存到: {train_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# image_event_mapping_test\n",
    "# 将字典转换为 DataFrame，便于查看\n",
    "image_event_mapping_test_df = pd.DataFrame.from_dict(image_event_mapping_test, orient='index', columns=['Category_Prefix', 'Image'])\n",
    "\n",
    "# 保存为 CSV 文件\n",
    "file_path = \"image_event_mapping_test.csv\"\n",
    "image_event_mapping_test_df.to_csv(file_path, index_label=\"Event_ID\")\n",
    "\n",
    "print(f\"文件已保存到: {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_image_event_mapping\n",
    "\n",
    "# full_image_event_mapping\n",
    "# 将字典转换为 DataFrame，便于查看\n",
    "full_image_event_mapping_df = pd.DataFrame.from_dict(full_image_event_mapping, orient='index', columns=['Category_Prefix', 'Image'])\n",
    "\n",
    "# 保存为 CSV 文件\n",
    "file_path = \"full_image_event_mapping.csv\"\n",
    "full_image_event_mapping_df.to_csv(file_path, index_label=\"Event_ID\")\n",
    "\n",
    "print(f\"文件已保存到: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject_id, fif_filename in subjects:\n",
    "    fif_file = os.path.join(base_fif_dir, fif_filename)\n",
    "    data_array, event_ids, ch_names, times = load_and_crop_epochs(fif_file)\n",
    "    \n",
    "    train_array = []\n",
    "    test_array = []\n",
    "    \n",
    "    for i, event_id in enumerate(event_ids):\n",
    "        if event_id in image_event_mapping_train:\n",
    "            train_array.append(data_array[i])\n",
    "        elif event_id in image_event_mapping_test:\n",
    "            test_array.append(data_array[i])\n",
    "\n",
    "    train_array = np.array(train_array)\n",
    "    test_array = np.array(test_array)\n",
    "    \n",
    "    subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "    os.makedirs(subject_output_dir, exist_ok=True)\n",
    "    \n",
    "    unmatched_train_keys = find_unmatched_keys(event_ids, image_event_mapping_train)\n",
    "    unmatched_test_keys = find_unmatched_keys(event_ids, image_event_mapping_test)\n",
    "    \n",
    "    filled_train_data, filled_train_ids, train_insert_indices = insert_unmatched_events(\n",
    "        unmatched_train_keys, image_event_mapping_train, data_array, event_ids)\n",
    "    filled_test_data, filled_test_ids, test_insert_indices = insert_unmatched_events(\n",
    "        unmatched_test_keys, image_event_mapping_test, data_array, event_ids)\n",
    "    \n",
    "    # Sort indices in descending order to avoid index shift during insertion\n",
    "    train_insert_indices = sorted(train_insert_indices, reverse=True)\n",
    "    test_insert_indices = sorted(test_insert_indices, reverse=True)\n",
    "\n",
    "    # Insert filled data from the end of the array, ensuring indices stay consistent\n",
    "    if filled_train_data.size > 0:\n",
    "        for data, idx in zip(filled_train_data, train_insert_indices):\n",
    "            # print(\"idx\", idx)\n",
    "            train_array = np.insert(train_array, idx, data, axis=0)\n",
    "\n",
    "    if filled_test_data.size > 0:\n",
    "        for data, idx in zip(filled_test_data, test_insert_indices):\n",
    "            print(\"idx\", idx)\n",
    "            test_array = np.insert(test_array, idx, data, axis=0)\n",
    "\n",
    "    # save_data({'meg_data': train_array, 'ch_names': ch_names, 'times': times},\n",
    "    #           os.path.join(subject_output_dir, \"preprocessed_meg_training.pkl\"))\n",
    "    # save_data({'meg_data': test_array, 'ch_names': ch_names, 'times': times},\n",
    "    #           os.path.join(subject_output_dir, \"preprocessed_meg_test.pkl\"))\n",
    "\n",
    "    print(f\"\\nUnmatched keys in training set for {subject_id}:\")\n",
    "    print(unmatched_train_keys)\n",
    "    \n",
    "    print(f\"\\nUnmatched keys in test set for {subject_id}:\")\n",
    "    print(unmatched_test_keys)\n",
    "\n",
    "print(\"处理完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mne\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# 目录设置\n",
    "base_fif_dir = \"/mnt/dataset0/ldy/datasets/meg_dataset/original_preprocessed/preprocessed\"\n",
    "source_image_dir = \"/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Images/images\"\n",
    "meg_image_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/images_set\"\n",
    "output_dir = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/Preprocessed\"\n",
    "subjects = [\n",
    "    ('sub-01', 'preprocessed_P1-epo.fif'),\n",
    "    ('sub-02', 'preprocessed_P2-epo.fif'),\n",
    "    ('sub-03', 'preprocessed_P3-epo.fif'),\n",
    "    ('sub-04', 'preprocessed_P4-epo.fif'),\n",
    "]\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_and_crop_epochs(fif_file):\n",
    "    # 读取并裁剪MEG数据\n",
    "    epochs = mne.read_epochs(fif_file, preload=True)\n",
    "    epochs.crop(tmin=0, tmax=1.0)\n",
    "    data_array = epochs.get_data()\n",
    "    event_ids = epochs.events[:, 2]\n",
    "    unique_indices = np.unique(event_ids[event_ids != 999999], return_index=True)[1]\n",
    "    data_array = data_array[unique_indices]\n",
    "    event_ids = event_ids[event_ids != 999999][unique_indices]\n",
    "    return data_array, event_ids, epochs.ch_names, epochs.times\n",
    "\n",
    "def build_category_prefix_mapping(meg_image_dir):\n",
    "    # 从 training_images 和 test_images 目录中提取带编号的前缀和类别名映射\n",
    "    category_prefix_mapping = {}\n",
    "    \n",
    "    for split in [\"training_images\", \"test_images\"]:\n",
    "        split_dir = os.path.join(meg_image_dir, split)\n",
    "        if os.path.isdir(split_dir):\n",
    "            for category_with_prefix in os.listdir(split_dir):\n",
    "                prefix, category = category_with_prefix.split(\"_\", 1)\n",
    "                category_prefix_mapping[category] = category_with_prefix  # 建立类别到带前缀名称的映射\n",
    "                \n",
    "    return category_prefix_mapping\n",
    "\n",
    "def get_full_image_event_mapping(base_dir, category_prefix_mapping):\n",
    "    # 获取完整的图像事件的映射，确保事件id从0到22447连续\n",
    "    event_id = 1\n",
    "    full_image_event_mapping = {}\n",
    "    categories = sorted(os.listdir(base_dir))\n",
    "    \n",
    "    for category in categories:\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        \n",
    "        # 确认该类别在带编号前缀的映射中\n",
    "        if os.path.isdir(category_path) and category in category_prefix_mapping:\n",
    "            category_with_prefix = category_prefix_mapping[category]\n",
    "            images = sorted(os.listdir(category_path))\n",
    "            for img in images:\n",
    "                full_image_event_mapping[event_id] = (category_with_prefix, img)\n",
    "                event_id += 1\n",
    "                \n",
    "    return full_image_event_mapping\n",
    "\n",
    "category_prefix_mapping = build_category_prefix_mapping(meg_image_dir)\n",
    "full_image_event_mapping = get_full_image_event_mapping(source_image_dir, category_prefix_mapping)\n",
    "\n",
    "train_images_expected = set()\n",
    "test_images_expected = set()\n",
    "\n",
    "for category_with_prefix in category_prefix_mapping.values():\n",
    "    train_dir = os.path.join(meg_image_dir, \"training_images\", category_with_prefix)\n",
    "    test_dir = os.path.join(meg_image_dir, \"test_images\", category_with_prefix)\n",
    "\n",
    "    if os.path.exists(train_dir):\n",
    "        train_images_expected.update(\n",
    "            [f\"{category_with_prefix}/{img}\" for img in sorted(os.listdir(train_dir))]\n",
    "        )\n",
    "    if os.path.exists(test_dir):\n",
    "        test_images_expected.update(\n",
    "            [f\"{category_with_prefix}/{img}\" for img in sorted(os.listdir(test_dir))]\n",
    "        )\n",
    "\n",
    "image_event_mapping_train = {}\n",
    "image_event_mapping_test = {}\n",
    "\n",
    "for event_id, (category_with_prefix, img) in full_image_event_mapping.items():\n",
    "    train_path = os.path.join(meg_image_dir, \"training_images\", category_with_prefix, img)\n",
    "    test_path = os.path.join(meg_image_dir, \"test_images\", category_with_prefix, img)\n",
    "    \n",
    "    if os.path.exists(train_path):\n",
    "        image_event_mapping_train[event_id] = (category_with_prefix, img)\n",
    "    elif os.path.exists(test_path):\n",
    "        image_event_mapping_test[event_id] = (category_with_prefix, img)\n",
    "\n",
    "# 统计哪些键不在加载的 event_ids 中\n",
    "def find_unmatched_keys(event_ids, image_event_mapping):\n",
    "    return [key for key in image_event_mapping.keys() if key not in event_ids]\n",
    "\n",
    "# 加载和保存每个受试者的MEG数据\n",
    "for subject_id, fif_filename in subjects:\n",
    "    fif_file = os.path.join(base_fif_dir, fif_filename)\n",
    "    data_array, event_ids, ch_names, times = load_and_crop_epochs(fif_file)\n",
    "    \n",
    "    train_array = []\n",
    "    test_array = []\n",
    "    \n",
    "    for i, event_id in enumerate(event_ids):\n",
    "        if event_id in image_event_mapping_train:\n",
    "            train_array.append(data_array[i])\n",
    "        elif event_id in image_event_mapping_test:\n",
    "            test_array.append(data_array[i])\n",
    "\n",
    "    train_array = np.array(train_array)\n",
    "    test_array = np.array(test_array)\n",
    "    \n",
    "    subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "    os.makedirs(subject_output_dir, exist_ok=True)\n",
    "    \n",
    "    save_data({'meg_data': train_array, 'ch_names': ch_names, 'times': times},\n",
    "              os.path.join(subject_output_dir, \"preprocessed_meg_training.pkl\"))\n",
    "    save_data({'meg_data': test_array, 'ch_names': ch_names, 'times': times},\n",
    "              os.path.join(subject_output_dir, \"preprocessed_meg_test.pkl\"))\n",
    "\n",
    "    # 输出未匹配的 event_id\n",
    "    unmatched_train_keys = find_unmatched_keys(event_ids, image_event_mapping_train)\n",
    "    unmatched_test_keys = find_unmatched_keys(event_ids, image_event_mapping_test)\n",
    "\n",
    "    print(f\"\\nUnmatched keys in training set for {subject_id}:\")\n",
    "    print(unmatched_train_keys)\n",
    "    \n",
    "    print(f\"\\nUnmatched keys in test set for {subject_id}:\")\n",
    "    print(unmatched_test_keys)\n",
    "\n",
    "    # 删除 unmatched_train_keys 和 unmatched_test_keys 对应的原图\n",
    "    for unmatched_keys, mapping, split_name in [\n",
    "        (unmatched_train_keys, image_event_mapping_train, \"training_images\"),\n",
    "        (unmatched_test_keys, image_event_mapping_test, \"test_images\"),\n",
    "    ]:\n",
    "        for event_id in unmatched_keys:\n",
    "            if event_id in mapping:\n",
    "                category, img_name = mapping[event_id]\n",
    "                img_path = os.path.join(meg_image_dir, split_name, category, img_name)\n",
    "                if os.path.exists(img_path):\n",
    "                    os.remove(img_path)\n",
    "                    print(f\"Deleted image: {img_path}\")\n",
    "                else:\n",
    "                    print(f\"Image not found for deletion: {img_path}\")\n",
    "\n",
    "print(\"处理完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shikra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
