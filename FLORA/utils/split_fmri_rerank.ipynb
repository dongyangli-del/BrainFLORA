{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THINGS-fMRI usage notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a detailed description of the data and the procedures that generated it, see [the THINGS-data preprint](https://doi.org/10.1101/2022.07.22.501123)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T08:56:42.038495Z",
     "start_time": "2024-11-01T08:56:40.679969Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nilearn.masking import unmask\n",
    "from nilearn.plotting import plot_stat_map\n",
    "from nilearn.image import load_img, index_img\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes you've downloaded the THINGS-fMRI data to this directory\n",
    "# basedir = '/Users/olivercontier/bigfri/openneuro/THINGS-data/THINGS-fMRI/derivatives'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single trial responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single trial responses are arguably the easiest way to analyze the THINGS-fMRI data. They contains the magnitude of the fMRI response to each stimulus in each voxel with a single number. The single trial responses are provided in two formats: a) In table format, b) in volumetric format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the fMRI response data, the table format contains metadata about each voxel (such as noise ceilings, pRF parameters, regions of interest) and about the stimulus (such as image file name, trial type, run and session). You can download this data [here](https://doi.org/10.25452/figshare.plus.20492835.v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T08:57:43.271486Z",
     "start_time": "2024-11-01T08:57:43.268956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assumes you downloaded the single trial responses in table format to this directory \n",
    "betas_csv_dir = pjoin(r'/mnt/dataset0/ldy/datasets/THINGS_fMRI/THINGS_fMRI_Single_Trial_table', 'betas_csv')\n",
    "\n",
    "# and that you're interested in the data for the first subject\n",
    "sub = '03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'03'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sub-{subject}_ResponseData.h5` files contain the actual single trial responses. Rows are voxels, columns are trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T09:00:32.803283Z",
     "start_time": "2024-11-01T08:58:38.782292Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3995974/146607979.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  responses = pd.read_hdf(data_file)  # this may take a minute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single trial response data\n"
     ]
    }
   ],
   "source": [
    "data_file = pjoin(betas_csv_dir, f'sub-{sub}_ResponseData.h5')\n",
    "responses = pd.read_hdf(data_file)  # this may take a minute\n",
    "print('Single trial response data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   voxel_id         0         1         2         3         4         5  \\\n",
      "0         0  0.013239  0.002022 -0.105115 -0.028644  0.010820  0.045174   \n",
      "1         1 -0.125393 -0.003670  0.091215 -0.040909  0.069582  0.069192   \n",
      "2         2  0.024484  0.065484 -0.042041 -0.014450 -0.002779  0.079440   \n",
      "3         3 -0.085995  0.081165  0.064865 -0.023732  0.000979  0.053820   \n",
      "4         4  0.045378  0.030528  0.052574  0.077042 -0.031905  0.029656   \n",
      "\n",
      "          6         7         8  ...      9830      9831      9832      9833  \\\n",
      "0  0.021671  0.067446 -0.012943  ... -0.034353 -0.120423  0.064878 -0.089535   \n",
      "1 -0.002254  0.052782  0.002731  ...  0.004860 -0.006326 -0.012317 -0.081720   \n",
      "2 -0.003625  0.052408 -0.003191  ...  0.002041 -0.013647 -0.021190  0.000605   \n",
      "3 -0.040727 -0.012333  0.045912  ...  0.041776  0.115980 -0.023257 -0.011929   \n",
      "4 -0.072517 -0.015580  0.032488  ...  0.045142  0.064435  0.123784  0.001207   \n",
      "\n",
      "       9834      9835      9836      9837      9838      9839  \n",
      "0  0.025040 -0.077540  0.075740  0.102647 -0.023470 -0.020143  \n",
      "1 -0.009490 -0.011595  0.130456  0.114616 -0.010227  0.057384  \n",
      "2  0.057942  0.064813  0.041211  0.075359 -0.003794 -0.046652  \n",
      "3 -0.060907  0.006370 -0.034301  0.038396  0.044679  0.025974  \n",
      "4 -0.027798  0.006506 -0.069370  0.024731  0.011598  0.094571  \n",
      "\n",
      "[5 rows x 9841 columns]\n"
     ]
    }
   ],
   "source": [
    "print(responses.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189164, 9841)\n"
     ]
    }
   ],
   "source": [
    "print(responses.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sub-{subject}_VoxelMetadata.csv` files contain additional information about each voxel, such as membership to ROIs, reliability measures, and noise ceilings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T09:01:31.462600Z",
     "start_time": "2024-11-01T09:01:28.819342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>voxel_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>voxel_x</th>\n",
       "      <th>voxel_y</th>\n",
       "      <th>voxel_z</th>\n",
       "      <th>nc_singletrial</th>\n",
       "      <th>nc_testset</th>\n",
       "      <th>splithalf_uncorrected</th>\n",
       "      <th>splithalf_corrected</th>\n",
       "      <th>prf-eccentricity</th>\n",
       "      <th>...</th>\n",
       "      <th>glasser-p47r</th>\n",
       "      <th>glasser-TGv</th>\n",
       "      <th>glasser-MBelt</th>\n",
       "      <th>glasser-LBelt</th>\n",
       "      <th>glasser-A4</th>\n",
       "      <th>glasser-STSva</th>\n",
       "      <th>glasser-TE1m</th>\n",
       "      <th>glasser-PI</th>\n",
       "      <th>glasser-a32pr</th>\n",
       "      <th>glasser-p24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000847</td>\n",
       "      <td>-0.001695</td>\n",
       "      <td>10.378512</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>35</td>\n",
       "      <td>1.551098</td>\n",
       "      <td>15.900257</td>\n",
       "      <td>0.080501</td>\n",
       "      <td>0.149006</td>\n",
       "      <td>11.895808</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.119311</td>\n",
       "      <td>-0.270950</td>\n",
       "      <td>8.952419</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>0.033844</td>\n",
       "      <td>0.404616</td>\n",
       "      <td>0.004827</td>\n",
       "      <td>0.009608</td>\n",
       "      <td>10.443779</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>34</td>\n",
       "      <td>2.784395</td>\n",
       "      <td>25.578476</td>\n",
       "      <td>0.146826</td>\n",
       "      <td>0.256057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 220 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   voxel_id  subject_id  voxel_x  voxel_y  voxel_z  nc_singletrial  \\\n",
       "0         0           3        0       33       34        0.000000   \n",
       "1         1           3        0       33       35        1.551098   \n",
       "2         2           3        0       34       34        0.000000   \n",
       "3         3           3        0       34       35        0.033844   \n",
       "4         4           3        0       35       34        2.784395   \n",
       "\n",
       "   nc_testset  splithalf_uncorrected  splithalf_corrected  prf-eccentricity  \\\n",
       "0    0.000000              -0.000847            -0.001695         10.378512   \n",
       "1   15.900257               0.080501             0.149006         11.895808   \n",
       "2    0.000000              -0.119311            -0.270950          8.952419   \n",
       "3    0.404616               0.004827             0.009608         10.443779   \n",
       "4   25.578476               0.146826             0.256057          0.000000   \n",
       "\n",
       "   ...  glasser-p47r  glasser-TGv  glasser-MBelt  glasser-LBelt  glasser-A4  \\\n",
       "0  ...             0            0              0              0           0   \n",
       "1  ...             0            0              0              0           0   \n",
       "2  ...             0            0              0              0           0   \n",
       "3  ...             0            0              0              0           0   \n",
       "4  ...             0            0              0              0           0   \n",
       "\n",
       "   glasser-STSva  glasser-TE1m  glasser-PI  glasser-a32pr  glasser-p24  \n",
       "0              0             0           0              0            0  \n",
       "1              0             0           0              0            0  \n",
       "2              0             0           0              0            0  \n",
       "3              0             0           0              0            0  \n",
       "4              0             0           0              0            0  \n",
       "\n",
       "[5 rows x 220 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vox_f = pjoin(betas_csv_dir, f'sub-{sub}_VoxelMetadata.csv')\n",
    "voxdata = pd.read_csv(vox_f)\n",
    "voxdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T09:01:34.092522Z",
     "start_time": "2024-11-01T09:01:34.089019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available voxel metadata:\n",
      " ['voxel_id', 'subject_id', 'voxel_x', 'voxel_y', 'voxel_z', 'nc_singletrial', 'nc_testset', 'splithalf_uncorrected', 'splithalf_corrected', 'prf-eccentricity', 'prf-polarangle', 'prf-rsquared', 'prf-size', 'V1', 'V2', 'V3', 'hV4', 'VO1', 'VO2', 'LO1 (prf)', 'LO2 (prf)', 'TO1', 'TO2', 'V3b', 'V3a', 'lEBA', 'rEBA', 'lFFA', 'rFFA', 'lOFA', 'rOFA', 'lPPA', 'rPPA', 'lRSC', 'rRSC', 'lTOS', 'rTOS', 'lLOC', 'rLOC', 'IT', 'glasser-V1', 'glasser-MST', 'glasser-V6', 'glasser-V2', 'glasser-V3', 'glasser-V4', 'glasser-V8', 'glasser-4', 'glasser-3b', 'glasser-FEF', 'glasser-PEF', 'glasser-55b', 'glasser-V3A', 'glasser-RSC', 'glasser-POS2', 'glasser-V7', 'glasser-IPS1', 'glasser-FFC', 'glasser-V3B', 'glasser-LO1', 'glasser-LO2', 'glasser-PIT', 'glasser-MT', 'glasser-A1', 'glasser-PSL', 'glasser-SFL', 'glasser-PCV', 'glasser-STV', 'glasser-7Pm', 'glasser-7m', 'glasser-POS1', 'glasser-23d', 'glasser-v23ab', 'glasser-d23ab', 'glasser-31pv', 'glasser-5m', 'glasser-5mv', 'glasser-23c', 'glasser-5L', 'glasser-24dd', 'glasser-24dv', 'glasser-7AL', 'glasser-SCEF', 'glasser-6ma', 'glasser-7Am', 'glasser-7Pl', 'glasser-7PC', 'glasser-LIPv', 'glasser-VIP', 'glasser-MIP', 'glasser-1', 'glasser-2', 'glasser-3a', 'glasser-6d', 'glasser-6mp', 'glasser-6v', 'glasser-p24pr', 'glasser-33pr', 'glasser-a24pr', 'glasser-p32pr', 'glasser-a24', 'glasser-d32', 'glasser-8BM', 'glasser-p32', 'glasser-10r', 'glasser-47m', 'glasser-8Av', 'glasser-8Ad', 'glasser-9m', 'glasser-8BL', 'glasser-9p', 'glasser-10d', 'glasser-8C', 'glasser-44', 'glasser-45', 'glasser-47l', 'glasser-a47r', 'glasser-6r', 'glasser-IFJa', 'glasser-IFJp', 'glasser-IFSp', 'glasser-IFSa', 'glasser-p9-\\n46v', 'glasser-46', 'glasser-a9-\\n46v', 'glasser-9-46d', 'glasser-9a', 'glasser-10v', 'glasser-a10p', 'glasser-10pp', 'glasser-11l', 'glasser-13l', 'glasser-OFC', 'glasser-47s', 'glasser-LIPd', 'glasser-6a', 'glasser-i6-8', 'glasser-s6-8', 'glasser-43', 'glasser-OP4', 'glasser-OP1', 'glasser-OP2-3', 'glasser-52', 'glasser-RI', 'glasser-PFcm', 'glasser-PoI2', 'glasser-TA2', 'glasser-FOP4', 'glasser-MI', 'glasser-Pir', 'glasser-AVI', 'glasser-AAIC', 'glasser-FOP1', 'glasser-FOP3', 'glasser-FOP2', 'glasser-PFt', 'glasser-AIP', 'glasser-EC', 'glasser-PreS', 'glasser-H', 'glasser-ProS', 'glasser-PeEc', 'glasser-STGa', 'glasser-PBelt', 'glasser-A5', 'glasser-PHA1', 'glasser-PHA3', 'glasser-STSda', 'glasser-STSdp', 'glasser-STSvp', 'glasser-TGd', 'glasser-TE1a', 'glasser-TE1p', 'glasser-TE2a', 'glasser-TF', 'glasser-TE2p', 'glasser-PHT', 'glasser-PH', 'glasser-TPOJ1', 'glasser-TPOJ2', 'glasser-TPOJ3', 'glasser-DVT', 'glasser-PGp', 'glasser-IP2', 'glasser-IP1', 'glasser-IP0', 'glasser-PFop', 'glasser-PF', 'glasser-PFm', 'glasser-PGi', 'glasser-PGs', 'glasser-V6A', 'glasser-VMV1', 'glasser-VMV3', 'glasser-PHA2', 'glasser-V4t', 'glasser-FST', 'glasser-V3CD', 'glasser-LO3', 'glasser-VMV2', 'glasser-31pd', 'glasser-31a', 'glasser-VVC', 'glasser-25', 'glasser-s32', 'glasser-pOFC', 'glasser-PoI1', 'glasser-Ig', 'glasser-FOP5', 'glasser-p10p', 'glasser-p47r', 'glasser-TGv', 'glasser-MBelt', 'glasser-LBelt', 'glasser-A4', 'glasser-STSva', 'glasser-TE1m', 'glasser-PI', 'glasser-a32pr', 'glasser-p24']\n"
     ]
    }
   ],
   "source": [
    "print('available voxel metadata:\\n', voxdata.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The voxel indices can be used to reconstruct a volume, e.g. for visualizing results obtained from the single trial responses. Alternatively, the brain mask can be used for that purpose (see below). Membership of each voxel to the available ROIs is dummy coded, e.g. in `voxdata[\"V1\"]` or `voxdata[\"rFFA\"]`. The population receptive field parameters are encoded in the following columns: `prf-eccentricity`, `prf-polarangle`, `prf-size`, and `prf-rsquared`. Finally, different reliability estimates are available in the columns: `nc_testset`, `nc_singletrial`, `splithalf_uncorrected`, and `splithalf_corrected`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T09:08:01.822823Z",
     "start_time": "2024-11-01T09:07:42.408532Z"
    }
   },
   "outputs": [],
   "source": [
    "# vox_pick = voxdata['V1']\n",
    "# responses_np = responses.to_numpy()\n",
    "# responses_pick = responses_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T09:14:15.327630Z",
     "start_time": "2024-11-01T09:14:15.319123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of picked true voxels: 5238\n",
      "0         False\n",
      "1         False\n",
      "2         False\n",
      "3         False\n",
      "4         False\n",
      "          ...  \n",
      "189159    False\n",
      "189160    False\n",
      "189161    False\n",
      "189162    False\n",
      "189163    False\n",
      "Length: 189164, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# V1, V2, V3, hV4, OFA, FFA, EBA, PPA, MPA and OPA\n",
    "roi_columns = ['V1', 'V2', 'V3', 'hV4', 'rOFA', 'lOFA', 'rFFA', 'lFFA', 'rEBA', 'lEBA', 'rPPA', 'lPPA']\n",
    "vox_pick = voxdata[roi_columns].any(axis=1)\n",
    "\n",
    "# sum of picked true voxels\n",
    "print('sum of picked true voxels:', vox_pick.sum())\n",
    "print(vox_pick)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9840, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stimulus metadata\n",
    "stim_f = pjoin(betas_csv_dir, f'sub-{sub}_StimulusMetadata.csv')\n",
    "stimdata = pd.read_csv(stim_f)\n",
    "stimdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aardvark', 'abacus', 'accordion', 'acorn', 'air_conditioner', 'air_mattress', 'air_pump', 'airbag', 'airboat', 'airplane', 'album', 'alligator', 'almond', 'aloe', 'alpaca', 'altar', 'aluminum_foil', 'amber', 'ambulance', 'amplifier', 'anchor', 'ankle', 'anklet', 'ant', 'anteater', 'antenna', 'anvil', 'appetizer', 'apple', 'apple_tree', 'applesauce', 'apron', 'aquarium', 'arch', 'arm', 'armor', 'arrow', 'artichoke', 'arugula', 'ashtray', 'asparagus', 'avocado', 'awning', 'axe', 'baby', 'backdrop', 'backgammon', 'backpack', 'bacon', 'badge', 'badger', 'bag', 'bagel', 'bagpipe', 'baklava', 'ball', 'balloon', 'ballot_box', 'bamboo', 'banana_peel', 'banana_split', 'bandage', 'bandanna', 'banjo', 'bank', 'banner', 'barbed_wire', 'barbell', 'barcode', 'bark', 'barnacle', 'barrel', 'barrette', 'baseball', 'baseball_glove', 'basket', 'basketball_hoop', 'bassinet', 'bat1', 'bat2', 'bathmat', 'bathrobe', 'bathtub', 'baton1', 'baton2', 'baton3', 'battery', 'bazooka', 'beachball', 'bead', 'beaker', 'bean', 'beanbag', 'beanie', 'bear', 'beard', 'bed', 'bedpan', 'bedpost', 'bee', 'beehive', 'beer', 'beet', 'beetle', 'bell', 'bell_pepper', 'belt', 'belt_buckle', 'berry', 'bib', 'bikini', 'bin', 'binder', 'binoculars', 'bird', 'birdbath', 'birdcage', 'birdhouse', 'biscuit', 'bison', 'blackberry', 'blanket', 'blazer', 'blender', 'blimp', 'blind', 'blinder', 'blindfold', 'block', 'blouse', 'blower', 'blowfish', 'blowgun', 'blueberry', 'boa', 'boar', 'board', 'board_game', 'bobsled', 'bolo_tie', 'bologna', 'bolt', 'bomb', 'bone', 'bongo', 'bonsai', 'book', 'bookmark', 'bookshelf', 'boomerang', 'boot', 'bottle', 'boulder', 'bouquet', 'bow1', 'bow2', 'bow3', 'bowl', 'bowler_hat', 'bowling_ball', 'bowtie', 'box', 'boxer_shorts', 'boxing_gloves', 'boy', 'bra', 'bracelet1', 'bracelet2', 'bracket', 'braid', 'brake', 'branch', 'brass_knuckles', 'breadstick', 'breakfast', 'breathalyzer', 'brick', 'briefcase', 'broccoli', 'brooch', 'broom', 'brownie', 'brush', 'brussels_sprouts', 'bubble', 'bubble_wrap', 'bucket', 'buckle', 'buffet', 'bull', 'bulldozer', 'bulletin_board', 'bulletproof_vest', 'bumper', 'bungee', 'bunkbed', 'buoy', 'burner', 'burrito', 'bus', 'butter', 'butterfly', 'button1', 'button2', 'cabbage', 'cabinet', 'cable', 'cactus', 'cage', 'cake', 'cake_mix', 'calculator', 'calf1', 'calf2', 'calzone', 'camcorder', 'camel', 'camera1', 'camera2', 'camera_lens', 'camper', 'can', 'can_opener', 'candelabra', 'candle', 'candy', 'candy_bar', 'candy_cane', 'cane', 'canister', 'cannon', 'cannonball', 'canoe', 'cantaloupe', 'canvas', 'cap', 'cape', 'car', 'car_door', 'car_seat', 'caramel', 'card', 'cardboard', 'cardigan', 'cardinal', 'carousel', 'carriage', 'carrot', 'cash_machine', 'cash_register', 'casserole', 'cassette', 'catapult', 'catfish', 'cauliflower', 'caviar', 'celery', 'cello', 'cellphone', 'cement_mixer', 'centerpiece', 'centrifuge', 'cereal', 'chainsaw', 'chair', 'chalice', 'chalk', 'chalkboard', 'champagne', 'chandelier', 'charcoal', 'charger', 'chariot', 'checkbook', 'checkers', 'cheeseburger', 'cheesecake', 'cherry', 'chess_piece', 'chessboard', 'chest1', 'chick', 'chicken1', 'chicken2', 'chicken_wire', 'chickpea', 'chihuahua', 'chili', 'chimney', 'chin', 'chinaware', 'chinchilla', 'chip', 'chipmunk', 'chips', 'chisel', 'chive', 'chocolate', 'christmas_card', 'christmas_tree', 'chute', 'cigar', 'cigarette', 'cigarette_butt', 'cigarette_holder', 'cilantro', 'cinnamon', 'clam', 'clarinet', 'clasp', 'clay', 'clipboard', 'clipper1', 'clipper2', 'cloak', 'clock', 'closet', 'clothes', 'clothesline', 'clothespin', 'cloud', 'clove', 'clover', 'coal', 'coaster', 'coat_rack', 'cockatoo', 'cockroach', 'cocktail', 'cocoon', 'coffee', 'coffee_filter', 'coffee_pot', 'coffee_table', 'coffin', 'coil', 'coin', 'coleslaw', 'collar', 'column', 'comb', 'combination_lock', 'comic_book', 'compass', 'compost', 'computer', 'computer_screen', 'confetti', 'contact_lens', 'container', 'cooker', 'cookie_cutter', 'cookie_sheet', 'cooler', 'coop', 'copier', 'coral', 'cord', 'cork', 'corkboard', 'corkscrew', 'corn', 'cornbread', 'cornhusk', 'cornmeal', 'cornucopia', 'corsage', 'corset', 'costume', 'cot', 'cotton_candy', 'couch', 'cougar', 'counter', 'cow', 'coyote', 'cracker', 'cranberry', 'crane', 'crank', 'crate', 'crayfish', 'crayon', 'cream', 'cream_cheese', 'credit_card', 'cross', 'crossbow', 'crouton', 'crowbar', 'crown', 'crucifix', 'crutch', 'crystal1', 'crystal2', 'crystal_ball', 'cuckoo_clock', 'cucumber', 'cufflink', 'cummerbund', 'cup', 'curb', 'curling_iron', 'curry', 'curtain', 'cushion', 'cutting_board', 'cymbal', 'daisy', 'dandelion', 'dart', 'dartboard', 'dashboard', 'deer', 'defibrillator', 'denture', 'deodorant', 'desk', 'detonator', 'dial', 'diamond', 'diaper', 'dice', 'dip', 'dirt_bike', 'dish', 'dishrag', 'dishwasher', 'dishwashing_liquid', 'diskette', 'diving_board', 'dog', 'dogfood', 'doghouse', 'doily', 'doll', 'dollhouse', 'dolly', 'dolphin', 'domino', 'donkey', 'donut', 'door', 'doorbell', 'doorhandle', 'doorknob', 'doorknocker', 'doormat', 'doorstop', 'dough', 'drain', 'drawer', 'dress', 'dresser', 'drill', 'drink', 'drumstick', 'dryer', 'duck', 'duckling', 'duct', 'duct_tape', 'dumbbell', 'dumbwaiter', 'dumpling', 'dumpster', 'duster', 'dustpan', 'dynamite', 'ear', 'earplug', 'earring', 'earwig', 'easel', 'easter_egg', 'eclair', 'egg_roll', 'eggbeater', 'eggnog', 'eggplant', 'eggshell', 'elbow', 'electric_chair', 'emerald', 'enchilada', 'engine', 'envelope', 'eraser', 'exerciser', 'exhaust_pipe', 'extinguisher', 'eye', 'eye_patch', 'eyedropper', 'eyeliner', 'eyepiece', 'face', 'fan', 'fast_food', 'faucet', 'feather', 'fence', 'fencepost', 'fern', 'ferret', 'ferris_wheel', 'fig', 'figurine', 'file1', 'file2', 'filing_cabinet', 'film', 'filter', 'finger', 'fingerprint', 'fire', 'fire_alarm', 'fire_hydrant', 'fire_pit', 'firecracker', 'fireplace', 'firetruck', 'firewood', 'fireworks', 'first-aid_kit', 'fish', 'fishbowl', 'fishhook', 'fishing_pole', 'fishnet_stockings', 'flag', 'flagpole', 'flamethrower', 'flan', 'flashbulb', 'flashlight', 'flask', 'flatiron', 'flip_flop', 'flipper', 'float', 'floss', 'flour', 'flower', 'flute', 'fly', 'flypaper', 'flyswatter', 'foam', 'fondue', 'food_processor', 'foot', 'football', 'football_helmet', 'footbath', 'footprint', 'footrest', 'forklift', 'fossil', 'fountain_pen', 'fox', 'frame', 'french_fries', 'frisbee', 'frog', 'fruitcake', 'fudge', 'fungus', 'funnel', 'fur_coat', 'furnace', 'fuse', 'gallows', 'game', 'garbage', 'garbage_truck', 'gargoyle', 'garter', 'gas_mask', 'gasket', 'gate', 'gauge', 'gauze', 'gavel', 'gazelle', 'gear', 'gearshift', 'gel', 'gem', 'generator', 'gift', 'ginger', 'gingerbread_man', 'giraffe', 'girl', 'glass', 'glasses', 'globe', 'glue', 'go-kart', 'goalpost', 'goat', 'goblet', 'goggles', 'gold', 'goldfish', 'golf_club', 'gong', 'gourd', 'graffiti', 'grain', 'gramophone', 'granite', 'granola', 'grape', 'grapefruit', 'grapevine', 'grass', 'grate', 'grater', 'gravel', 'gravestone', 'gravy', 'green_beans', 'grill', 'grille', 'grinder', 'grits', 'groundhog', 'guacamole', 'guardrail', 'guillotine', 'guinea_pig', 'guitar', 'gum', 'gumball', 'gumdrop', 'gun', 'gurney', 'gutter', 'gyro', 'gyroscope', 'hail', 'hair', 'hairbrush', 'hairdryer', 'hairnet', 'hairpin', 'hairspray', 'ham', 'hammock', 'hamster', 'hand', 'handcuff', 'handkerchief', 'handle', 'handlebar', 'handprint', 'hanger', 'hard_disk', 'harmonica', 'harness', 'harp', 'hash', 'hat', 'hatbox', 'hatchet', 'hawk', 'hay', 'headband', 'headdress', 'headlamp', 'headlight', 'headphones', 'headrest', 'headset', 'hearing_aid', 'hearse', 'heater', 'hedge', 'hedgehog', 'helicopter', 'helmet', 'highlighter', 'hinge', 'hip', 'hippopotamus', 'hobbyhorse', 'hockey_stick', 'hoe', 'hole', 'holster', 'home_plate', 'honey', 'honeycomb', 'honeypot', 'hood', 'hook1', 'hook2', 'hookah', 'hopscotch', 'horn', 'horse', 'horseshoe', 'hose', 'hot-air_balloon', 'hot-water_bottle', 'hot_chocolate', 'hot_tub', 'hotdog', 'hotplate', 'hourglass', 'hovercraft', 'hubcap', 'hula_hoop', 'hummus', 'humvee', 'hydrant', 'hyena', 'ice', 'ice-cream_cone', 'ice_cream', 'icemaker', 'icepick', 'iceskate', 'icicle', 'iguana', 'incense', 'incubator', 'inhaler', 'ink', 'inkwell', 'insole', 'iron', 'ironing_board', 'ivy', 'jack', 'jacket', 'jackhammer', 'jalapeno', 'jam', 'jar', 'javelin', 'jeans', 'jellyfish', 'jersey', 'jet', 'jetski', 'jewel', 'jewelry', 'jigsaw_puzzle', 'joystick', 'jug', 'juice', 'juicer1', 'juicer2', 'jump_rope', 'jumpsuit', 'kale', 'kaleidoscope', 'kangaroo', 'kayak', 'kazoo', 'kebab', 'keg', 'ketchup', 'key', 'keyboard', 'keyhole', 'kilt', 'kimono', 'kite', 'kitten', 'kiwi', 'knee', 'knife', 'knitting', 'knitting_needle', 'knob', 'knot', 'koala', 'lab_coat', 'ladder', 'ladybug', 'lamb_chop', 'lamp', 'lamppost', 'landmine', 'lantern', 'lanyard', 'laptop', 'lasagna', 'laser_pointer', 'latch', 'latte', 'lava', 'lavender', 'lawnmower', 'leaf', 'leash', 'lectern', 'leech', 'leek', 'leg', 'leggings', 'lego', 'lemon', 'lemonade', 'lens', 'leopard', 'leotard', 'letter_opener', 'license_plate', 'licorice', 'lid', 'life_jacket', 'lifesaver', 'light_switch', 'lightbulb', 'lighter', 'lime', 'limousine', 'lingerie', 'lion', 'lip_balm', 'lip_gloss', 'lipstick', 'lizard', 'llama', 'lobster', 'lock', 'locker', 'locket', 'log', 'loincloth', 'lollipop', 'loom', 'loveseat', 'luggage', 'lumber', 'lunchbox', 'macadamia', 'macaroni', 'machete', 'machine_gun', 'maggot', 'magnet', 'magnifier', 'magnifying_glass', 'mail', 'mailbox', 'makeup', 'mallet', 'man', 'mandolin', 'mango', 'manhole', 'mannequin', 'mantle', 'map', 'maple_syrup', 'marble', 'margarita', 'marker', 'marmalade', 'marshmallow', 'mascara', 'mashed_potato', 'mask', 'mast', 'mat', 'match', 'matchbox', 'mattress', 'measuring_cup', 'meat', 'meat_grinder', 'meatball', 'medal', 'meerkat', 'megaphone', 'melon', 'memory_stick', 'metronome', 'microphone', 'microscope', 'microwave', 'milk', 'milkshake', 'mint', 'mirror', 'missile', 'mistletoe', 'mitten', 'mixer', 'moccasin', 'mold1', 'mold2', 'mole', 'money', 'mongoose', 'monkey', 'moose', 'mop', 'mosquito_net', 'moss', 'moth', 'motherboard', 'motorcycle', 'mouse1', 'mouse2', 'mousepad', 'mousetrap', 'mousse', 'mouth', 'mouthpiece', 'mud', 'muffin', 'mug', 'mulberry', 'mulch', 'mullet', 'mushroom', 'mustache', 'mustard', 'nacho', 'nail', 'nail_clippers', 'nail_file', 'nail_polish', 'napkin', 'napkin_ring', 'navel', 'neck', 'necklace', 'needle', 'nest', 'net', 'nightshirt', 'noisemaker', 'noodle', 'noose', 'nose', 'notebook', 'notepad', 'nut', 'nutcracker', 'oar', 'oatmeal', 'octopus', 'odometer', 'oil', 'oilcan', 'olive', 'orange_rind', 'orangutan', 'organ', 'origami', 'otter', 'ottoman', 'outfit', 'outlet', 'oven', 'overalls', 'owl', 'oyster', 'pacifier', 'paddle', 'padlock', 'paint', 'paintbrush', 'painting', 'palette', 'pallet', 'palm_tree', 'pan', 'pancake', 'panda', 'panties', 'pants', 'pantsuit', 'pantyhose', 'papaya', 'paper', 'paper_bag', 'paper_plate', 'paper_towel', 'paperclip', 'parachute', 'parfait', 'parking_meter', 'parrot', 'parsley', 'pasta', 'pastry', 'patch', 'patty', 'payphone', 'pea', 'peach', 'peacock', 'peanut', 'peanut_butter', 'pearl', 'pecan', 'pedal', 'pedometer', 'peeler', 'peg', 'pelican', 'pen', 'pencil', 'pencil_sharpener', 'pendulum', 'penguin', 'penholder', 'penlight', 'pennant', 'pepper2', 'pepper_mill', 'peppermint', 'pepperoni', 'perfume', 'periscope', 'pesto', 'pet_food', 'petal', 'petri_dish', 'phone', 'phone_booth', 'photo_booth', 'photograph', 'piano', 'pickle', 'piecrust', 'pig', 'piggy_bank', 'pill', 'pillbox', 'pillow', 'pin', 'pinball', 'pincushion', 'pine_needle', 'pine_tree', 'pineapple', 'pinecone', 'ping-pong_table', 'pinwheel', 'pipe1', 'pipe2', 'pistachio', 'pita', 'pitcher', 'pitchfork', 'pizza', 'place_mat', 'plant', 'plaster_cast', 'plastic_film', 'plate', 'platypus', 'playing_card', 'playpen', 'pliers', 'plug', 'plum', 'plunger', 'pocket_watch', 'pogo_stick', 'poinsettia', 'poker', 'polar_bear', 'polaroid', 'pole', 'police_car', 'polisher', 'polo_shirt', 'polygraph', 'pom-pom', 'pomegranate', 'pony', 'poodle', 'pool_table', 'poppy', 'porcupine', 'porthole', 'poster', 'pot', 'potato', 'potholder', 'pothole', 'potpie', 'potpourri', 'powder', 'power_line', 'praying_mantis', 'printer', 'prism', 'projector', 'propeller', 'prune', 'puck', 'pudding', 'puddle', 'puffin', 'pulley', 'pulpit', 'pump', 'pumpkin', 'punch1', 'punching_bag', 'puppet', 'puppy', 'quad', 'quesadilla', 'quiche', 'quill', 'quilt', 'rabbit', 'raccoon', 'racehorse', 'rack1', 'rack2', 'racket', 'radar', 'radiator', 'radio', 'raft', 'rag', 'railing', 'rain_gauge', 'raincoat', 'raisin', 'rake', 'ram', 'ramp', 'rat', 'ratchet', 'rattle', 'rattlesnake', 'ravioli', 'razor', 'razor_blade', 'ready_meal', 'rearview_mirror', 'recliner', 'record', 'record_player', 'red_carpet', 'reel', 'refrigerator', 'reindeer', 'remote_control', 'retainer', 'revolver', 'revolving_door', 'rhubarb', 'ribbon', 'rice', 'rickshaw', 'rifle', 'rim', 'ring', 'riser', 'road_sign', 'roadsweeper', 'robe', 'rock', 'rocket', 'rocking_chair', 'rocking_horse', 'roll', 'roller', 'roller_coaster', 'rollerblade', 'rollerskate', 'rolling_pin', 'roof_rack', 'root', 'rope', 'rosary', 'rose', 'roulette_wheel', 'router', 'rubber_band', 'ruby', 'rudder', 'ruler', 'rust', 'saddle', 'safety_pin', 'saffron', 'sail', 'salad', 'salami', 'saltshaker', 'sand', 'sandbag', 'sandbox', 'sandcastle', 'sandwich', 'sarcophagus', 'sardine', 'satellite', 'satellite_dish', 'sauce', 'saucer', 'sauerkraut', 'saw', 'sawhorse', 'saxophone', 'scaffold', 'scaffolding', 'scale', 'scalpel', 'scanner', 'scarecrow', 'scarf', 'school_bus', 'scissors', 'scone', 'scoop', 'scoreboard', 'scorpion', 'scrabble', 'scrambled_egg', 'scraper', 'screen1', 'screen2', 'screw', 'screwdriver', 'scuba', 'sea_urchin', 'seafood', 'seahorse', 'seal', 'seaplane', 'seatbelt', 'seesaw', 'seismograph', 'sequin', 'sewage', 'sewing_kit', 'sewing_machine', 'shaker', 'shark', 'shaving_cream', 'shawl', 'shears', 'sheath', 'sheep', 'sheet', 'shelf', 'shell1', 'shell2', 'shell3', 'shield', 'ship', 'shirt', 'shoe', 'shoe_polish', 'shoehorn', 'shoelace', 'shopping_basket', 'shopping_cart', 'shortbread', 'shorts', 'shoulder', 'shovel', 'shower', 'shower_cap', 'shower_curtain', 'showerhead', 'shredder', 'shrimp', 'shuffleboard', 'shutter', 'sickle', 'sidecar', 'sifter', 'silicone', 'silverware', 'sim_card', 'sink', 'siren', 'skeleton', 'skewer', 'ski', 'ski_boots', 'ski_lift', 'ski_pole', 'skin', 'skirt', 'skull', 'skunk', 'sledgehammer', 'slicer', 'slime', 'sling', 'slipper', 'slot', 'slot_machine', 'sloth', 'slug', 'smoke_alarm', 'smoothie', 'snack', 'snail', 'snake', 'snorkel', 'snow', 'snowball', 'snowboard', 'snowman', 'snowmobile', 'snowplow', 'snowsuit', 'soap', 'soccer_ball', 'sock', 'soda', 'soda_fountain', 'sofa_bed', 'solar_panel', 'soldering_iron', 'sombrero', 'sonogram', 'sorbet', 'souffle', 'soup', 'soy_sauce', 'space_shuttle', 'spacesuit', 'spaghetti', 'spam', 'spareribs', 'spark_plug', 'sparkler', 'speaker', 'spear', 'speedboat', 'speedometer', 'spider', 'spider_web', 'spinach', 'splinter', 'sponge', 'spool', 'spout', 'spring_roll', 'springboard', 'sprinkler', 'sprouts', 'spur', 'squash', 'squeegee', 'squid', 'squirrel', 'squirt_gun', 'stained_glass', 'stair', 'stake', 'stalagmite', 'stamp1', 'stamp2', 'staple', 'staple_gun', 'stapler', 'star_fruit', 'starfish', 'statue', 'steak', 'steamroller', 'steering_wheel', 'stem', 'step_stool', 'stereo', 'stew', 'stick', 'sticker', 'stiletto', 'stilt', 'stingray', 'stir_fry', 'stirrup', 'stockings', 'stomach', 'stool', 'stopwatch', 'stove1', 'stove2', 'straightjacket', 'strainer', 'strap', 'straw1', 'straw2', 'streetlight', 'stretcher', 'string_cheese', 'stroller', 'stuffing', 'stump', 'subway', 'sugar_cube', 'suitcase', 'sundae', 'sundial', 'sunflower', 'sunglasses', 'sunroof', 'surfboard', 'sushi', 'suspenders', 'swab', 'swan', 'sweater', 'sweatsuit', 'sweeper', 'sweet_potato', 'swimming_pool', 'swimsuit', 'swing', 'swing_set', 'switch', 'swizzle_stick', 'sword', 'swordfish', 'syringe', 'syrup', 'tab', 'tablecloth', 'tablet', 'tack', 'tackle', 'taco', 'tadpole', 'taffy', 'tag', 'tamale', 'tambourine', 'tank1', 'tank2', 'tape', 'tape_measure', 'tapestry', 'tarantula', 'target', 'tarp', 'tattoo', 'taxi', 'tea', 'teabag', 'teacup', 'teapot', 'teddy_bear', 'tee', 'teepee', 'telegraph', 'telephone_pole', 'telescope', 'tennis_ball', 'tent', 'terrarium', 'test_tube', 'thermometer', 'thermos', 'thermostat', 'thimble', 'thorn', 'thread', 'throne', 'thumb', 'thumbtack', 'ticktacktoe', 'tie', 'tiger', 'tile', 'timer', 'tinsel', 'tiramisu', 'toad', 'toast', 'toaster', 'toaster_oven', 'toe', 'toga', 'toilet', 'toilet_paper', 'tomato', 'tongue', 'toolbox', 'tooth', 'toothbrush', 'toothpaste', 'toothpick', 'torch', 'torpedo', 'torso', 'tortellini', 'tortilla', 'tostada', 'totem_pole', 'toucan', 'touchpad', 'towel', 'towel_rack', 'toy', 'tractor', 'traffic_light', 'trailer', 'train', 'train_car', 'train_set', 'trampoline', 'trap', 'trapdoor', 'trashcan', 'tray', 'treasure', 'tree', 'tree_trunk', 'triangle', 'tricycle', 'trident', 'trigger', 'tripod', 'trolley', 'trombone', 'trophy', 'trough', 'trowel', 'truck', 'trumpet', 'trunk', 'tuba', 'tugboat', 'tulip', 'tumbleweed', 'tuning_fork', 'tupperware', 'turban', 'turbine', 'turf', 'turnstile', 'turntable', 'turtle', 'turtleneck', 'tuxedo', 'tweezers', 'twig', 'typewriter', 'ukulele', 'umbrella', 'undershirt', 'underwear', 'uniform', 'urinal', 'urn', 'vacuum', 'valve', 'van', 'vase', 'vegetable', 'veil', 'velcro', 'vending_machine', 'vent', 'vest', 'vial', 'videocassette', 'videogame', 'viewfinder', 'violin', 'visor', 'vulture', 'wafer', 'waffle', 'waffle_iron', 'wagon', 'walker1', 'walker2', 'wall', 'wallet', 'walrus', 'wand', 'warthog', 'washboard', 'washcloth', 'washing_machine', 'wasp', 'watch', 'water_bottle', 'water_cooler', 'water_filter', 'water_fountain', 'water_heater', 'watering_can', 'watermelon', 'waterwheel', 'wax', 'wax_paper', 'weasel', 'weather_vane', 'webcam', 'wedding_cake', 'wedge', 'weed', 'wetsuit', 'whale', 'wheel', 'wheelbarrow', 'whip', 'whipped_cream', 'whisk', 'whistle', 'whiteboard', 'whoopee_cushion', 'wick', 'wig', 'wind_chimes', 'window', 'windowsill', 'windshield_wiper', 'windsock', 'wine_bottle', 'wine_cooler', 'wineglass', 'wing', 'wire', 'wire_cutters', 'wolf', 'woman', 'wood', 'wooden_leg', 'workbench', 'worm', 'wrap', 'wrapping_paper', 'wreath', 'wreck', 'wrench', 'wrist', 'xylophone', 'yacht', 'yak', 'yarn', 'yo-yo', 'yogurt', 'yoke', 'yolk', 'zebra', 'zipper', 'zucchini'] 1654\n",
      "['aircraft_carrier', 'antelope', 'backscratcher', 'balance_beam', 'banana', 'baseball_bat', 'basil', 'basketball', 'bassoon', 'baton4', 'batter', 'beaver', 'bench', 'bike', 'birthday_cake', 'blowtorch', 'boat', 'bok_choy', 'bonnet', 'bottle_opener', 'brace', 'bread', 'breadbox', 'bug', 'buggy', 'bullet', 'bun', 'bush', 'calamari', 'candlestick', 'cart', 'cashew', 'cat', 'caterpillar', 'cd_player', 'chain', 'chaps', 'cheese', 'cheetah', 'chest2', 'chime', 'chopsticks', 'cleat', 'cleaver', 'coat', 'cobra', 'coconut', 'coffee_bean', 'coffeemaker', 'cookie', 'cordon_bleu', 'coverall', 'crab', 'creme_brulee', 'crepe', 'crib', 'croissant', 'crow', 'cruise_ship', 'crumb', 'cupcake', 'dagger', 'dalmatian', 'dessert', 'dragonfly', 'dreidel', 'drum', 'duffel_bag', 'eagle', 'eel', 'egg', 'elephant', 'espresso', 'face_mask', 'ferry', 'flamingo', 'folder', 'fork', 'freezer', 'french_horn', 'fruit', 'garlic', 'glove', 'golf_cart', 'gondola', 'goose', 'gopher', 'gorilla', 'grasshopper', 'grenade', 'hamburger', 'hammer', 'handbrake', 'headscarf', 'highchair', 'hoodie', 'hummingbird', 'ice_cube', 'ice_pack', 'jeep', 'jelly_bean', 'jukebox', 'kettle', 'kneepad', 'ladle', 'lamb', 'lampshade', 'laundry_basket', 'lettuce', 'lightning_bug', 'manatee', 'marijuana', 'meatloaf', 'metal_detector', 'minivan', 'modem', 'mosquito', 'muff', 'music_box', 'mussel', 'nightstand', 'okra', 'omelet', 'onion', 'orange', 'orchid', 'ostrich', 'pajamas', 'panther', 'paperweight', 'pear', 'pepper1', 'pheasant', 'pickax', 'pie', 'pigeon', 'piglet', 'pocket', 'pocketknife', 'popcorn', 'popsicle', 'possum', 'pretzel', 'pug', 'punch2', 'purse', 'radish', 'raspberry', 'recorder', 'rhinoceros', 'robot', 'rooster', 'rug', 'sailboat', 'sandal', 'sandpaper', 'sausage', 'scallion', 'scallop', 'scooter', 'seagull', 'seaweed', 'seed', 'skateboard', 'sled', 'sleeping_bag', 'slide', 'slingshot', 'snowshoe', 'spatula', 'spoon', 'station_wagon', 'stethoscope', 'strawberry', 'submarine', 'suit', 't-shirt', 'table', 'taillight', 'tape_recorder', 'television', 'tiara', 'tick', 'tomato_sauce', 'tongs', 'tool', 'top_hat', 'treadmill', 'tube_top', 'turkey', 'unicycle', 'vise', 'volleyball', 'wallpaper', 'walnut', 'wheat', 'wheelchair', 'windshield', 'wine', 'wok'] 200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "training_images_dir = '/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/images_set/training_images'\n",
    "test_images_dir = '/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/images_set/test_images'\n",
    "train_dir_names = os.listdir(training_images_dir)\n",
    "test_dir_names = os.listdir(test_images_dir)\n",
    "# erase the part before the first underscore\n",
    "train_dir_names = [name.split('_', 1)[1] for name in train_dir_names]\n",
    "test_dir_names = [name.split('_', 1)[1] for name in test_dir_names]\n",
    "print(sorted(train_dir_names), len(train_dir_names))\n",
    "print(sorted(test_dir_names), len(test_dir_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concepts: 720\n"
     ]
    }
   ],
   "source": [
    "# extract all concepts from stimulus metadata\n",
    "concepts = stimdata['concept'].unique()\n",
    "print('concepts:', len(concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_concepts: 633\n",
      "test_concepts: 87\n"
     ]
    }
   ],
   "source": [
    "# find concepts that are in the training set and test set\n",
    "train_concepts = [concept for concept in concepts if concept in train_dir_names]\n",
    "test_concepts = [concept for concept in concepts if concept in test_dir_names]\n",
    "print('train_concepts:', len(train_concepts))\n",
    "print('test_concepts:', len(test_concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all trials that trial_type is test\n",
    "# stimdata = stimdata[stimdata['trial_type'] == 'train']\n",
    "# print('stimdata:', stimdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find trial_ids for test and train concepts\n",
    "# train_trial_ids = stimdata[stimdata['concept'].isin(train_concepts)]['trial_id']\n",
    "# test_trial_ids = stimdata[stimdata['concept'].isin(test_concepts)]['trial_id']\n",
    "# print('train_trial_ids:', len(train_trial_ids))\n",
    "# print('test_trial_ids:', len(test_trial_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "responses_pick: (5238, 9841)\n",
      "responses_pick: (5238, 9840)\n",
      "responses_pick: [[ 0.01900595  0.0229135  -0.06697899 ...  0.08979687  0.05588575\n",
      "   0.04279312]\n",
      " [ 0.01402964 -0.01851454 -0.16293375 ...  0.05386849  0.12791638\n",
      "   0.00577851]\n",
      " [ 0.00799971 -0.04215239 -0.06869916 ...  0.06243646  0.00178091\n",
      "  -0.01686477]\n",
      " ...\n",
      " [-0.08852731 -0.07090534 -0.0025794  ... -0.01899403 -0.02433767\n",
      "  -0.0199908 ]\n",
      " [ 0.12763731  0.01390089 -0.00898259 ...  0.00069999  0.08681266\n",
      "  -0.00326645]\n",
      " [-0.04713013  0.02682863  0.03472633 ... -0.09340548 -0.0028976\n",
      "   0.03100214]]\n"
     ]
    }
   ],
   "source": [
    "# apply voxel mask to responses\n",
    "responses_np = responses.to_numpy()\n",
    "responses_pick = responses_np[vox_pick,:]\n",
    "print('responses_pick:', responses_pick.shape)\n",
    "\n",
    "# remove the first column of responses_pick\n",
    "responses_pick = responses_pick[:,1:]\n",
    "print('responses_pick:', responses_pick.shape)\n",
    "print('responses_pick:', responses_pick)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        True\n",
       "1        True\n",
       "2        True\n",
       "3       False\n",
       "4        True\n",
       "        ...  \n",
       "9835     True\n",
       "9836     True\n",
       "9837     True\n",
       "9838     True\n",
       "9839     True\n",
       "Name: concept, Length: 9840, dtype: bool"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimdata['concept'].isin(train_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            jack\n",
       "1           sword\n",
       "2          peeler\n",
       "3           onion\n",
       "4         go-kart\n",
       "          ...    \n",
       "9835    butterfly\n",
       "9836         lime\n",
       "9837       seesaw\n",
       "9838      dustpan\n",
       "9839          pea\n",
       "Name: concept, Length: 9840, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimdata['concept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3        True\n",
       "4       False\n",
       "        ...  \n",
       "9835    False\n",
       "9836    False\n",
       "9837    False\n",
       "9838    False\n",
       "9839    False\n",
       "Name: concept, Length: 9840, dtype: bool"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimdata['concept'].isin(test_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设 stimdata 是一个字典或类似结构，包含 'trial_type' 和 'stimulus'\n",
    "# 并且 responses_pick 是一个二维 NumPy 数组，维度为 (features, trials)\n",
    "# fill train_data and test_data with responses_pick according to the sorted trial_ids one by one\n",
    "test_data = responses_pick[:, stimdata['concept'].isin(test_concepts)]\n",
    "train_data = responses_pick[:, stimdata['concept'].isin(train_concepts)]\n",
    "\n",
    "# 创建布尔掩码\n",
    "# test_mask = stimdata['concept'].isin(test_concepts)\n",
    "test_mask = stimdata['concept'].isin(test_concepts) & (stimdata['trial_type'] != 'test')\n",
    "\n",
    "# train_mask = stimdata['concept'].isin(train_concepts)\n",
    "train_mask = stimdata['concept'].isin(train_concepts) & (stimdata['trial_type'] != 'test')\n",
    "\n",
    "# # 分别提取 test 和 train 数据\n",
    "# test_data = responses_pick[:, test_mask]\n",
    "# train_data = responses_pick[:, train_mask]\n",
    "\n",
    "# 提取对应的 stimulus\n",
    "test_stimulus = stimdata['stimulus'][test_mask]\n",
    "train_stimulus = stimdata['stimulus'][train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3              onion_11n.jpg\n",
       "5           antelope_11s.jpg\n",
       "9            manatee_11s.jpg\n",
       "12      sleeping_bag_11s.jpg\n",
       "33            possum_11s.jpg\n",
       "                ...         \n",
       "9765      wheelchair_12n.jpg\n",
       "9793       taillight_12s.jpg\n",
       "9801           tongs_12s.jpg\n",
       "9817     caterpillar_12s.jpg\n",
       "9825      rhinoceros_12s.jpg\n",
       "Name: stimulus, Length: 1044, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 获取排序后的索引（升序）\n",
    "sorted_test_indices = np.argsort(test_stimulus)\n",
    "sorted_train_indices = np.argsort(train_stimulus)\n",
    "\n",
    "# 重排 test_data 和 train_data\n",
    "sorted_test_data = test_data[:, sorted_test_indices]\n",
    "sorted_train_data = train_data[:, sorted_train_indices]\n",
    "\n",
    "\n",
    "# 如果需要，可以将排序后的数据重新赋值回原变量\n",
    "test_data = sorted_test_data\n",
    "train_data = sorted_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map between each concept and its corresponding trial_ids\n",
    "# concept_trial_ids = {}\n",
    "# for concept in concepts:\n",
    "#     concept_trial_ids[concept] = stimdata[stimdata['concept'] == concept]['trial_id'].to_list()\n",
    "# print('concept_trial_ids:', concept_trial_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the stimulus data into training and test sets\n",
    "# train_stimdata = stimdata[stimdata['concept'].isin(train_concepts)]\n",
    "# test_stimdata = stimdata[stimdata['concept'].isin(test_concepts)]\n",
    "# # reaaranage stimdata in order of stimulus, alphabetically\n",
    "# train_stimdata = train_stimdata.sort_values(by='stimulus')\n",
    "# test_stimdata = test_stimdata.sort_values(by='stimulus')\n",
    "# print('train_stimdata:', train_stimdata.shape)\n",
    "# print('test_stimdata:', test_stimdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the responses data into training and test sets one by one according to the trial_id in the stimulus data\n",
    "# # get list of trial_ids for training and test\n",
    "# train_trial_ids = train_stimdata['trial_id'].to_list()\n",
    "# test_trial_ids = test_stimdata['trial_id'].to_list()\n",
    "# print('train_trial_ids:', len(train_trial_ids))\n",
    "# print('test_trial_ids:', len(test_trial_ids))\n",
    "# # erase the first column of responses\n",
    "# responses_pure = responses.iloc[:, 1:]\n",
    "# print('responses_pure:', responses_pure.shape)\n",
    "# test_responses = responses_pure.iloc[:, test_trial_ids]\n",
    "# train_responses = responses_pure.iloc[:, train_trial_ids]\n",
    "# print('train_responses:', train_responses.shape)\n",
    "# print('test_responses:', test_responses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply the voxel mask\n",
    "# train_responses = train_responses[vox_pick]\n",
    "# test_responses = test_responses[vox_pick]\n",
    "# print('train_responses:', train_responses.shape)\n",
    "# print('test_responses:', test_responses.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_voxels = train_responses.T.values\n",
    "# test_voxels = test_responses.T.values\n",
    "# test_voxels.shape\n",
    "# # train_voxels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7596, 5238)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.transpose()\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1044, 5238)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data.transpose()\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(633, 12, 5238)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_voxels = train_data.reshape(train_data.shape[0] // 12, 12, -1)\n",
    "train_voxels.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87, 12, 5238)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 拆分 test_responses\n",
    "# 计算新形状\n",
    "test_voxels = test_data.reshape(test_data.shape[0] // 12,  12, -1)\n",
    "test_voxels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_responses and test_responses saved to pkl successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from os.path import join as pjoin\n",
    "\n",
    "output_dir = pjoin(f'/mnt/dataset0/ldy/datasets/fmri_dataset/Preprocessed_rerank/sub-{sub}')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 保存 train_voxels 到 pkl 文件\n",
    "with open(pjoin(output_dir, 'train_responses.pkl'), 'wb') as f:\n",
    "    pickle.dump(train_voxels, f)\n",
    "\n",
    "# 保存 test_voxels 到 pkl 文件\n",
    "with open(pjoin(output_dir, 'test_responses.pkl'), 'wb') as f:\n",
    "    pickle.dump(test_voxels, f)\n",
    "\n",
    "print('train_responses and test_responses saved to pkl successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# copy images to the output directory\n",
    "origin_root = '/mnt/dataset0/ldy/4090_Workspace/4090_THINGS/osfstorage/THINGS/Images/images'\n",
    "output_dir_train = '/mnt/dataset0/ldy/datasets/fmri_dataset/images_rerank/train_images'\n",
    "output_dir_test = '/mnt/dataset0/ldy/datasets/fmri_dataset/images_rerank/test_images'\n",
    "if not os.path.exists(output_dir_train):\n",
    "    os.makedirs(output_dir_train)\n",
    "if not os.path.exists(output_dir_test):\n",
    "    os.makedirs(output_dir_test)\n",
    "# join origin_root with concept, stimulus to get the full path of the image\n",
    "train_paths = [pjoin(origin_root, concept, stimulus) for concept, stimulus in zip(stimdata['concept'][train_mask], stimdata['stimulus'][train_mask])]\n",
    "test_paths = [pjoin(origin_root, concept, stimulus) for concept, stimulus in zip(stimdata['concept'][test_mask], stimdata['stimulus'][test_mask])]\n",
    "print(train_paths[:5])\n",
    "print(test_paths[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src_path in train_paths:\n",
    "    # 获取概念名称（文件路径的倒数第二部分）\n",
    "    concept = src_path.split('/')[-2]  # 获取目录名作为概念\n",
    "    # 创建目标子目录\n",
    "    concept_dir = os.path.join(output_dir_train, concept)\n",
    "    os.makedirs(concept_dir, exist_ok=True)\n",
    "    # 复制文件\n",
    "    shutil.copy(src_path, concept_dir)\n",
    "\n",
    "for src_path in test_paths:\n",
    "    # 获取概念名称（文件路径的倒数第二部分）\n",
    "    concept = src_path.split('/')[-2]  # 获取目录名作为概念\n",
    "    # 创建目标子目录\n",
    "    concept_dir = os.path.join(output_dir_test, concept)\n",
    "    os.makedirs(concept_dir, exist_ok=True)\n",
    "    # 复制文件\n",
    "    shutil.copy(src_path, concept_dir)\n",
    "    \n",
    "print('images copied successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "training_images_dir = \"/mnt/dataset0/ldy/datasets/fmri_dataset/images_rerank/train_images\"\n",
    "\n",
    "def count_images(directory):\n",
    "    total_dirs = 0\n",
    "    total_images = 0\n",
    "\n",
    "\n",
    "    for entry in os.listdir(directory):\n",
    "        path = os.path.join(directory, entry)\n",
    "        if os.path.isdir(path):\n",
    "            total_dirs += 1\n",
    "\n",
    "            total_images += len([file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))])\n",
    "\n",
    "    return total_dirs, total_images\n",
    "\n",
    "num_dirs, num_images = count_images(training_images_dir)\n",
    "\n",
    "print(f\"There are {num_dirs} subdirectories in total\")\n",
    "print(f\"All subdirectories together contain {num_images} images in total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "test_images_dir = \"/mnt/dataset0/ldy/datasets/fmri_dataset/images_rerank/test_images\"\n",
    "\n",
    "def count_images(directory):\n",
    "    total_dirs = 0\n",
    "    total_images = 0\n",
    "\n",
    "\n",
    "    for entry in os.listdir(directory):\n",
    "        path = os.path.join(directory, entry)\n",
    "        if os.path.isdir(path):\n",
    "            total_dirs += 1\n",
    "\n",
    "            total_images += len([file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))])\n",
    "\n",
    "    return total_dirs, total_images\n",
    "\n",
    "num_dirs, num_images = count_images(test_images_dir)\n",
    "\n",
    "print(f\"There are {num_dirs} subdirectories in total\")\n",
    "print(f\"All subdirectories together contain {num_images} images in total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path = \"/mnt/dataset0/ldy/datasets/fmri_dataset/Preprocessed_rerank/sub-01/train_responses.pkl\"\n",
    "\n",
    "\n",
    "with open(path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "fmri_data = data\n",
    "# ch_names = data['ch_names']\n",
    "# times = data['times']\n",
    "# meg_data.shape\n",
    "fmri_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "path = \"/mnt/dataset0/ldy/datasets/fmri_dataset/Preprocessed_rerank/sub-01/test_responses.pkl\"\n",
    "\n",
    "\n",
    "with open(path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "fmri_data = data\n",
    "# ch_names = data['ch_names']\n",
    "# times = data['times']\n",
    "fmri_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sub-{subject}_StimulusMetadata.csv` files contain information about the file name of the image shown in each trial, which run and session a given trial occured in, and the trial_type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🚨 **Trial types**\n",
    ">\n",
    "> The THINGS-fMRI experiment presented participants with three different trial types:\n",
    "> - `train`: Participants passively viewed an object image.\n",
    "> - `test`: Same as train, but these trials belonged to a set of 200 images which were presented in each session. It's main purpose is to allow for estimating the reliability of the single trial responses in a given voxel.\n",
    "> - `catch`: Participants saw a non-object image and responded with a button press. This was included to ensure participants were engaged throughout the experiment.\n",
    ">\n",
    "> Note: Catch trials are excluded from the single trial responses in table format as they are likely not of interest for most applications. However, catch trials are included in the volumetric format in order to make it possible to account for them in analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select subset of trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select a subset of the response data based on the stimulus type, e.g. only the repeatedly presented `test` stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = stimdata.query('trial_type == \"test\"').index\n",
    "test_responses = responses[test_indices]\n",
    "test_responses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example, we can also select data based on which object category (or `concept`) was shown. Let's select all trials with images showing a `mango`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mango_indices = stimdata.query('concept == \"mango\"').index\n",
    "mango_responses = responses[mango_indices]\n",
    "mango_responses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single trial responses are also provided in volume format which preserves the spatial structure of the data. The data is broken up into runs and sessions, similar to the raw data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the directory containing the single trial responses in volume form\n",
    "betas_vol_dir = pjoin(basedir, 'betas_vol', f'sub-{sub}')\n",
    "# show directory content\n",
    "files = glob.glob(pjoin(betas_vol_dir, '*', '*'))\n",
    "files = [f.replace(basedir, '') for f in files]\n",
    "print('\\n'.join(files[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `...betas.nii.gz` files are 3D+time nifti images where the time dimension corresponds to  trials. The `...conditions.tsv` contain the file names of object images for each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load responses for example run\n",
    "betas_f = pjoin(betas_vol_dir, 'ses-things01', f'sub-{sub}_ses-things01_run-01_betas.nii.gz')\n",
    "betas_example = load_img(betas_f)\n",
    "\n",
    "# and plot the volume of the 3rd trial\n",
    "betas_example = index_img(betas_example, 2)\n",
    "g = plot_stat_map(\n",
    "    betas_example, bg_img=None, annotate=False, cmap='twilight', vmax=.4,  draw_cross=False, \n",
    ")\n",
    "g.title('Example: Volumetric single trial response (trial# 3)', bgcolor='white', color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trial conditions\n",
    "# Note that the volumetric single trial responses include catch trials\n",
    "conds_tsv = pjoin(betas_vol_dir, 'ses-things01', f'sub-{sub}_ses-things01_run-01_conditions.tsv')\n",
    "conds = pd.read_csv(conds_tsv, sep='\\t').drop(columns='Unnamed: 0')\n",
    "print('Content of \"...conditions.tsv\"')\n",
    "conds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brain masks indicate wether a given voxel is located in the brain or not (`1: brain`, `0: not brain`). They can be used e.g. to create a nifti volume from the results you obtained from the single trial responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Say you have analyzed the single trial responses in table format have produced these results\n",
    "# These results are an array of n elements, where n is the number of voxels within the brain.\n",
    "results = np.random.randn(responses.shape[0])\n",
    "loinds = voxdata['lLOC'].astype(bool) # pretend we found activity in LOC\n",
    "results[loinds] += 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can easily use nilearn's masking functions to transform them \n",
    "# to an image object and save it as a nifti file\n",
    "\n",
    "# This is the provided brain mask\n",
    "bmask_dir = pjoin(basedir, 'brainmasks')\n",
    "bmask_f = pjoin(bmask_dir, f'sub-{sub}_space-T1w_brainmask.nii.gz')\n",
    "\n",
    "# use the brain mask to turn your results array into a 3D image\n",
    "results_img = unmask(results, bmask_f)\n",
    "# plot the image to verify\n",
    "g = plot_stat_map(results_img, bg_img=None, cmap='twilight', draw_cross=False, annotate=False)\n",
    "# g.add_contours(bmask_f)\n",
    "g.title('Example: Reconstructing volume from array-like data', bgcolor='white', color='black')\n",
    "# and save it as an nifti file\n",
    "results_img.to_filename('results.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🚨 **Co-registration and volumetric space**\n",
    ">\n",
    "> The THINGS-fMRI data was preprocessed with [fmriprep](https://fmriprep.org/en/stable/), which includes co-registration of all functional images to a high-resolution anatomical MRI image. In other words, all functional data for a given subject (including the brain masks aind regions of interest) was transformed into a common \"space\", meaning that a given voxel always points to the same location in the brain - with some level of imperfection.\n",
    "> Of course, it's possible to analyze the data in a different space (e.g. MNI) by downloading the raw data and preprocessing it according to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cortical flat maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide flat maps for each subject for visualizing results on a flat representation of the cortical surface with `pycortex`. Provided that you saved your results as a nifti file in the same space that the volumetric data was in, you can use the flat maps and transformation matrices we prepared. Before you can get started, check out the [pycortex documentation](https://gallantlab.github.io/pycortex/) for an explanation on how to set up your installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results we prepared above as a volume object in pycortex\n",
    "results_data = np.swapaxes(load_img('results.nii.gz').get_fdata(), 0, -1)\n",
    "vol_data = cortex.Volume(results_data, 'S1', 'align_auto', cmap='twilight', vmin=-6, vmax=6)\n",
    "# plot with pycortex\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "cortex.quickshow(\n",
    "    vol_data, pixelwise=True, nanmean=True, colorbar_location='left', with_rois=False, fig=fig,\n",
    ")\n",
    "plt.title(f'Example: Visualizing results on flat maps')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
