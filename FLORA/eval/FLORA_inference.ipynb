{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/local_attention/rotary.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/local_attention/rotary.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/colt5_attention/coor_descent.py:16: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/colt5_attention/topk.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ[\"WANDB_API_KEY\"] = \"KEY\"\n",
    "os.environ[\"WANDB_MODE\"] = 'offline'\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "# 获取当前工作目录（假设 Notebook 位于 parent_dir）\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# 构建项目根目录的路径（假设 parent_dir 和 model 同级）\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "# 将项目根目录添加到 sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# 现在可以使用绝对导入\n",
    "from model.unified_encoder_multi_tower import UnifiedEncoder\n",
    "\n",
    "# 导入必要的库\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import wandb\n",
    "wandb.init(mode=\"disabled\")\n",
    "\n",
    "from data_preparing.eegdatasets import EEGDataset\n",
    "from data_preparing.megdatasets_averaged import MEGDataset\n",
    "from data_preparing.fmri_datasets_joint_subjects import fMRIDataset\n",
    "from data_preparing.datasets_mixer import MetaEEGDataset, MetaMEGDataset, MetafMRIDataset, MetaDataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from loss import ClipLoss\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Modality: eeg\n",
      "Test Subjects: ['sub-01', 'sub-02', 'sub-03', 'sub-04', 'sub-05', 'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10']\n",
      "Number of Test Classes: 200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_id_from_string(s):\n",
    "    match = re.search(r'\\d+$', s)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "def get_eegfeatures(unified_model, dataloader, device, text_features_all, img_features_all, k, eval_modality, test_classes):\n",
    "    unified_model.eval()\n",
    "    text_features_all = text_features_all[eval_modality].to(device).float()\n",
    "    if eval_modality=='eeg' or eval_modality=='fmri':\n",
    "        img_features_all = (img_features_all[eval_modality]).to(device).float()\n",
    "    elif eval_modality=='meg':\n",
    "        img_features_all = (img_features_all[eval_modality][::12]).to(device).float()  \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    top5_correct_count=0\n",
    "    total = 0\n",
    "    loss_func = ClipLoss() \n",
    "    all_labels = set(range(text_features_all.size(0)))\n",
    "    save_features = False\n",
    "    features_list = []  # List to store features    \n",
    "    features_tensor = torch.zeros(0, 0)\n",
    "    sub = 'sub-02'\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (modal, data, labels, text, text_features, img, img_features, _, _, sub_ids) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            text_features = text_features.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            img_features = img_features.to(device).float()\n",
    "            \n",
    "            batch_size = data.size(0) \n",
    "            subject_ids = [extract_id_from_string(sub_id) for sub_id in sub_ids]\n",
    "            subject_ids = torch.tensor(subject_ids, dtype=torch.long).to(device)\n",
    "            neural_features = unified_model(data, subject_ids, modal=eval_modality)\n",
    "            \n",
    "            logit_scale = unified_model.logit_scale.float()            \n",
    "            features_list.append(neural_features)\n",
    "               \n",
    "            img_loss = loss_func(neural_features, img_features, logit_scale)\n",
    "            loss = img_loss        \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            for idx, label in enumerate(labels):\n",
    "\n",
    "                possible_classes = list(all_labels - {label.item()})\n",
    "                selected_classes = random.sample(possible_classes, k-1) + [label.item()]\n",
    "                selected_img_features = img_features_all[selected_classes]\n",
    "                \n",
    "\n",
    "                logits_img = logit_scale * neural_features[idx] @ selected_img_features.T\n",
    "                # logits_text = logit_scale * neural_features[idx] @ selected_text_features.T\n",
    "                # logits_single = (logits_text + logits_img) / 2.0\n",
    "                logits_single = logits_img\n",
    "                # print(\"logits_single\", logits_single.shape)\n",
    "\n",
    "                # predicted_label = selected_classes[torch.argmax(logits_single).item()]\n",
    "                predicted_label = selected_classes[torch.argmax(logits_single).item()] # (n_batch, ) \\in {0, 1, ..., n_cls-1}\n",
    "                if predicted_label == label.item():\n",
    "                    correct += 1        \n",
    "                if k==test_classes:\n",
    "                    _, top5_indices = torch.topk(logits_single, 5, largest =True)\n",
    "                                                            \n",
    "                    # Check if the ground truth label is among the top-5 predictions\n",
    "                    if label.item() in [selected_classes[i] for i in top5_indices.tolist()]:                \n",
    "                        top5_correct_count+=1                                 \n",
    "                total += 1              \n",
    "\n",
    "        if save_features:\n",
    "            features_tensor = torch.cat(features_list, dim=0)\n",
    "            print(\"features_tensor\", features_tensor.shape)\n",
    "            torch.save(features_tensor.cpu(), f\"ATM_S_neural_features_{sub}_train.pt\")  # Save features as .pt file\n",
    "    average_loss = total_loss / (batch_idx+1)\n",
    "    accuracy = correct / total    \n",
    "    top5_acc = top5_correct_count / total    \n",
    "    return average_loss, accuracy, top5_acc, labels, features_tensor.cpu()\n",
    "\n",
    "\n",
    "\n",
    "# Define Parameters\n",
    "encoder_paths_list = [\n",
    "    'eeg=/mnt/dataset1/ldy/Workspace/EEG_Image_decode/Retrieval/models/contrast/across/ATMS/01-06_01-46/150.pth',\n",
    "    'meg=/mnt/dataset1/ldy/Workspace/EEG_Image_decode/Retrieval/models/contrast/across/ATMS/01-11_14-50/150.pth',\n",
    "    'fmri=/mnt/dataset1/ldy/Workspace/EEG_Image_decode/Retrieval/models/contrast/across/ATMS/01-18_01-35/150.pth'\n",
    "]\n",
    "eval_modality = 'eeg'  # Modality to evaluate on\n",
    "\n",
    "# Subjects Configuration\n",
    "test_subjects = ['sub-01', 'sub-02', 'sub-03', 'sub-04', 'sub-05', 'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10']\n",
    "eeg_subjects = ['sub-01', 'sub-02', 'sub-03', 'sub-04', 'sub-05', 'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10']\n",
    "meg_subjects = ['sub-01', 'sub-02', 'sub-03', 'sub-04']\n",
    "fmri_subjects = ['sub-01', 'sub-02', 'sub-03']\n",
    "\n",
    "modalities = ['eeg', 'meg', 'fmri']  # Modalities to include in inference\n",
    "test_classes = 200\n",
    "# Update test_subjects and test_classes based on eval_modality\n",
    "if eval_modality == 'eeg':\n",
    "    test_subjects = eeg_subjects\n",
    "    test_classes = 200\n",
    "elif eval_modality == 'meg':\n",
    "    test_subjects = meg_subjects\n",
    "    test_classes = 200\n",
    "elif eval_modality == 'fmri':\n",
    "    test_subjects = fmri_subjects\n",
    "    test_classes = 100\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported modality: {eval_modality}\")\n",
    "\n",
    "# Example usage\n",
    "print(f\"Evaluation Modality: {eval_modality}\")\n",
    "print(f\"Test Subjects: {test_subjects}\")\n",
    "print(f\"Number of Test Classes: {test_classes}\")\n",
    "\n",
    "# Dataset Paths\n",
    "eeg_data_path = \"/mnt/dataset1/ldy/datasets/THINGS_EEG1/processed_250Hz\"\n",
    "meg_data_path = \"/home/ldy/THINGS-MEG/preprocessed_newsplit\"\n",
    "fmri_data_path = \"/home/ldy/fmri_dataset/Preprocessed\"\n",
    "\n",
    "# Output and Logging Configuration (Not needed for inference, but kept for completeness)\n",
    "output_dir = './outputs/contrast'\n",
    "project = \"train_pos_img_text_rep\"\n",
    "entity = \"sustech_rethinkingbci\"\n",
    "name = \"lr=3e-4_img_pos_pro_eeg\"\n",
    "\n",
    "# Inference Parameters\n",
    "device_preference = 'cuda:3'  # e.g., 'cuda:0' or 'cpu'\n",
    "device_type = 'gpu'  # 'cpu' or 'gpu'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n",
      "Total parameters: 161.96M\n",
      "Trainable parameters: 7.36M\n",
      "Trainable parameters percentage: 4.54%\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m test_subjects:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Prepare test dataset based on eval_modality and test_subjects\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_modality \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meeg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mEEGDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43meeg_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msub\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m eval_modality \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     65\u001b[0m         test_dataset \u001b[38;5;241m=\u001b[39m MEGDataset(meg_data_path, subjects\u001b[38;5;241m=\u001b[39m[sub], train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/dataset1/ldy/Workspace/FLORA/data_preparing/eegdatasets.py:110\u001b[0m, in \u001b[0;36mEEGDataset.__init__\u001b[0;34m(self, data_path, adap_subject, subjects, train, use_caption, time_window, classes, pictures)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meeg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# assert any subjects in subject_list\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28many\u001b[39m(sub \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubject_list \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubjects)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_caption \u001b[38;5;241m=\u001b[39m use_caption\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_data()\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process encoder_paths into a dictionary\n",
    "encoder_paths = {}\n",
    "for path in encoder_paths_list:\n",
    "    key, value = path.split('=')\n",
    "    encoder_paths[key] = value\n",
    "\n",
    "# Set device based on the argument\n",
    "device = torch.device(device_preference if device_type == 'gpu' and torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize empty datasets for each modality\n",
    "text_features_test_all = {}\n",
    "img_features_test_all = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "# Initialize the Unified Encoder Model\n",
    "unified_model = UnifiedEncoder(encoder_paths, device)\n",
    "unified_model.load_state_dict(torch.load(\n",
    "    \"/mnt/dataset1/ldy/Workspace/FLORA/models/contrast/across/Unified_EEG+MEG+fMRI_EEG/01-27_02-32/60.pth\",\n",
    "    map_location=device  # 使用当前设备\n",
    "))\n",
    "unified_model.to(device)\n",
    "unified_model.eval()  # Set model to evaluation mode\n",
    "\n",
    "\n",
    "# Print model parameters info\n",
    "def format_num(num):\n",
    "    for unit in ['','K','M','B','T']:\n",
    "        if num < 1000:\n",
    "            return f\"{num:.2f}{unit}\"\n",
    "        num /= 1000\n",
    "    return f\"{num:.2f}P\"\n",
    "\n",
    "total_params = sum(p.numel() for p in unified_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in unified_model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {format_num(total_params)}\")\n",
    "print(f\"Trainable parameters: {format_num(trainable_params)}\")\n",
    "\n",
    "if total_params > 0:\n",
    "    trainable_percentage = (trainable_params / total_params) * 100\n",
    "    print(f\"Trainable parameters percentage: {trainable_percentage:.2f}%\")\n",
    "else:\n",
    "    print(\"Total parameters count is zero, cannot compute percentage.\")\n",
    "#####################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "import numpy as np  # 导入numpy用于计算平均值\n",
    "test_accuracies = []\n",
    "test_accuracies_top5 = []\n",
    "v2_accuracies = []\n",
    "v4_accuracies = []\n",
    "v10_accuracies = []\n",
    "\n",
    "for sub in test_subjects:\n",
    "    # Prepare test dataset based on eval_modality and test_subjects\n",
    "    if eval_modality == 'eeg':\n",
    "        test_dataset = EEGDataset(eeg_data_path, subjects=[sub], train=False)\n",
    "    elif eval_modality == 'meg':\n",
    "        test_dataset = MEGDataset(meg_data_path, subjects=[sub], train=False)\n",
    "    elif eval_modality == 'fmri':\n",
    "        test_dataset = fMRIDataset(fmri_data_path, adap_subject=sub, subjects=[sub], train=False)\n",
    "    \n",
    "    # Collect test features\n",
    "    text_features_test_all[eval_modality] = test_dataset.text_features\n",
    "    img_features_test_all[eval_modality] = test_dataset.img_features\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, drop_last=False)\n",
    "    test_loss, test_accuracy, top5_acc, labels, eeg_features_test = get_eegfeatures(\n",
    "        unified_model, test_loader, device, text_features_test_all, img_features_test_all, k=test_classes, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )\n",
    "    _, v2_acc, _, _, _ = get_eegfeatures(\n",
    "        unified_model, test_loader, device, text_features_test_all, img_features_test_all, k=2, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )\n",
    "    _, v4_acc, _, _, _ = get_eegfeatures(\n",
    "        unified_model, test_loader, device, text_features_test_all, img_features_test_all, k=4, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )\n",
    "    _, v10_acc, _, _, _ = get_eegfeatures(\n",
    "        unified_model, test_loader, device, text_features_test_all, img_features_test_all, k=10, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )    \n",
    "    \n",
    "    test_accuracies.append(test_accuracy)\n",
    "    test_accuracies_top5.append(top5_acc)\n",
    "    v2_accuracies.append(v2_acc)\n",
    "    v4_accuracies.append(v4_acc)\n",
    "    v10_accuracies.append(v10_acc)\n",
    "    \n",
    "    print(f\" - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Top5 Accuracy: {top5_acc:.4f}\")    \n",
    "    print(f\" - Test Loss: {test_loss:.4f}, v2_acc Accuracy: {v2_acc:.4f}\")\n",
    "    print(f\" - Test Loss: {test_loss:.4f}, v4_acc Accuracy: {v4_acc:.4f}\")\n",
    "    print(f\" - Test Loss: {test_loss:.4f}, v10_acc Accuracy: {v10_acc:.4f}\")\n",
    "\n",
    "# 计算各项指标的平均准确率\n",
    "average_test_accuracy = np.mean(test_accuracies)\n",
    "average_test_accuracy_top5 = np.mean(test_accuracies_top5)\n",
    "average_v2_acc = np.mean(v2_accuracies)\n",
    "average_v4_acc = np.mean(v4_accuracies)\n",
    "average_v10_acc = np.mean(v10_accuracies)\n",
    "\n",
    "print(f\"\\nAverage Test Accuracy across all subjects: {average_test_accuracy:.4f}\")\n",
    "print(f\"\\nAverage Test Top5 Accuracy across all subjects: {average_test_accuracy_top5:.4f}\")\n",
    "print(f\"Average v2_acc Accuracy across all subjects: {average_v2_acc:.4f}\")\n",
    "print(f\"Average v4_acc Accuracy across all subjects: {average_v4_acc:.4f}\")\n",
    "print(f\"Average v10_acc Accuracy across all subjects: {average_v10_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
