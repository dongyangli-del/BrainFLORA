{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/local_attention/rotary.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/local_attention/rotary.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/colt5_attention/coor_descent.py:16: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/colt5_attention/topk.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "# 获取当前工作目录（假设 Notebook 位于 parent_dir）\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# 构建项目根目录的路径（假设 parent_dir 和 model 同级）\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "# 将项目根目录添加到 sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# 现在可以使用绝对导入\n",
    "from data_preparing.megdatasets_averaged import MEGDataset\n",
    "# 现在可以使用绝对导入\n",
    "from model.MEG_MedformerTS import meg_encoder\n",
    "from loss import ClipLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_id_from_string(s):\n",
    "    match = re.search(r'\\d+$', s)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "def get_megfeatures(sub, meg_model, dataloader, device, text_features_all, img_features_all, k, eval_modality, test_classes):\n",
    "    meg_model.eval()\n",
    "    text_features_all = text_features_all.to(device).float()\n",
    "    img_features_all = img_features_all[::12].to(device).float()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    top5_correct_count=0\n",
    "    total = 0\n",
    "    loss_func = ClipLoss() \n",
    "    all_labels = set(range(text_features_all.size(0)))\n",
    "    save_features = False\n",
    "    features_list = []  # List to store features    \n",
    "    features_tensor = torch.zeros(0, 0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (_, data, labels, text, text_features, img, img_features, index, img_index, subject_id) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            text_features = text_features.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            img_features = img_features.to(device).float()\n",
    "            \n",
    "            batch_size = data.size(0) \n",
    "            subject_id = extract_id_from_string(subject_id[0])\n",
    "            # data = data.permute(0, 2, 1)\n",
    "            subject_ids = torch.full((batch_size,), subject_id, dtype=torch.long).to(device)\n",
    "            neural_features = meg_model(data, subject_ids)\n",
    "            \n",
    "            logit_scale = meg_model.logit_scale.float()            \n",
    "            features_list.append(neural_features)\n",
    "               \n",
    "            img_loss = loss_func(neural_features, img_features, logit_scale)\n",
    "            loss = img_loss        \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            for idx, label in enumerate(labels):\n",
    "\n",
    "                possible_classes = list(all_labels - {label.item()})\n",
    "                selected_classes = random.sample(possible_classes, k-1) + [label.item()]\n",
    "                selected_img_features = img_features_all[selected_classes]\n",
    "                \n",
    "\n",
    "                logits_img = logit_scale * neural_features[idx] @ selected_img_features.T\n",
    "                # logits_text = logit_scale * neural_features[idx] @ selected_text_features.T\n",
    "                # logits_single = (logits_text + logits_img) / 2.0\n",
    "                logits_single = logits_img\n",
    "                # print(\"logits_single\", logits_single.shape)\n",
    "\n",
    "                # predicted_label = selected_classes[torch.argmax(logits_single).item()]\n",
    "                predicted_label = selected_classes[torch.argmax(logits_single).item()] # (n_batch, ) \\in {0, 1, ..., n_cls-1}\n",
    "                if predicted_label == label.item():\n",
    "                    correct += 1       \n",
    "                     \n",
    "                if k==test_classes:\n",
    "                    _, top5_indices = torch.topk(logits_single, 5, largest =True)\n",
    "                                                            \n",
    "                    # Check if the ground truth label is among the top-5 predictions\n",
    "                    if label.item() in [selected_classes[i] for i in top5_indices.tolist()]:                \n",
    "                        top5_correct_count+=1                                 \n",
    "                total += 1                    \n",
    "\n",
    "\n",
    "        if save_features:\n",
    "            features_tensor = torch.cat(features_list, dim=0)\n",
    "            print(\"features_tensor\", features_tensor.shape)\n",
    "            torch.save(features_tensor.cpu(), f\"ATM_S_neural_features_{sub}_train.pt\")  # Save features as .pt file\n",
    "    average_loss = total_loss / (batch_idx+1)\n",
    "    accuracy = correct / total    \n",
    "    top5_acc = top5_correct_count / total    \n",
    "    return average_loss, accuracy, top5_acc, labels, features_tensor.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2068106/2419663289.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  meg_model.load_state_dict(torch.load(\"/mnt/dataset0/ldy/Workspace/EEG_Image_decode/Retrieval/models/contrast/across/ATMS/01-11_14-50/150.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "meg_encoder(\n",
       "  (encoder): Medformer(\n",
       "    (enc_embedding): ListPatchEmbedding(\n",
       "      (value_embeddings): ModuleList(\n",
       "        (0): CrossChannelTokenEmbedding(\n",
       "          (tokenConv): Conv2d(1, 250, kernel_size=(271, 2), stride=(1, 2), bias=False, padding_mode=circular)\n",
       "        )\n",
       "        (1): CrossChannelTokenEmbedding(\n",
       "          (tokenConv): Conv2d(1, 250, kernel_size=(271, 4), stride=(1, 4), bias=False, padding_mode=circular)\n",
       "        )\n",
       "        (2): CrossChannelTokenEmbedding(\n",
       "          (tokenConv): Conv2d(1, 250, kernel_size=(271, 8), stride=(1, 8), bias=False, padding_mode=circular)\n",
       "        )\n",
       "      )\n",
       "      (position_embedding): PositionalEmbedding()\n",
       "      (dropout): Dropout(p=0.25, inplace=False)\n",
       "      (augmentation): ModuleList(\n",
       "        (0): Flip()\n",
       "        (1): Shuffle()\n",
       "        (2): FrequencyMask()\n",
       "        (3): Jitter()\n",
       "        (4): TemporalMask()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (learnable_embeddings): ParameterList(\n",
       "          (0): Parameter containing: [torch.float32 of size 1x250 (cuda:4)]\n",
       "          (1): Parameter containing: [torch.float32 of size 1x250 (cuda:4)]\n",
       "          (2): Parameter containing: [torch.float32 of size 1x250 (cuda:4)]\n",
       "      )\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (attn_layers): ModuleList(\n",
       "        (0): EncoderLayer(\n",
       "          (attention): MedformerLayer(\n",
       "            (intra_attentions): ModuleList(\n",
       "              (0-2): 3 x AttentionLayer(\n",
       "                (inner_attention): FullAttention(\n",
       "                  (dropout): Dropout(p=0.25, inplace=False)\n",
       "                )\n",
       "                (query_projection): Linear(in_features=250, out_features=248, bias=True)\n",
       "                (key_projection): Linear(in_features=250, out_features=248, bias=True)\n",
       "                (value_projection): Linear(in_features=250, out_features=248, bias=True)\n",
       "                (out_projection): Linear(in_features=248, out_features=250, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (inter_attention): AttentionLayer(\n",
       "              (inner_attention): FullAttention(\n",
       "                (dropout): Dropout(p=0.25, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=250, out_features=248, bias=True)\n",
       "              (key_projection): Linear(in_features=250, out_features=248, bias=True)\n",
       "              (value_projection): Linear(in_features=250, out_features=248, bias=True)\n",
       "              (out_projection): Linear(in_features=248, out_features=250, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (conv1): Conv1d(250, 256, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(256, 250, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.25, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (projection): Linear(in_features=44500, out_features=250, bias=True)\n",
       "  )\n",
       "  (subject_wise_linear): ModuleList(\n",
       "    (0-9): 10 x Linear(in_features=250, out_features=250, bias=True)\n",
       "  )\n",
       "  (enc_eeg): Enc_eeg(\n",
       "    (0): PatchEmbedding(\n",
       "      (tsconv): Sequential(\n",
       "        (0): Conv2d(1, 40, kernel_size=(1, 25), stride=(1, 1))\n",
       "        (1): AvgPool2d(kernel_size=(1, 51), stride=(1, 5), padding=0)\n",
       "        (2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ELU(alpha=1.0)\n",
       "        (4): Conv2d(40, 40, kernel_size=(178, 1), stride=(1, 1))\n",
       "        (5): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ELU(alpha=1.0)\n",
       "        (7): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (projection): Sequential(\n",
       "        (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Rearrange('b e (h) (w) -> b (h w) e')\n",
       "      )\n",
       "    )\n",
       "    (1): FlattenHead()\n",
       "  )\n",
       "  (proj_eeg): Proj_eeg(\n",
       "    (0): Linear(in_features=1440, out_features=1024, bias=True)\n",
       "    (1): ResidualAdd(\n",
       "      (fn): Sequential(\n",
       "        (0): GELU(approximate='none')\n",
       "        (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (loss_func): ClipLoss()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_subjects = ['sub-01', 'sub-02', 'sub-03', 'sub-04']\n",
    "# Inference Parameters\n",
    "device_preference = 'cuda:4'  # e.g., 'cuda:0' or 'cpu'\n",
    "device_type = 'gpu'  # 'cpu' or 'gpu'\n",
    "data_path = \"/home/ldy/THINGS-MEG/preprocessed_newsplit\"\n",
    "# Set device based on the argument\n",
    "device = torch.device(device_preference if device_type == 'gpu' and torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "test_classes = 200\n",
    "eval_modality = 'meg'  # Modality to evaluate on\n",
    "# encoder_path='/mnt/dataset0/ldy/Workspace/EEG_Image_decode/Retrieval/models/contrast/across/ATMS/01-11_16-25/150.pth'\n",
    "\n",
    "\n",
    "meg_model = meg_encoder()\n",
    "meg_model.load_state_dict(torch.load(\"/mnt/dataset0/ldy/Workspace/EEG_Image_decode/Retrieval/models/contrast/across/ATMS/01-11_14-50/150.pth\"))\n",
    "meg_model.to(device)\n",
    "meg_model.eval()  # Set model to evaluation mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.subjects ['sub-01', 'sub-02', 'sub-03', 'sub-04']\n",
      "adap_subject sub-01\n",
      "preprocessed_eeg_data torch.Size([2400, 271, 201])\n",
      "data_tensor torch.Size([200, 271, 201])\n",
      "Data tensor shape: torch.Size([200, 271, 201]), label tensor shape: torch.Size([200]), text length: 200, image length: 2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/dataset0/ldy/Workspace/FLORA/data_preparing/megdatasets_averaged.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_features = torch.load(features_filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Test Loss: 0.0000, Test Accuracy: 0.0650, Top5 Accuracy: 0.1450\n",
      " - Test Loss: 0.0000, v2_acc Accuracy: 0.8050\n",
      " - Test Loss: 0.0000, v4_acc Accuracy: 0.5750\n",
      " - Test Loss: 0.0000, v10_acc Accuracy: 0.3950\n",
      "self.subjects ['sub-01', 'sub-02', 'sub-03', 'sub-04']\n",
      "adap_subject sub-02\n",
      "preprocessed_eeg_data torch.Size([2400, 271, 201])\n",
      "data_tensor torch.Size([200, 271, 201])\n",
      "Data tensor shape: torch.Size([200, 271, 201]), label tensor shape: torch.Size([200]), text length: 200, image length: 2400\n",
      " - Test Loss: 0.0000, Test Accuracy: 0.1350, Top5 Accuracy: 0.4000\n",
      " - Test Loss: 0.0000, v2_acc Accuracy: 0.8800\n",
      " - Test Loss: 0.0000, v4_acc Accuracy: 0.7750\n",
      " - Test Loss: 0.0000, v10_acc Accuracy: 0.6050\n",
      "self.subjects ['sub-01', 'sub-02', 'sub-03', 'sub-04']\n",
      "adap_subject sub-03\n",
      "preprocessed_eeg_data torch.Size([2400, 271, 201])\n",
      "data_tensor torch.Size([200, 271, 201])\n",
      "Data tensor shape: torch.Size([200, 271, 201]), label tensor shape: torch.Size([200]), text length: 200, image length: 2400\n",
      " - Test Loss: 0.0000, Test Accuracy: 0.0750, Top5 Accuracy: 0.2500\n",
      " - Test Loss: 0.0000, v2_acc Accuracy: 0.8250\n",
      " - Test Loss: 0.0000, v4_acc Accuracy: 0.6650\n",
      " - Test Loss: 0.0000, v10_acc Accuracy: 0.5200\n",
      "self.subjects ['sub-01', 'sub-02', 'sub-03', 'sub-04']\n",
      "adap_subject sub-04\n",
      "preprocessed_eeg_data torch.Size([2400, 271, 201])\n",
      "data_tensor torch.Size([200, 271, 201])\n",
      "Data tensor shape: torch.Size([200, 271, 201]), label tensor shape: torch.Size([200]), text length: 200, image length: 2400\n",
      " - Test Loss: 0.0000, Test Accuracy: 0.0450, Top5 Accuracy: 0.1800\n",
      " - Test Loss: 0.0000, v2_acc Accuracy: 0.7600\n",
      " - Test Loss: 0.0000, v4_acc Accuracy: 0.5650\n",
      " - Test Loss: 0.0000, v10_acc Accuracy: 0.3450\n",
      "\n",
      "Average Test Accuracy across all subjects: 0.0800\n",
      "\n",
      "Average Test Top5 Accuracy across all subjects: 0.2438\n",
      "Average v2_acc Accuracy across all subjects: 0.8175\n",
      "Average v4_acc Accuracy across all subjects: 0.6450\n",
      "Average v10_acc Accuracy across all subjects: 0.4662\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################\n",
    "import numpy as np  # 导入numpy用于计算平均值\n",
    "test_accuracies = []\n",
    "test_accuracies_top5 = []\n",
    "v2_accuracies = []\n",
    "v4_accuracies = []\n",
    "v10_accuracies = []\n",
    "\n",
    "for sub in test_subjects:\n",
    "    test_dataset = MEGDataset(data_path, adap_subject=sub, subjects=test_subjects, train=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, drop_last=False)\n",
    "    \n",
    "    text_features_test_all = test_dataset.text_features    \n",
    "    img_features_test_all = test_dataset.img_features\n",
    "    \n",
    "    test_loss, test_accuracy, top5_acc, labels, meg_features_test = get_megfeatures(\n",
    "        sub, meg_model, test_loader, device, text_features_test_all, img_features_test_all, k=test_classes, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )\n",
    "    _, v2_acc, _, _, _ = get_megfeatures(\n",
    "        sub, meg_model, test_loader, device, text_features_test_all, img_features_test_all, k=2, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )\n",
    "    _, v4_acc, _, _, _ = get_megfeatures(\n",
    "        sub, meg_model, test_loader, device, text_features_test_all, img_features_test_all, k=4, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )\n",
    "    _, v10_acc, _, _, _ = get_megfeatures(\n",
    "        sub, meg_model, test_loader, device, text_features_test_all, img_features_test_all, k=10, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )    \n",
    "    \n",
    "    test_accuracies.append(test_accuracy)\n",
    "    test_accuracies_top5.append(top5_acc)\n",
    "    v2_accuracies.append(v2_acc)\n",
    "    v4_accuracies.append(v4_acc)\n",
    "    v10_accuracies.append(v10_acc)\n",
    "    \n",
    "    print(f\" - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Top5 Accuracy: {top5_acc:.4f}\")    \n",
    "    print(f\" - Test Loss: {test_loss:.4f}, v2_acc Accuracy: {v2_acc:.4f}\")\n",
    "    print(f\" - Test Loss: {test_loss:.4f}, v4_acc Accuracy: {v4_acc:.4f}\")\n",
    "    print(f\" - Test Loss: {test_loss:.4f}, v10_acc Accuracy: {v10_acc:.4f}\")\n",
    "\n",
    "# 计算各项指标的平均准确率\n",
    "average_test_accuracy = np.mean(test_accuracies)\n",
    "average_test_accuracy_top5 = np.mean(test_accuracies_top5)\n",
    "average_v2_acc = np.mean(v2_accuracies)\n",
    "average_v4_acc = np.mean(v4_accuracies)\n",
    "average_v10_acc = np.mean(v10_accuracies)\n",
    "\n",
    "print(f\"\\nAverage Test Accuracy across all subjects: {average_test_accuracy:.4f}\")\n",
    "print(f\"\\nAverage Test Top5 Accuracy across all subjects: {average_test_accuracy_top5:.4f}\")\n",
    "print(f\"Average v2_acc Accuracy across all subjects: {average_v2_acc:.4f}\")\n",
    "print(f\"Average v4_acc Accuracy across all subjects: {average_v4_acc:.4f}\")\n",
    "print(f\"Average v10_acc Accuracy across all subjects: {average_v10_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
