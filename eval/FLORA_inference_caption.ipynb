{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ[\"WANDB_API_KEY\"] = \"KEY\"\n",
    "os.environ[\"WANDB_MODE\"] = 'offline'\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "# 获取当前工作目录（假设 Notebook 位于 parent_dir）\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# 构建项目根目录的路径（假设 parent_dir 和 model 同级）\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "# 将项目根目录添加到 sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# 现在可以使用绝对导入\n",
    "from model.unified_encoder_multi_tower import UnifiedEncoder\n",
    "\n",
    "# 导入必要的库\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import wandb\n",
    "wandb.init(mode=\"disabled\")\n",
    "\n",
    "from data_preparing.eegdatasets import EEGDataset\n",
    "from data_preparing.megdatasets_averaged import MEGDataset\n",
    "from data_preparing.fmri_datasets_joint_subjects import fMRIDataset\n",
    "from data_preparing.datasets_mixer import MetaEEGDataset, MetaMEGDataset, MetafMRIDataset, MetaDataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from loss import ClipLoss\n",
    "from model.diffusion_prior import Pipe, EmbeddingDataset, DiffusionPriorUNet\n",
    "from model.custom_pipeline import Generator4Embeds\n",
    "# 忽略警告\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# proxy = 'http://10.20.37.38:7890'\n",
    "# os.environ['http_proxy'] = proxy\n",
    "# os.environ['https_proxy'] = proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Modality: fmri\n",
      "Test Subjects: ['sub-01']\n",
      "Number of Test Classes: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_id_from_string(s):\n",
    "    match = re.search(r'\\d+$', s)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "def get_eegfeatures(unified_model, dataloader, device, text_features_all, img_features_all, k, eval_modality, test_classes):\n",
    "    unified_model.eval()\n",
    "    text_features_all = text_features_all[eval_modality].to(device).float()\n",
    "    if eval_modality=='eeg' or eval_modality=='fmri':\n",
    "        img_features_all = (img_features_all[eval_modality]).to(device).float()\n",
    "    elif eval_modality=='meg':\n",
    "        img_features_all = (img_features_all[eval_modality][::12]).to(device).float()  \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    top5_correct_count=0\n",
    "    total = 0\n",
    "    loss_func = ClipLoss() \n",
    "    all_labels = set(range(text_features_all.size(0)))\n",
    "    save_features = True\n",
    "    features_list = []  # List to store features    \n",
    "    features_tensor = torch.zeros(0, 0)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (modal, data, labels, text, text_features, img, img_features, _, _, sub_ids) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            text_features = text_features.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            img_features = img_features.to(device).float()\n",
    "            \n",
    "            batch_size = data.size(0) \n",
    "            subject_ids = [extract_id_from_string(sub_id) for sub_id in sub_ids]\n",
    "            subject_ids = torch.tensor(subject_ids, dtype=torch.long).to(device)\n",
    "            ret_emb, neural_features = unified_model(data, subject_ids, modal=eval_modality)\n",
    "            \n",
    "            logit_scale = unified_model.logit_scale.float()            \n",
    "            features_list.append(neural_features)\n",
    "               \n",
    "            img_loss = loss_func(ret_emb, img_features, logit_scale)\n",
    "            loss = img_loss        \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            for idx, label in enumerate(labels):\n",
    "\n",
    "                possible_classes = list(all_labels - {label.item()})\n",
    "                selected_classes = random.sample(possible_classes, k-1) + [label.item()]\n",
    "                selected_img_features = img_features_all[selected_classes]\n",
    "                \n",
    "\n",
    "                logits_img = logit_scale * ret_emb[idx] @ selected_img_features.T\n",
    "                # logits_text = logit_scale * ret_emb[idx] @ selected_text_features.T\n",
    "                # logits_single = (logits_text + logits_img) / 2.0\n",
    "                logits_single = logits_img\n",
    "                # print(\"logits_single\", logits_single.shape)\n",
    "\n",
    "                # predicted_label = selected_classes[torch.argmax(logits_single).item()]\n",
    "                predicted_label = selected_classes[torch.argmax(logits_single).item()] # (n_batch, ) \\in {0, 1, ..., n_cls-1}\n",
    "                if predicted_label == label.item():\n",
    "                    correct += 1        \n",
    "                if k==test_classes:\n",
    "                    _, top5_indices = torch.topk(logits_single, 5, largest =True)\n",
    "                                                            \n",
    "                    # Check if the ground truth label is among the top-5 predictions\n",
    "                    if label.item() in [selected_classes[i] for i in top5_indices.tolist()]:                \n",
    "                        top5_correct_count+=1                                 \n",
    "                total += 1              \n",
    "        if save_features:\n",
    "            features_tensor = torch.cat(features_list, dim=0)\n",
    "            print(\"features_tensor\", features_tensor.shape)\n",
    "            torch.save(features_tensor.cpu(), f\"test.pt\")  # Save features as .pt file\n",
    "\n",
    "    average_loss = total_loss / (batch_idx+1)\n",
    "    accuracy = correct / total    \n",
    "    top5_acc = top5_correct_count / total    \n",
    "    return average_loss, accuracy, top5_acc, labels, features_tensor.cpu()\n",
    "\n",
    "\n",
    "\n",
    "# Define Parameters\n",
    "encoder_paths_list = [\n",
    "    'eeg=/mnt/dataset1/ldy/Workspace/EEG_Image_decode/Retrieval/models/contrast/across/ATMS/01-06_01-46/150.pth',\n",
    "    'meg=/mnt/dataset1/ldy/Workspace/EEG_Image_decode/Retrieval/models/contrast/across/ATMS/01-11_14-50/150.pth',\n",
    "    'fmri=/mnt/dataset0/ldy/Workspace/EEG_Image_decode/Retrieval/models/contrast/across/ATMS/01-18_01-35/50.pth'\n",
    "]\n",
    "eval_modality = 'fmri'  # Modality to evaluate on\n",
    "\n",
    "# Subjects Configuration\n",
    "# test_subjects = ['sub-01', 'sub-02', 'sub-03', 'sub-04', 'sub-05', 'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10']\n",
    "test_subjects = ['sub-01']\n",
    "eeg_subjects = ['sub-01', 'sub-02', 'sub-03', 'sub-04', 'sub-05', 'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10']\n",
    "meg_subjects = ['sub-01', 'sub-02', 'sub-03', 'sub-04']\n",
    "fmri_subjects = ['sub-01', 'sub-02', 'sub-03']\n",
    "\n",
    "modalities = ['eeg', 'meg', 'fmri']  # Modalities to include in inference\n",
    "test_classes = 100\n",
    "# Update test_subjects and test_classes based on eval_modality\n",
    "# if eval_modality == 'eeg':\n",
    "#     test_subjects = eeg_subjects\n",
    "#     test_classes = 200\n",
    "# elif eval_modality == 'meg':\n",
    "#     test_subjects = meg_subjects\n",
    "#     test_classes = 200\n",
    "# elif eval_modality == 'fmri':\n",
    "#     test_subjects = fmri_subjects\n",
    "#     test_classes = 100\n",
    "# else:\n",
    "#     raise ValueError(f\"Unsupported modality: {eval_modality}\")\n",
    "\n",
    "# Example usage\n",
    "print(f\"Evaluation Modality: {eval_modality}\")\n",
    "print(f\"Test Subjects: {test_subjects}\")\n",
    "print(f\"Number of Test Classes: {test_classes}\")\n",
    "\n",
    "# Dataset Paths\n",
    "# eeg_data_path = \"/home/ldy/4090_Workspace/4090_THINGS/Preprocessed_data_250Hz\"\n",
    "# meg_data_path = \"/home/ldy/THINGS-MEG/preprocessed_newsplit\"\n",
    "# fmri_data_path = \"/home/ldy/fmri_dataset/Preprocessed\"\n",
    "\n",
    "# parser.add_argument('--eeg_data_path', type=str, default=\"/mnt/dataset0/ldy/datasets/THINGS_EEG/Preprocessed_data_250Hz\", help='Path to the EEG dataset')\n",
    "# parser.add_argument('--meg_data_path', type=str, default=\"/mnt/dataset0/ldy/datasets/THINGS_MEG/preprocessed_newsplit\", help='Path to the MEG dataset')    \n",
    "# parser.add_argument('--fmri_data_path', type=str, default=\"/mnt/dataset0/ldy/datasets/fmri_dataset/Preprocessed\", help='Path to the fMRI dataset')     \n",
    "\n",
    "eeg_data_path = \"/mnt/dataset0/ldy/datasets/THINGS_EEG/Preprocessed_data_250Hz\"\n",
    "meg_data_path = \"/mnt/dataset0/ldy/datasets/THINGS_MEG/preprocessed_newsplit\"\n",
    "fmri_data_path = \"/mnt/dataset0/ldy/datasets/fmri_dataset/Preprocessed\"\n",
    "\n",
    "# Output and Logging Configuration (Not needed for inference, but kept for completeness)\n",
    "output_dir = './outputs/contrast'\n",
    "project = \"train_pos_img_text_rep\"\n",
    "entity = \"sustech_rethinkingbci\"\n",
    "name = \"lr=3e-4_img_pos_pro_eeg\"\n",
    "\n",
    "# Inference Parameters\n",
    "device_preference = 'cuda'  # e.g., 'cuda:0' or 'cpu'\n",
    "device_type = 'gpu'  # 'cpu' or 'gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total parameters: 297.50M\n",
      "Trainable parameters: 142.89M\n",
      "Trainable parameters percentage: 48.03%\n",
      "Data list length: 100, label list length: 100, text length: 100, image length: 100\n",
      "features_tensor torch.Size([100, 256, 1024])\n",
      "features_tensor torch.Size([100, 256, 1024])\n",
      "features_tensor torch.Size([100, 256, 1024])\n",
      "features_tensor torch.Size([100, 256, 1024])\n",
      " - Test Loss: 0.0000, Test Accuracy: 0.0000, Top5 Accuracy: 0.0500\n",
      " - Test Loss: 0.0000, v2_acc Accuracy: 0.5300\n",
      " - Test Loss: 0.0000, v4_acc Accuracy: 0.2900\n",
      " - Test Loss: 0.0000, v10_acc Accuracy: 0.1100\n",
      "\n",
      "Average Test Accuracy across all subjects: 0.0000\n",
      "\n",
      "Average Test Top5 Accuracy across all subjects: 0.0500\n",
      "Average v2_acc Accuracy across all subjects: 0.5300\n",
      "Average v4_acc Accuracy across all subjects: 0.2900\n",
      "Average v10_acc Accuracy across all subjects: 0.1100\n"
     ]
    }
   ],
   "source": [
    "# Process encoder_paths into a dictionary\n",
    "encoder_paths = {}\n",
    "for path in encoder_paths_list:\n",
    "    key, value = path.split('=')\n",
    "    encoder_paths[key] = value\n",
    "\n",
    "# Set device based on the argument\n",
    "device = torch.device(device_preference if device_type == 'gpu' and torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize empty datasets for each modality\n",
    "text_features_test_all = {}\n",
    "img_features_test_all = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "# Initialize the Unified Encoder Model\n",
    "unified_model = UnifiedEncoder(encoder_paths, device, user_caption=True)\n",
    "unified_model.load_state_dict(torch.load(\"/mnt/dataset1/ldy/Workspace/FLORA/models/contrast/across/Unified_EEG+MEG+fMRI_EEG/01-29_14-48/90.pth\"))\n",
    "unified_model.to(device)\n",
    "unified_model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# diffusion_prior = DiffusionPriorUNet(cond_dim=1024, dropout=0.1)\n",
    "# high_pipe = Pipe(diffusion_prior, device=device)\n",
    "# high_pipe.diffusion_prior.load_state_dict(torch.load(\"/mnt/dataset0/ldy/models/contrast/across/Unified_EEG+MEG+fMRI_EEG/01-22_18-16/prior_diffusion/60.pth\"))\n",
    "# high_pipe.diffusion_prior.to(device)\n",
    "# high_pipe.diffusion_prior.eval()  # Set model to evaluation mode\n",
    "\n",
    "\n",
    "# Print model parameters info\n",
    "def format_num(num):\n",
    "    for unit in ['','K','M','B','T']:\n",
    "        if num < 1000:\n",
    "            return f\"{num:.2f}{unit}\"\n",
    "        num /= 1000\n",
    "    return f\"{num:.2f}P\"\n",
    "\n",
    "total_params = sum(p.numel() for p in unified_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in unified_model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {format_num(total_params)}\")\n",
    "print(f\"Trainable parameters: {format_num(trainable_params)}\")\n",
    "\n",
    "if total_params > 0:\n",
    "    trainable_percentage = (trainable_params / total_params) * 100\n",
    "    print(f\"Trainable parameters percentage: {trainable_percentage:.2f}%\")\n",
    "else:\n",
    "    print(\"Total parameters count is zero, cannot compute percentage.\")\n",
    "#####################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "import numpy as np  # 导入numpy用于计算平均值\n",
    "test_accuracies = []\n",
    "test_accuracies_top5 = []\n",
    "v2_accuracies = []\n",
    "v4_accuracies = []\n",
    "v10_accuracies = []\n",
    "\n",
    "for sub in test_subjects:\n",
    "    # Prepare test dataset based on eval_modality and test_subjects\n",
    "    if eval_modality == 'eeg':\n",
    "        test_dataset = EEGDataset(eeg_data_path, subjects=[sub], train=False)\n",
    "    elif eval_modality == 'meg':\n",
    "        test_dataset = MEGDataset(meg_data_path, subjects=[sub], train=False)\n",
    "    elif eval_modality == 'fmri':\n",
    "        test_dataset = fMRIDataset(fmri_data_path, adap_subject=sub, subjects=[sub], train=False)\n",
    "    \n",
    "    # Collect test features\n",
    "    text_features_test_all[eval_modality] = test_dataset.text_features\n",
    "    img_features_test_all[eval_modality] = test_dataset.img_features\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, drop_last=False)\n",
    "    test_loss, test_accuracy, top5_acc, labels, eeg_features_test = get_eegfeatures(\n",
    "        unified_model, test_loader, device, text_features_test_all, img_features_test_all, k=test_classes, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )\n",
    "    _, v2_acc, _, _, _ = get_eegfeatures(\n",
    "        unified_model, test_loader, device, text_features_test_all, img_features_test_all, k=2, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )\n",
    "    _, v4_acc, _, _, _ = get_eegfeatures(\n",
    "        unified_model, test_loader, device, text_features_test_all, img_features_test_all, k=4, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )\n",
    "    _, v10_acc, _, _, _ = get_eegfeatures(\n",
    "        unified_model, test_loader, device, text_features_test_all, img_features_test_all, k=10, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )    \n",
    "    \n",
    "    test_accuracies.append(test_accuracy)\n",
    "    test_accuracies_top5.append(top5_acc)\n",
    "    v2_accuracies.append(v2_acc)\n",
    "    v4_accuracies.append(v4_acc)\n",
    "    v10_accuracies.append(v10_acc)\n",
    "    \n",
    "    print(f\" - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Top5 Accuracy: {top5_acc:.4f}\")    \n",
    "    print(f\" - Test Loss: {test_loss:.4f}, v2_acc Accuracy: {v2_acc:.4f}\")\n",
    "    print(f\" - Test Loss: {test_loss:.4f}, v4_acc Accuracy: {v4_acc:.4f}\")\n",
    "    print(f\" - Test Loss: {test_loss:.4f}, v10_acc Accuracy: {v10_acc:.4f}\")\n",
    "\n",
    "# 计算各项指标的平均准确率\n",
    "average_test_accuracy = np.mean(test_accuracies)\n",
    "average_test_accuracy_top5 = np.mean(test_accuracies_top5)\n",
    "average_v2_acc = np.mean(v2_accuracies)\n",
    "average_v4_acc = np.mean(v4_accuracies)\n",
    "average_v10_acc = np.mean(v10_accuracies)\n",
    "\n",
    "print(f\"\\nAverage Test Accuracy across all subjects: {average_test_accuracy:.4f}\")\n",
    "print(f\"\\nAverage Test Top5 Accuracy across all subjects: {average_test_accuracy_top5:.4f}\")\n",
    "print(f\"Average v2_acc Accuracy across all subjects: {average_v2_acc:.4f}\")\n",
    "print(f\"Average v4_acc Accuracy across all subjects: {average_v4_acc:.4f}\")\n",
    "print(f\"Average v10_acc Accuracy across all subjects: {average_v10_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BrainDiffusionPrior(\n",
       "  (noise_scheduler): NoiseScheduler()\n",
       "  (net): PriorNetwork(\n",
       "    (to_time_embeds): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): SinusoidalPosEmb()\n",
       "        (1): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "              (1): SiLU()\n",
       "              (2): Identity()\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              (1): SiLU()\n",
       "              (2): Identity()\n",
       "            )\n",
       "            (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Rearrange('b (n d) -> b n d', n=1)\n",
       "    )\n",
       "    (causal_transformer): FlaggedCausalTransformer(\n",
       "      (init_norm): Identity()\n",
       "      (rel_pos_bias): RelPosBias(\n",
       "        (relative_attention_bias): Embedding(32, 256)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (to_kv): Linear(in_features=1024, out_features=8, bias=False)\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=1024, out_features=8192, bias=False)\n",
       "            (2): SwiGLU()\n",
       "            (3): Identity()\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "            (5): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm()\n",
       "      (project_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.diffusion_prior_caption import Pipe, EmbeddingDataset, DiffusionPriorUNet, PriorNetwork, BrainDiffusionPrior\n",
    "# setup diffusion prior network\n",
    "clip_emb_dim = 1024\n",
    "clip_seq_dim = 256\n",
    "depth = 1\n",
    "dim_head = 4\n",
    "heads = clip_emb_dim//4 # heads * dim_head = clip_emb_dim\n",
    "timesteps = 100\n",
    "out_dim = clip_emb_dim\n",
    "\n",
    "prior_network = PriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        num_tokens = clip_seq_dim,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    )\n",
    "\n",
    "high_pipe = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    ")\n",
    "high_pipe.to(device)\n",
    "high_pipe.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 256, 1024])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_features_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 19/19 [00:09<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "eeg_features_test = eeg_features_test.to(device)\n",
    "prior_out = high_pipe.p_sample_loop(eeg_features_test.shape, \n",
    "                text_cond = dict(text_embed = eeg_features_test), \n",
    "                cond_scale = 1., timesteps = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 256, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load mm_projector\n",
    "mm_projector = torch.nn.Linear(1024, 4096)\n",
    "mm_projector_weights = torch.load('/mnt/dataset1/ldy/Workspace/UMBRAE/brainx/model_weights/mm_projector.bin', map_location='cpu')\n",
    "mm_projector.load_state_dict({k.split('.')[-1]: v for k, v in mm_projector_weights.items()})\n",
    "mm_projector.to(torch.bfloat16).to(device)\n",
    "\n",
    "emb_img_test = mm_projector(prior_out.to(torch.bfloat16).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER:\"\n",
    "user_image = \" <im_start>\" + \"<im_patch>\" * 256 + \"<im_end> \"\n",
    "prompt = 'Describe this image <image> as simply as possible.'\n",
    "if '<image>' in prompt:\n",
    "    user_prompt = prompt.replace('<image>', user_image)\n",
    "else:\n",
    "    user_prompt = prompt + user_image\n",
    "input_text = system + user_prompt + \" ASSISTANT:\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nLlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# load llama with the fine-tuned shikra model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m finetuned_llama \u001b[38;5;241m=\u001b[39m shikra_path \u001b[38;5;66;03m# 'model_weights/shikra-7b' # shikra\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(finetuned_llama, padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(finetuned_llama, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/transformers/utils/import_utils.py:1651\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1651\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/transformers/utils/import_utils.py:1639\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1637\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nLlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import time\n",
    "shikra_path = '/mnt/dataset1/ldy/Workspace/UMBRAE/brainx/model_weights/shikra-7b'\n",
    "save_path= \"outputs/shikra_captions.txt\"\n",
    "# load llama with the fine-tuned shikra model\n",
    "\n",
    "finetuned_llama = shikra_path # 'model_weights/shikra-7b' # shikra\n",
    "tokenizer = LlamaTokenizer.from_pretrained(finetuned_llama, padding_side='left')\n",
    "model = LlamaForCausalLM.from_pretrained(finetuned_llama, torch_dtype=torch.bfloat16)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emb_img_test.device)\n",
    "\n",
    "emb_img_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)#.cuda()\n",
    "inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    use_cache=True,\n",
    "    do_sample=False,\n",
    "    pad_token_id=2, # tokenizer.pad_token_id,\n",
    "    bos_token_id=1, # tokenizer.bos_token_id,\n",
    "    eos_token_id=2, # tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "# os.makedirs(save_path, exist_ok=True)\n",
    "with open(save_path, 'w') as f:\n",
    "    cap_result = {}\n",
    "    for cur_image_idx in range(emb_img_test.shape[0]):\n",
    "        new_input_embeds = []\n",
    "        for cur_input_ids, cur_input_embeds in zip(input_ids, inputs_embeds):\n",
    "            cur_image_features = emb_img_test[cur_image_idx]\n",
    "            num_patches = cur_image_features.shape[0]\n",
    "            image_start_tokens = torch.where(cur_input_ids == 32001)[0]\n",
    "            \n",
    "            # Ensure there is enough space for the image features\n",
    "            for image_start_token_pos in image_start_tokens:\n",
    "                if image_start_token_pos + num_patches + 1 >= cur_input_ids.size(0):\n",
    "                    raise ValueError(\"The input sequence is too short to accommodate the image features.\")\n",
    "                \n",
    "                if cur_input_ids[image_start_token_pos + num_patches + 1] != 32002:\n",
    "                    raise ValueError(\"The image end token should follow the image start token.\")\n",
    "                \n",
    "                cur_new_input_embeds = torch.cat((cur_input_embeds[:image_start_token_pos + 1], cur_image_features,\n",
    "                                                cur_input_embeds[image_start_token_pos + num_patches + 1:]), dim=0)\n",
    "            new_input_embeds.append(cur_new_input_embeds)\n",
    "        inputs_embeds = torch.stack(new_input_embeds, dim=0)\n",
    "\n",
    "        st_time = time.time()\n",
    "        with torch.inference_mode():\n",
    "            with torch.autocast(dtype=torch.bfloat16, device_type='cuda'):\n",
    "                output_ids = model.generate(inputs_embeds=inputs_embeds.bfloat16(), **gen_kwargs)\n",
    "        # print(f\"done generated in {time.time() - st_time} seconds\")\n",
    "\n",
    "        response = tokenizer.batch_decode(output_ids)[0]\n",
    "        print(f\"response: {response.strip(' <s></s>')}\")\n",
    "        # 将结果按行写入文件\n",
    "    \n",
    "        f.write(response.strip(' <s></s>')+'\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meg2speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
