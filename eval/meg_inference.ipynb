{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "# 获取当前工作目录（假设 Notebook 位于 parent_dir）\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# 构建项目根目录的路径（假设 parent_dir 和 model 同级）\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "# 将项目根目录添加到 sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "sys.path.insert(0,'/mnt/dataset1/ldy/Workspace/EEG_Image_decode_Wrong/Retrieval')\n",
    "sys.path.insert(0,'/mnt/dataset1/ldy/Workspace/EEG_Image_decode/Retrieval')\n",
    "# from contrast_retrieval_MEG import EEGNetv4_Encoder, MetaEEG, NICE\n",
    "\n",
    "sys.path.insert(0,'/mnt/dataset1/ldy/Workspace/EEG_Image_decode/Retrieval/model')\n",
    "from umbrae import BrainXS_thingsmeg\n",
    "\n",
    "# 现在可以使用绝对导入\n",
    "from data_preparing.megdatasets import MEGDataset\n",
    "# 现在可以使用绝对导入\n",
    "# from model.MEG_MedformerTS import meg_encoder\n",
    "from loss import ClipLoss\n",
    "from ATMS_retrieval_joint_and_in_train_MEG import ATMS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_id_from_string(s):\n",
    "    match = re.search(r'\\d+$', s)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "def get_megfeatures(sub, meg_model, dataloader, device, text_features_all, img_features_all, k, eval_modality, test_classes):\n",
    "    meg_model.eval()\n",
    "    text_features_all = text_features_all.to(device).float()\n",
    "    img_features_all = img_features_all[::12].to(device).float()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    top5_correct_count=0\n",
    "    total = 0\n",
    "    loss_func = ClipLoss() \n",
    "    all_labels = set(range(text_features_all.size(0)))\n",
    "    save_features = False\n",
    "    features_list = []  # List to store features    \n",
    "    features_tensor = torch.zeros(0, 0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (_, data, labels, text, text_features, img, img_features, index, img_index, subject_id) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            text_features = text_features.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            img_features = img_features.to(device).float()\n",
    "            \n",
    "            batch_size = data.size(0) \n",
    "            subject_id = extract_id_from_string(subject_id[0])\n",
    "            # data = data.permute(0, 2, 1)\n",
    "            subject_ids = torch.full((batch_size,), subject_id, dtype=torch.long).to(device)\n",
    "            # neural_features = meg_model(data, subject_ids)\n",
    "            neural_features = meg_model(data)\n",
    "            \n",
    "            logit_scale = meg_model.logit_scale.float()            \n",
    "            features_list.append(neural_features)\n",
    "               \n",
    "            img_loss = loss_func(neural_features, img_features, logit_scale)\n",
    "            loss = img_loss        \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            for idx, label in enumerate(labels):\n",
    "\n",
    "                possible_classes = list(all_labels - {label.item()})\n",
    "                selected_classes = random.sample(possible_classes, k-1) + [label.item()]\n",
    "                selected_img_features = img_features_all[selected_classes]\n",
    "                \n",
    "\n",
    "                logits_img = logit_scale * neural_features[idx] @ selected_img_features.T\n",
    "                # logits_text = logit_scale * neural_features[idx] @ selected_text_features.T\n",
    "                # logits_single = (logits_text + logits_img) / 2.0\n",
    "                logits_single = logits_img\n",
    "                # print(\"logits_single\", logits_single.shape)\n",
    "\n",
    "                # predicted_label = selected_classes[torch.argmax(logits_single).item()]\n",
    "                predicted_label = selected_classes[torch.argmax(logits_single).item()] # (n_batch, ) \\in {0, 1, ..., n_cls-1}\n",
    "                if predicted_label == label.item():\n",
    "                    correct += 1       \n",
    "                     \n",
    "                if k==test_classes:\n",
    "                    _, top5_indices = torch.topk(logits_single, 5, largest =True)\n",
    "                                                            \n",
    "                    # Check if the ground truth label is among the top-5 predictions\n",
    "                    if label.item() in [selected_classes[i] for i in top5_indices.tolist()]:                \n",
    "                        top5_correct_count+=1                                 \n",
    "                total += 1                    \n",
    "\n",
    "\n",
    "        if save_features:\n",
    "            features_tensor = torch.cat(features_list, dim=0)\n",
    "            print(\"features_tensor\", features_tensor.shape)\n",
    "            torch.save(features_tensor.cpu(), f\"ATM_S_neural_features_{sub}_train.pt\")  # Save features as .pt file\n",
    "    average_loss = total_loss / (batch_idx+1)\n",
    "    accuracy = correct / total    \n",
    "    top5_acc = top5_correct_count / total    \n",
    "    return average_loss, accuracy, top5_acc, labels, features_tensor.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ATMS(\n",
       "  (encoder): iTransformer(\n",
       "    (enc_embedding): DataEmbedding(\n",
       "      (value_embedding): Linear(in_features=201, out_features=250, bias=True)\n",
       "      (position_embedding): PositionalEmbedding()\n",
       "      (temporal_embedding): TimeFeatureEmbedding(\n",
       "        (embed): Linear(in_features=4, out_features=250, bias=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.25, inplace=False)\n",
       "      (subject_embedding): SubjectEmbedding(\n",
       "        (subject_embedding): Embedding(10, 250)\n",
       "      )\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (attn_layers): ModuleList(\n",
       "        (0): EncoderLayer(\n",
       "          (attention): AttentionLayer(\n",
       "            (inner_attention): FullAttention(\n",
       "              (dropout): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "            (query_projection): Linear(in_features=250, out_features=248, bias=True)\n",
       "            (key_projection): Linear(in_features=250, out_features=248, bias=True)\n",
       "            (value_projection): Linear(in_features=250, out_features=248, bias=True)\n",
       "            (out_projection): Linear(in_features=248, out_features=250, bias=True)\n",
       "          )\n",
       "          (conv1): Conv1d(250, 256, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(256, 250, kernel_size=(1,), stride=(1,))\n",
       "          (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.25, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (subject_wise_linear): ModuleList(\n",
       "    (0-9): 10 x Linear(in_features=250, out_features=201, bias=True)\n",
       "  )\n",
       "  (enc_eeg): Enc_eeg(\n",
       "    (0): PatchEmbedding(\n",
       "      (tsconv): Sequential(\n",
       "        (0): Conv2d(1, 40, kernel_size=(1, 25), stride=(1, 1))\n",
       "        (1): AvgPool2d(kernel_size=(1, 51), stride=(1, 5), padding=0)\n",
       "        (2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ELU(alpha=1.0)\n",
       "        (4): Conv2d(40, 40, kernel_size=(271, 1), stride=(1, 1))\n",
       "        (5): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ELU(alpha=1.0)\n",
       "        (7): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (projection): Sequential(\n",
       "        (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Rearrange('b e (h) (w) -> b (h w) e')\n",
       "      )\n",
       "    )\n",
       "    (1): FlattenHead()\n",
       "  )\n",
       "  (proj_eeg): Proj_eeg(\n",
       "    (0): Linear(in_features=1440, out_features=1024, bias=True)\n",
       "    (1): ResidualAdd(\n",
       "      (fn): Sequential(\n",
       "        (0): GELU(approximate='none')\n",
       "        (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (loss_func): ClipLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_subjects = ['sub-01', 'sub-02', 'sub-03', 'sub-04', 'sub-05', 'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10']\n",
    "# Inference Parameters\n",
    "device_preference = 'cuda:0'  # e.g., 'cuda:0' or 'cpu'\n",
    "device_type = 'gpu'  # 'cpu' or 'gpu'\n",
    "data_path = \"/home/ldy/THINGS-MEG/preprocessed_newsplit\"\n",
    "# Set device based on the argument\n",
    "device = torch.device(device_preference if device_type == 'gpu' and torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "test_classes = 1024\n",
    "eval_modality = 'meg'  # Modality to evaluate on\n",
    "\n",
    "meg_model = ATMS()\n",
    "# Add map_location parameter when loading the model\n",
    "meg_model.load_state_dict(torch.load(\n",
    "    \"/mnt/dataset1/ldy/Workspace/FLORA/models/ATMS/across/MEG/01-21_19-05/40.pth\",\n",
    "    map_location=device  # This will map the model to your specified device\n",
    "))\n",
    "meg_model.to(device)\n",
    "meg_model.eval()  # Set model to evaluation mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.subjects ['sub-01', 'sub-02', 'sub-03', 'sub-04', 'sub-05', 'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10']\n",
      "adap_subject sub-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_list 200\n",
      "Data tensor shape: torch.Size([200, 271, 201]), label tensor shape: torch.Size([200]), text length: 200, image length: 200\n",
      "\n",
      "Debug info for test mode:\n",
      "- Index: 0\n",
      "- Label: 0\n",
      "- Class index: 0\n",
      "- Total images: 200\n",
      "- Sampled images: 17\n",
      "- Total texts: 200\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ATMS.forward() missing 1 required positional argument: 'subject_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m text_features_test_all \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mtext_features    \n\u001b[1;32m     14\u001b[0m img_features_test_all \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mimg_features\n\u001b[0;32m---> 16\u001b[0m test_loss, test_accuracy, top5_acc, labels, meg_features_test \u001b[38;5;241m=\u001b[39m \u001b[43mget_megfeatures\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeg_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features_test_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_features_test_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_modality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_modality\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_classes\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m _, v2_acc, _, _, _ \u001b[38;5;241m=\u001b[39m get_megfeatures(\n\u001b[1;32m     20\u001b[0m     sub, meg_model, test_loader, device, text_features_test_all, img_features_test_all, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, eval_modality\u001b[38;5;241m=\u001b[39meval_modality, test_classes\u001b[38;5;241m=\u001b[39mtest_classes\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m _, v4_acc, _, _, _ \u001b[38;5;241m=\u001b[39m get_megfeatures(\n\u001b[1;32m     23\u001b[0m     sub, meg_model, test_loader, device, text_features_test_all, img_features_test_all, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, eval_modality\u001b[38;5;241m=\u001b[39meval_modality, test_classes\u001b[38;5;241m=\u001b[39mtest_classes\n\u001b[1;32m     24\u001b[0m )\n",
      "Cell \u001b[0;32mIn[3], line 33\u001b[0m, in \u001b[0;36mget_megfeatures\u001b[0;34m(sub, meg_model, dataloader, device, text_features_all, img_features_all, k, eval_modality, test_classes)\u001b[0m\n\u001b[1;32m     31\u001b[0m subject_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((batch_size,), subject_id, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# neural_features = meg_model(data, subject_ids)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m neural_features \u001b[38;5;241m=\u001b[39m \u001b[43mmeg_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m logit_scale \u001b[38;5;241m=\u001b[39m meg_model\u001b[38;5;241m.\u001b[39mlogit_scale\u001b[38;5;241m.\u001b[39mfloat()            \n\u001b[1;32m     36\u001b[0m features_list\u001b[38;5;241m.\u001b[39mappend(neural_features)\n",
      "File \u001b[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: ATMS.forward() missing 1 required positional argument: 'subject_ids'"
     ]
    }
   ],
   "source": [
    "#####################################################################################\n",
    "import numpy as np  # 导入numpy用于计算平均值\n",
    "test_accuracies = []\n",
    "test_accuracies_top5 = []\n",
    "v2_accuracies = []\n",
    "v4_accuracies = []\n",
    "v10_accuracies = []\n",
    "\n",
    "for sub in test_subjects:\n",
    "    test_dataset = MEGDataset(data_path, adap_subject=sub, subjects=test_subjects, train=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, drop_last=False)\n",
    "    \n",
    "    text_features_test_all = test_dataset.text_features    \n",
    "    img_features_test_all = test_dataset.img_features\n",
    "    \n",
    "    test_loss, test_accuracy, top5_acc, labels, meg_features_test = get_megfeatures(\n",
    "        sub, meg_model, test_loader, device, text_features_test_all, img_features_test_all, k=test_classes, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )\n",
    "    _, v2_acc, _, _, _ = get_megfeatures(\n",
    "        sub, meg_model, test_loader, device, text_features_test_all, img_features_test_all, k=2, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )\n",
    "    _, v4_acc, _, _, _ = get_megfeatures(\n",
    "        sub, meg_model, test_loader, device, text_features_test_all, img_features_test_all, k=4, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )\n",
    "    _, v10_acc, _, _, _ = get_megfeatures(\n",
    "        sub, meg_model, test_loader, device, text_features_test_all, img_features_test_all, k=10, eval_modality=eval_modality, test_classes=test_classes\n",
    "    )    \n",
    "    \n",
    "    test_accuracies.append(test_accuracy)\n",
    "    test_accuracies_top5.append(top5_acc)\n",
    "    v2_accuracies.append(v2_acc)\n",
    "    v4_accuracies.append(v4_acc)\n",
    "    v10_accuracies.append(v10_acc)\n",
    "    \n",
    "    print(f\" - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Top5 Accuracy: {top5_acc:.4f}\")    \n",
    "    print(f\" - Test Loss: {test_loss:.4f}, v2_acc Accuracy: {v2_acc:.4f}\")\n",
    "    print(f\" - Test Loss: {test_loss:.4f}, v4_acc Accuracy: {v4_acc:.4f}\")\n",
    "    print(f\" - Test Loss: {test_loss:.4f}, v10_acc Accuracy: {v10_acc:.4f}\")\n",
    "\n",
    "# 计算各项指标的平均准确率\n",
    "average_test_accuracy = np.mean(test_accuracies)\n",
    "average_test_accuracy_top5 = np.mean(test_accuracies_top5)\n",
    "average_v2_acc = np.mean(v2_accuracies)\n",
    "average_v4_acc = np.mean(v4_accuracies)\n",
    "average_v10_acc = np.mean(v10_accuracies)\n",
    "\n",
    "print(f\"\\nAverage Test Accuracy across all subjects: {average_test_accuracy:.4f}\")\n",
    "print(f\"\\nAverage Test Top5 Accuracy across all subjects: {average_test_accuracy_top5:.4f}\")\n",
    "print(f\"Average v2_acc Accuracy across all subjects: {average_v2_acc:.4f}\")\n",
    "print(f\"Average v4_acc Accuracy across all subjects: {average_v4_acc:.4f}\")\n",
    "print(f\"Average v10_acc Accuracy across all subjects: {average_v10_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
