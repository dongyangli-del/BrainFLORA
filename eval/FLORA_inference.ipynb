{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/local_attention/rotary.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/local_attention/rotary.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/colt5_attention/coor_descent.py:16: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/ldy/miniconda3/envs/BCI/lib/python3.10/site-packages/colt5_attention/topk.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "# Environment configuration\n",
    "os.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_API_KEY_HERE\"  # Replace with your actual key\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "# Path configuration\n",
    "current_dir = os.getcwd()\n",
    "# Build project root path (assuming parent_dir and model are at the same level)\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "\n",
    "# Add project root to sys.path for absolute imports\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Model imports\n",
    "from model.unified_encoder_multi_tower import UnifiedEncoder\n",
    "\n",
    "# Deep learning framework imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Experiment tracking\n",
    "import wandb\n",
    "wandb.init(mode=\"disabled\")\n",
    "\n",
    "# Dataset imports\n",
    "from data_preparing.eegdatasets import EEGDataset\n",
    "from data_preparing.megdatasets_averaged import MEGDataset\n",
    "from data_preparing.fmri_datasets_joint_subjects import fMRIDataset\n",
    "from data_preparing.datasets_mixer import (\n",
    "    MetaEEGDataset,\n",
    "    MetaMEGDataset,\n",
    "    MetafMRIDataset,\n",
    "    MetaDataLoader\n",
    ")\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Custom loss functions\n",
    "from loss import ClipLoss\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Modality: eeg\n",
      "Test Subjects: ['sub-01', 'sub-02', 'sub-03', 'sub-04', 'sub-05', 'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10']\n",
      "Number of Test Classes: 200\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "def extract_id_from_string(s):\n",
    "    \"\"\"Extract numeric ID from the end of a string.\n",
    "    \n",
    "    Args:\n",
    "        s (str): Input string containing numeric ID at the end\n",
    "        \n",
    "    Returns:\n",
    "        int or None: Extracted numeric ID or None if not found\n",
    "    \"\"\"\n",
    "    match = re.search(r'\\d+$', s)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_eegfeatures(unified_model, dataloader, device, text_features_all, \n",
    "                   img_features_all, k, eval_modality, test_classes):\n",
    "    \"\"\"Evaluate model performance on neural data and compute features.\n",
    "    \n",
    "    Args:\n",
    "        unified_model: Trained unified encoder model\n",
    "        dataloader: DataLoader for evaluation data\n",
    "        device: Device to run evaluation on\n",
    "        text_features_all: Pre-computed text features for all classes\n",
    "        img_features_all: Pre-computed image features for all classes\n",
    "        k: Number of classes to sample for evaluation\n",
    "        eval_modality: Modality being evaluated ('eeg', 'meg', 'fmri')\n",
    "        test_classes: Total number of test classes\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy, top5_accuracy, labels, features_tensor)\n",
    "    \"\"\"\n",
    "    unified_model.eval()\n",
    "    \n",
    "    # Prepare features based on modality\n",
    "    text_features_all = text_features_all[eval_modality].to(device).float()\n",
    "    \n",
    "    if eval_modality in ['eeg', 'fmri']:\n",
    "        img_features_all = img_features_all[eval_modality].to(device).float()\n",
    "    elif eval_modality == 'meg':\n",
    "        # Sample every 12th feature for MEG data\n",
    "        img_features_all = img_features_all[eval_modality][::12].to(device).float()\n",
    "    \n",
    "    # Initialize evaluation metrics\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    top5_correct_count = 0\n",
    "    total = 0\n",
    "    \n",
    "    loss_func = ClipLoss()\n",
    "    all_labels = set(range(text_features_all.size(0)))\n",
    "    \n",
    "    # Feature saving configuration\n",
    "    save_features = False\n",
    "    features_list = []\n",
    "    features_tensor = torch.zeros(0, 0)\n",
    "    sub = 'sub-02'  # Subject identifier for saving\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (modal, data, labels, text, text_features, \n",
    "                       img, img_features, _, _, sub_ids) in enumerate(dataloader):\n",
    "            \n",
    "            # Move data to device\n",
    "            data = data.to(device)\n",
    "            text_features = text_features.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            img_features = img_features.to(device).float()\n",
    "            \n",
    "            # Extract subject IDs\n",
    "            subject_ids = [extract_id_from_string(sub_id) for sub_id in sub_ids]\n",
    "            subject_ids = torch.tensor(subject_ids, dtype=torch.long).to(device)\n",
    "            \n",
    "            # Get neural features from model\n",
    "            neural_features = unified_model(data, subject_ids, modal=eval_modality)\n",
    "            logit_scale = unified_model.logit_scale.float()\n",
    "            \n",
    "            # Store features for potential saving\n",
    "            features_list.append(neural_features)\n",
    "            \n",
    "            # Compute loss\n",
    "            img_loss = loss_func(neural_features, img_features, logit_scale)\n",
    "            loss = img_loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Evaluate each sample in the batch\n",
    "            for idx, label in enumerate(labels):\n",
    "                # Sample k-1 random classes plus the true class\n",
    "                possible_classes = list(all_labels - {label.item()})\n",
    "                selected_classes = random.sample(possible_classes, k-1) + [label.item()]\n",
    "                selected_img_features = img_features_all[selected_classes]\n",
    "                \n",
    "                # Compute logits for image features\n",
    "                logits_img = logit_scale * neural_features[idx] @ selected_img_features.T\n",
    "                logits_single = logits_img\n",
    "                \n",
    "                # Get prediction\n",
    "                predicted_label = selected_classes[torch.argmax(logits_single).item()]\n",
    "                \n",
    "                # Check top-1 accuracy\n",
    "                if predicted_label == label.item():\n",
    "                    correct += 1\n",
    "                \n",
    "                # Check top-5 accuracy (only when k equals test_classes)\n",
    "                if k == test_classes:\n",
    "                    _, top5_indices = torch.topk(logits_single, 5, largest=True)\n",
    "                    if label.item() in [selected_classes[i] for i in top5_indices.tolist()]:\n",
    "                        top5_correct_count += 1\n",
    "                \n",
    "                total += 1\n",
    "        \n",
    "        # Save features if enabled\n",
    "        if save_features:\n",
    "            features_tensor = torch.cat(features_list, dim=0)\n",
    "            print(f\"Features tensor shape: {features_tensor.shape}\")\n",
    "            torch.save(features_tensor.cpu(), f\"ATM_S_neural_features_{sub}_train.pt\")\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    average_loss = total_loss / (batch_idx + 1)\n",
    "    accuracy = correct / total\n",
    "    top5_acc = top5_correct_count / total\n",
    "    \n",
    "    return average_loss, accuracy, top5_acc, labels, features_tensor.cpu()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration Parameters\n",
    "# ============================================================================\n",
    "\n",
    "# Model paths for different modalities\n",
    "ENCODER_PATHS = {\n",
    "    'eeg': '/mnt/dataset1/ldy/Workspace/EEG_Image_decode/Retrieval/models/contrast/across/ATMS/01-06_01-46/150.pth',\n",
    "    'meg': '/mnt/dataset1/ldy/Workspace/EEG_Image_decode/Retrieval/models/contrast/across/ATMS/01-11_14-50/150.pth',\n",
    "    'fmri': '/mnt/dataset1/ldy/Workspace/EEG_Image_decode/Retrieval/models/contrast/across/ATMS/01-18_01-35/150.pth'\n",
    "}\n",
    "\n",
    "# Evaluation configuration\n",
    "EVAL_MODALITY = 'eeg'  # Options: 'eeg', 'meg', 'fmri'\n",
    "\n",
    "# Subject configurations for different modalities\n",
    "SUBJECTS_CONFIG = {\n",
    "    'eeg': ['sub-01', 'sub-02', 'sub-03', 'sub-04', 'sub-05', \n",
    "            'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10'],\n",
    "    'meg': ['sub-01', 'sub-02', 'sub-03', 'sub-04'],\n",
    "    'fmri': ['sub-01', 'sub-02', 'sub-03']\n",
    "}\n",
    "\n",
    "# Class configurations for different modalities\n",
    "CLASSES_CONFIG = {\n",
    "    'eeg': 200,\n",
    "    'meg': 200,\n",
    "    'fmri': 100\n",
    "}\n",
    "\n",
    "# Available modalities\n",
    "MODALITIES = ['eeg', 'meg', 'fmri']\n",
    "\n",
    "# Update configuration based on evaluation modality\n",
    "if EVAL_MODALITY not in MODALITIES:\n",
    "    raise ValueError(f\"Unsupported modality: {EVAL_MODALITY}\")\n",
    "\n",
    "test_subjects = SUBJECTS_CONFIG[EVAL_MODALITY]\n",
    "test_classes = CLASSES_CONFIG[EVAL_MODALITY]\n",
    "\n",
    "# Dataset paths\n",
    "DATASET_PATHS = {\n",
    "    'eeg': \"/mnt/dataset1/ldy/datasets/THINGS_EEG1/processed_250Hz\",\n",
    "    'meg': \"/home/ldy/THINGS-MEG/preprocessed_newsplit\",\n",
    "    'fmri': \"/home/ldy/fmri_dataset/Preprocessed\"\n",
    "}\n",
    "\n",
    "# Output configuration (for logging and saving results)\n",
    "OUTPUT_CONFIG = {\n",
    "    'output_dir': './outputs/contrast',\n",
    "    'project': \"train_pos_img_text_rep\",\n",
    "    'entity': \"sustech_rethinkingbci\",\n",
    "    'name': \"lr=3e-4_img_pos_pro_eeg\"\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "DEVICE_CONFIG = {\n",
    "    'device_preference': 'cuda:3',  # Options: 'cuda:0', 'cuda:1', 'cpu'\n",
    "    'device_type': 'gpu'  # Options: 'cpu', 'gpu'\n",
    "}\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Evaluation Modality: {EVAL_MODALITY}\")\n",
    "print(f\"Test Subjects: {test_subjects}\")\n",
    "print(f\"Number of Test Classes: {test_classes}\")\n",
    "print(f\"Device: {DEVICE_CONFIG['device_preference']}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n",
      "Total parameters: 161.96M\n",
      "Trainable parameters: 7.36M\n",
      "Trainable parameters percentage: 4.54%\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m test_subjects:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Prepare test dataset based on eval_modality and test_subjects\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_modality \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meeg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mEEGDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43meeg_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msub\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m eval_modality \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     65\u001b[0m         test_dataset \u001b[38;5;241m=\u001b[39m MEGDataset(meg_data_path, subjects\u001b[38;5;241m=\u001b[39m[sub], train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/dataset1/ldy/Workspace/FLORA/data_preparing/eegdatasets.py:110\u001b[0m, in \u001b[0;36mEEGDataset.__init__\u001b[0;34m(self, data_path, adap_subject, subjects, train, use_caption, time_window, classes, pictures)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meeg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# assert any subjects in subject_list\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28many\u001b[39m(sub \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubject_list \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubjects)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_caption \u001b[38;5;241m=\u001b[39m use_caption\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_data()\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def format_num(num):\n",
    "    \"\"\"Format number with appropriate unit (K, M, B, T, P).\n",
    "    \n",
    "    Args:\n",
    "        num (int): Number to format\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted number with unit\n",
    "    \"\"\"\n",
    "    for unit in ['', 'K', 'M', 'B', 'T']:\n",
    "        if num < 1000:\n",
    "            return f\"{num:.2f}{unit}\"\n",
    "        num /= 1000\n",
    "    return f\"{num:.2f}P\"\n",
    "\n",
    "\n",
    "def print_model_info(model):\n",
    "    \"\"\"Print model parameter information.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to analyze\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {format_num(total_params)}\")\n",
    "    print(f\"Trainable parameters: {format_num(trainable_params)}\")\n",
    "    \n",
    "    if total_params > 0:\n",
    "        trainable_percentage = (trainable_params / total_params) * 100\n",
    "        print(f\"Trainable parameters percentage: {trainable_percentage:.2f}%\")\n",
    "    else:\n",
    "        print(\"Total parameters count is zero, cannot compute percentage.\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Model Initialization and Setup\n",
    "# ============================================================================\n",
    "\n",
    "# Parse encoder paths from configuration\n",
    "encoder_paths = {}\n",
    "for path in ENCODER_PATHS.items():\n",
    "    key, value = path\n",
    "    encoder_paths[key] = value\n",
    "\n",
    "# Set device based on configuration\n",
    "device = torch.device(\n",
    "    DEVICE_CONFIG['device_preference'] \n",
    "    if DEVICE_CONFIG['device_type'] == 'gpu' and torch.cuda.is_available() \n",
    "    else 'cpu'\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize feature storage dictionaries\n",
    "text_features_test_all = {}\n",
    "img_features_test_all = {}\n",
    "\n",
    "# Initialize the Unified Encoder Model\n",
    "print(\"Initializing Unified Encoder Model...\")\n",
    "unified_model = UnifiedEncoder(encoder_paths, device)\n",
    "\n",
    "# Load pre-trained model weights\n",
    "MODEL_PATH = \"/mnt/dataset1/ldy/Workspace/FLORA/models/contrast/across/Unified_EEG+MEG+fMRI_EEG/01-27_02-32/60.pth\"\n",
    "unified_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "unified_model.to(device)\n",
    "unified_model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Print model parameter information\n",
    "print_model_info(unified_model)\n",
    "\n",
    "# ============================================================================\n",
    "# Model Evaluation Loop\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nStarting evaluation on {EVAL_MODALITY} modality...\")\n",
    "print(f\"Testing on subjects: {test_subjects}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize accuracy tracking lists\n",
    "test_accuracies = []\n",
    "test_accuracies_top5 = []\n",
    "v2_accuracies = []\n",
    "v4_accuracies = []\n",
    "v10_accuracies = []\n",
    "\n",
    "# Evaluate each subject\n",
    "for sub in test_subjects:\n",
    "    print(f\"\\nEvaluating subject: {sub}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Prepare test dataset based on evaluation modality\n",
    "    if EVAL_MODALITY == 'eeg':\n",
    "        test_dataset = EEGDataset(\n",
    "            DATASET_PATHS['eeg'], \n",
    "            subjects=[sub], \n",
    "            train=False\n",
    "        )\n",
    "    elif EVAL_MODALITY == 'meg':\n",
    "        test_dataset = MEGDataset(\n",
    "            DATASET_PATHS['meg'], \n",
    "            subjects=[sub], \n",
    "            train=False\n",
    "        )\n",
    "    elif EVAL_MODALITY == 'fmri':\n",
    "        test_dataset = fMRIDataset(\n",
    "            DATASET_PATHS['fmri'], \n",
    "            adap_subject=sub, \n",
    "            subjects=[sub], \n",
    "            train=False\n",
    "        )\n",
    "    \n",
    "    # Extract features from dataset\n",
    "    text_features_test_all[EVAL_MODALITY] = test_dataset.text_features\n",
    "    img_features_test_all[EVAL_MODALITY] = test_dataset.img_features\n",
    "    \n",
    "    # Create data loader\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=1, \n",
    "        shuffle=False, \n",
    "        num_workers=0, \n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate with different k values\n",
    "    # Full test classes evaluation\n",
    "    test_loss, test_accuracy, top5_acc, labels, eeg_features_test = get_eegfeatures(\n",
    "        unified_model, test_loader, device, text_features_test_all, \n",
    "        img_features_test_all, k=test_classes, eval_modality=EVAL_MODALITY, \n",
    "        test_classes=test_classes\n",
    "    )\n",
    "    \n",
    "    # k=2 evaluation (binary classification)\n",
    "    _, v2_acc, _, _, _ = get_eegfeatures(\n",
    "        unified_model, test_loader, device, text_features_test_all, \n",
    "        img_features_test_all, k=2, eval_modality=EVAL_MODALITY, \n",
    "        test_classes=test_classes\n",
    "    )\n",
    "    \n",
    "    # k=4 evaluation (4-way classification)\n",
    "    _, v4_acc, _, _, _ = get_eegfeatures(\n",
    "        unified_model, test_loader, device, text_features_test_all, \n",
    "        img_features_test_all, k=4, eval_modality=EVAL_MODALITY, \n",
    "        test_classes=test_classes\n",
    "    )\n",
    "    \n",
    "    # k=10 evaluation (10-way classification)\n",
    "    _, v10_acc, _, _, _ = get_eegfeatures(\n",
    "        unified_model, test_loader, device, text_features_test_all, \n",
    "        img_features_test_all, k=10, eval_modality=EVAL_MODALITY, \n",
    "        test_classes=test_classes\n",
    "    )\n",
    "    \n",
    "    # Store accuracies\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    test_accuracies_top5.append(top5_acc)\n",
    "    v2_accuracies.append(v2_acc)\n",
    "    v4_accuracies.append(v4_acc)\n",
    "    v10_accuracies.append(v10_acc)\n",
    "    \n",
    "    # Print subject results\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy (k={test_classes}): {test_accuracy:.4f}\")\n",
    "    print(f\"Top-5 Accuracy: {top5_acc:.4f}\")\n",
    "    print(f\"Binary Accuracy (k=2): {v2_acc:.4f}\")\n",
    "    print(f\"4-way Accuracy (k=4): {v4_acc:.4f}\")\n",
    "    print(f\"10-way Accuracy (k=10): {v10_acc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Results Summary\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate average accuracies across all subjects\n",
    "average_test_accuracy = np.mean(test_accuracies)\n",
    "average_test_accuracy_top5 = np.mean(test_accuracies_top5)\n",
    "average_v2_acc = np.mean(v2_accuracies)\n",
    "average_v4_acc = np.mean(v4_accuracies)\n",
    "average_v10_acc = np.mean(v10_accuracies)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL RESULTS - AVERAGE ACROSS ALL SUBJECTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Average Test Accuracy (k={test_classes}): {average_test_accuracy:.4f}\")\n",
    "print(f\"Average Top-5 Accuracy: {average_test_accuracy_top5:.4f}\")\n",
    "print(f\"Average Binary Accuracy (k=2): {average_v2_acc:.4f}\")\n",
    "print(f\"Average 4-way Accuracy (k=4): {average_v4_acc:.4f}\")\n",
    "print(f\"Average 10-way Accuracy (k=10): {average_v10_acc:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Optional: Print individual subject results for reference\n",
    "print(\"\\nINDIVIDUAL SUBJECT RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "for i, sub in enumerate(test_subjects):\n",
    "    print(f\"{sub}: Acc={test_accuracies[i]:.4f}, \"\n",
    "          f\"Top5={test_accuracies_top5[i]:.4f}, \"\n",
    "          f\"k=2={v2_accuracies[i]:.4f}, \"\n",
    "          f\"k=4={v4_accuracies[i]:.4f}, \"\n",
    "          f\"k=10={v10_accuracies[i]:.4f}\")\n",
    "print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meg2speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
