<div align="center">

<h2 style="border-bottom: 1px solid lightgray;">üß†BrainFLORA: Uncovering Brain Concept Representation via Multimodal Neural Embeddings</h2>
<!-- Badges and Links Section -->
<div style="display: flex; align-items: center; justify-content: center;">

<p align="center">
  <a href="#">
  <p align="center">
    <a href='https://arxiv.org/pdf/2403.07721'><img src='http://img.shields.io/badge/Paper-arxiv.2403.07721-B31B1B.svg'></a>
    <a href='https://huggingface.co/datasets/LidongYang/EEG_Image_decode/tree/main'><img src='https://img.shields.io/badge/BrainFLORA-%F0%9F%A4%97%20Hugging%20Face-blue'></a>
  </p>
</p>

<!-- Badges and Links Section -->
<div style="display: flex; align-items: center; justify-content: center;">

</div>

<br/>

</div>

<!-- BrainFLORA: Uncovering Brain Concept Representation via Multimodal Neural Embeddings -->


<!--  -->
<img src="fig-overview_00.png" alt="fig-genexample" style="max-width: 75%; height: auto;"/>  

A comparative overview of visual decoding paradigms.





<h2 style="border-bottom: 1px solid lightgray; margin-bottom: 5px;">üê£ Update</h2>
<!-- * **2025/07/09**, We update endpose control mode, please see [[RoboTwin Doc - Usage - Control  -->
* **2025/07/12**, Officially released BrainFLORA.


<img src="fig-framework_00.png" alt="Framework" style="max-width: 70%; height: auto;"/>

Overall Architecture of BrainFLORA.


<!-- ## Environment setup -->
<h2 style="border-bottom: 1px solid lightgray; margin-bottom: 5px;">üõ†Ô∏èEnvironment setup</h2>

Run ``setup.sh`` to quickly create a conda environment that contains the packages necessary to run our scripts; activate the environment with conda activate FLORA.


```
. setup.sh
```

You can also create a new conda environment and install the required dependencies by running
```
conda env create -f environment.yml
conda activate FLORA
```

<!-- ## Prepare for Dataset -->

To download the raw dataÔºåyou can followÔºö
Dataset | Download path| Dataset | Download path
:---: | :---:|:---: | :---:
THINGS-EEG1 |  [Download](https://openneuro.org/datasets/ds003825/versions/1.1.0) | THINGS-EEG2 | [Download](https://osf.io/3jk45/)
THINGS-MEG |  [Download](https://openneuro.org/datasets/ds004212/versions/2.0.0)| THINGS-fMRI  |  [Download](https://openneuro.org/datasets/ds004192/versions/1.0.7)
THINGS-Images |  [Download](https://osf.io/rdxy2)

<!-- We will release the processed data (such as THINGS-EEG1, THINGS-EEG2, THINGS-MEG, THINGS-fMRI) on [Huggingface], which can be directly used for training.
 -->


<!-- ## Quick training and test  -->
<h2 style="border-bottom: 1px solid lightgray; margin-bottom: 5px;">üö¥‚Äç‚ôÇÔ∏èQuick training and test</h2>


#### 1.Visual Retrieval
We provide the script to train the modality encoders for ``joint subject training`` Please modify your data set path and run:
```
cd Retrieval/
python retrieval_joint_train_medformer.py --logger True --gpu cuda:0  --output_dir ./outputs/contrast
```

Additionally, replicating the results of other modalities (e.g. MEG, fMRI) by run
```
cd Retrieval/
python retrieval_joint_train_MEG_rerank_medformer.py --logger True --gpu cuda:0  --output_dir ./outputs/contrast
```
We provide the script to evaluation the models:
```
cd eval/
FLORA_inference.ipynb
```

#### 2.Visual Reconstruction
We provide quick training and inference scripts for ``high level and low level pipeline`` of visual reconstruction. Please modify your data set path and run zero-shot on test dataset:
```
# Train and get multimodal neural embeddings aligned with clip embedding:
python train_unified_encoder_highlevel_diffprior.py --modalities ['eeg', 'meg', 'fmri'] --gpu cuda:0  --output_dir ./outputs/contrast
```

```
# Reconstruct images by assigning modalities and subjects:
python FLORA_inference_reconst.py
```
#### 3.Visual Captioning

We provide scripts for caption generation.
```
# step 1: train feature adapter
python train_unified_encoder_highlevel_diffprior.py --modalities ['eeg', 'meg', 'fmri'] --gpu cuda:0  --output_dir ./outputs/contrast

# step 2: get caption from prior latent
FLORA_inference_caption.ipynb

```

# üëç Citations
If you find our work useful, please consider citing:

<b>RoboTwin 2.0</b>: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation
```

```


<!-- ## Acknowledge -->
<h2 style="border-bottom: 1px solid lightgray; margin-bottom: 5px;">üò∫Acknowledge</h2>

1.Thanks to Y Song et al. for their contribution in data set preprocessing and neural network structure, we refer to their work:</br>"[Decoding Natural Images from EEG for Object Recognition](https://arxiv.org/pdf/2308.13234.pdf)".</br> Yonghao Song, Bingchuan Liu, Xiang Li, Nanlin Shi, Yijun Wang, and Xiaorong Gao. 

2.We also thank the authors of [SDRecon](https://github.com/yu-takagi/StableDiffusionReconstruction) for providing the codes and the results. Some parts of the training script are based on [MindEye](https://medarc-ai.github.io/mindeye/) and [MindEye2](https://github.com/MedARC-AI/MindEyeV2). Thanks for the awesome research works.

3.Here we provide our THING-EEG dataset cited in the paper:</br>"[A large and rich EEG dataset for modeling human visual object recognition](https://www.sciencedirect.com/science/article/pii/S1053811922008758?via%3Dihub)".</br>
Alessandro T. Gifford, Kshitij Dwivedi, Gemma Roig, Radoslaw M. Cichy.


4.Another used THINGS-MEG data set provides a reference:</br>"[THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior.](https://elifesciences.org/articles/82580.pdf)".</br> Hebart, Martin N., Oliver Contier, Lina Teichmann, Adam H. Rockter, Charles Y. Zheng, Alexis Kidder, Anna Corriveau, Maryam Vaziri-Pashkam, and Chris I. Baker.





Contact [Dongyang Li](https://github.com/dongyangli-del) if you have any questions or suggestions.

# üè∑Ô∏è License
This repository is released under the MIT license. See [LICENSE](./LICENSE) for additional details.